{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/function9/bidirectional-lstm-gan-music-generation\n",
    "#https://www.kaggle.com/abhisheksinha28/bidirectional-lstm/data\n",
    "#https://www.kaggle.com/sekfook97/gan-for-anomaly-detection\n",
    "# importing libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Bidirectional, LSTM, Reshape, RepeatVector, TimeDistributed\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "\"feature1\": np.float16,\n",
    "\"feature2\": np.float16,\n",
    "\"feature3\": np.float16,\n",
    "\"feature4\": np.float16,\n",
    "\"feature5\": np.float16,\n",
    "\"feature6\": np.float16,\n",
    "\"feature7\": np.float16,\n",
    "\"feature8\": np.float16,\n",
    "\"feature9\": np.float16,\n",
    "\"feature10\": np.float16,\n",
    "\"feature11\": np.float16,\n",
    "\"feature12\": np.float16,\n",
    "\"feature13\": np.float16,\n",
    "\"feature14\": np.float16,\n",
    "\"feature15\": np.float16,\n",
    "\"feature16\": np.float16,\n",
    "\"feature17\": np.float16,\n",
    "\"feature18\": np.float16,\n",
    "\"feature19\": np.float16,\n",
    "\"feature20\": np.float16,\n",
    "\"feature21\": np.float16,\n",
    "\"feature22\": np.float16,\n",
    "\"feature23\": np.float16,\n",
    "\"feature24\": np.float16,\n",
    "\"feature25\": np.float16,    \n",
    "\"feature26\": np.float16,\n",
    "\"feature27\": np.float16,\n",
    "\"feature28\": np.float16,\n",
    "\"feature29\": np.float16,\n",
    "\"feature30\": np.float16,    \n",
    "\"label\": np.object}    \n",
    "columns = [\"feature1\",\"feature2\",\"feature3\",\"feature4\",\"feature5\",\"feature6\",\"feature7\",\"feature8\",\"feature9\",\"feature10\",\"feature11\",\"feature12\",\"feature13\",\"feature14\",\"feature15\",\"feature16\",\"feature17\",\"feature18\",\"feature19\",\"feature20\",\"feature21\",\"feature22\",\"feature23\",\"feature24\",\"feature25\",\"feature26\",\"feature27\",\"feature28\",\"feature29\",\"feature30\",\"label\"]\n",
    "#df = pd.read_csv(\"/kaggle/input/kdd-cup-1999-data/kddcup.data_10_percent_corrected\", sep=\",\", names=columns, dtype=dtypes, index_col=None)\n",
    "df = pd.read_csv(r\"C:\\Users\\admin\\SDN\\afterdimensionalityreduction\",sep=\",\", names=columns, dtype=str, index_col=None)\n",
    "#df_read = pd.read_csv(savefile, dtype=str, index_col=0)\n",
    "#print(type(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Abnormal    275465\n",
       "Normal       68424\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        feature1              feature2              feature3 feature4 feature5  \\\n",
       "0           0.0  0.006962014399999999            0.05562538      0.0      0.0   \n",
       "1           0.0            0.05218774                   0.0      0.0      0.0   \n",
       "2           0.0           0.048415996           0.014630627      0.0      0.0   \n",
       "3           0.0                   0.0           0.038806356      0.0      0.0   \n",
       "4           0.0            0.05172342            0.01308862      0.0      0.0   \n",
       "...         ...                   ...                   ...      ...      ...   \n",
       "343884      0.0            0.04297894           0.017960861      0.0      0.0   \n",
       "343885      0.0            0.04324762           0.014465368      0.0      0.0   \n",
       "343886      0.0           0.047583852                   0.0      0.0      0.0   \n",
       "343887      0.0            0.04297537  0.017960499999999997      0.0      0.0   \n",
       "343888      0.0            0.04319027           0.014465845      0.0      0.0   \n",
       "\n",
       "       feature6              feature7 feature8              feature9  \\\n",
       "0           0.0           0.036115788      0.0                   0.0   \n",
       "1           0.0           0.047844697      0.0                   0.0   \n",
       "2           0.0            0.04589876      0.0                   0.0   \n",
       "3           0.0                   0.0      0.0                   0.0   \n",
       "4           0.0           0.047847077      0.0                   0.0   \n",
       "...         ...                   ...      ...                   ...   \n",
       "343884      0.0           0.021094704      0.0            0.04364106   \n",
       "343885      0.0             0.0216345      0.0  0.043688737000000005   \n",
       "343886      0.0            0.02868637      0.0            0.03592718   \n",
       "343887      0.0  0.021132400000000003      0.0           0.043612387   \n",
       "343888      0.0            0.02155185      0.0            0.04372048   \n",
       "\n",
       "       feature10  ... feature22    feature23      feature24 feature25  \\\n",
       "0            0.0  ...       0.0          0.0  0.00031781942       0.0   \n",
       "1            0.0  ...       0.0          0.0            0.0       0.0   \n",
       "2            0.0  ...       0.0          0.0            0.0       0.0   \n",
       "3            0.0  ...       0.0  0.027110513            0.0       0.0   \n",
       "4            0.0  ...       0.0          0.0            0.0       0.0   \n",
       "...          ...  ...       ...          ...            ...       ...   \n",
       "343884       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "343885       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "343886       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "343887       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "343888       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "\n",
       "       feature26 feature27    feature28 feature29    feature30     label  \n",
       "0            0.0       0.0  0.021631505       0.0          0.0    Normal  \n",
       "1            0.0       0.0  0.057460763       0.0          0.0    Normal  \n",
       "2            0.0       0.0   0.05959917       0.0          0.0    Normal  \n",
       "3            0.0       0.0  0.044530995       0.0  0.013936298    Normal  \n",
       "4            0.0       0.0  0.060422726       0.0          0.0    Normal  \n",
       "...          ...       ...          ...       ...          ...       ...  \n",
       "343884       0.0       0.0  0.023048477       0.0          0.0  Abnormal  \n",
       "343885       0.0       0.0  0.023150168       0.0          0.0  Abnormal  \n",
       "343886       0.0       0.0  0.021780338       0.0          0.0  Abnormal  \n",
       "343887       0.0       0.0  0.023061449       0.0          0.0  Abnormal  \n",
       "343888       0.0       0.0  0.023140473       0.0          0.0  Abnormal  \n",
       "\n",
       "[343889 rows x 31 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 for normal data and 1 for abnormalities\n",
    "df.label=df.label.apply(lambda x: 0 if x == 'Normal' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(343889, 31)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        feature1              feature2              feature3 feature4 feature5  \\\n",
       "0           0.0  0.006962014399999999            0.05562538      0.0      0.0   \n",
       "1           0.0            0.05218774                   0.0      0.0      0.0   \n",
       "2           0.0           0.048415996           0.014630627      0.0      0.0   \n",
       "3           0.0                   0.0           0.038806356      0.0      0.0   \n",
       "4           0.0            0.05172342            0.01308862      0.0      0.0   \n",
       "...         ...                   ...                   ...      ...      ...   \n",
       "343884      0.0            0.04297894           0.017960861      0.0      0.0   \n",
       "343885      0.0            0.04324762           0.014465368      0.0      0.0   \n",
       "343886      0.0           0.047583852                   0.0      0.0      0.0   \n",
       "343887      0.0            0.04297537  0.017960499999999997      0.0      0.0   \n",
       "343888      0.0            0.04319027           0.014465845      0.0      0.0   \n",
       "\n",
       "       feature6              feature7 feature8              feature9  \\\n",
       "0           0.0           0.036115788      0.0                   0.0   \n",
       "1           0.0           0.047844697      0.0                   0.0   \n",
       "2           0.0            0.04589876      0.0                   0.0   \n",
       "3           0.0                   0.0      0.0                   0.0   \n",
       "4           0.0           0.047847077      0.0                   0.0   \n",
       "...         ...                   ...      ...                   ...   \n",
       "343884      0.0           0.021094704      0.0            0.04364106   \n",
       "343885      0.0             0.0216345      0.0  0.043688737000000005   \n",
       "343886      0.0            0.02868637      0.0            0.03592718   \n",
       "343887      0.0  0.021132400000000003      0.0           0.043612387   \n",
       "343888      0.0            0.02155185      0.0            0.04372048   \n",
       "\n",
       "       feature10  ... feature22    feature23      feature24 feature25  \\\n",
       "0            0.0  ...       0.0          0.0  0.00031781942       0.0   \n",
       "1            0.0  ...       0.0          0.0            0.0       0.0   \n",
       "2            0.0  ...       0.0          0.0            0.0       0.0   \n",
       "3            0.0  ...       0.0  0.027110513            0.0       0.0   \n",
       "4            0.0  ...       0.0          0.0            0.0       0.0   \n",
       "...          ...  ...       ...          ...            ...       ...   \n",
       "343884       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "343885       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "343886       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "343887       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "343888       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "\n",
       "       feature26 feature27    feature28 feature29    feature30 label  \n",
       "0            0.0       0.0  0.021631505       0.0          0.0     0  \n",
       "1            0.0       0.0  0.057460763       0.0          0.0     0  \n",
       "2            0.0       0.0   0.05959917       0.0          0.0     0  \n",
       "3            0.0       0.0  0.044530995       0.0  0.013936298     0  \n",
       "4            0.0       0.0  0.060422726       0.0          0.0     0  \n",
       "...          ...       ...          ...       ...          ...   ...  \n",
       "343884       0.0       0.0  0.023048477       0.0          0.0     1  \n",
       "343885       0.0       0.0  0.023150168       0.0          0.0     1  \n",
       "343886       0.0       0.0  0.021780338       0.0          0.0     1  \n",
       "343887       0.0       0.0  0.023061449       0.0          0.0     1  \n",
       "343888       0.0       0.0  0.023140473       0.0          0.0     1  \n",
       "\n",
       "[343889 rows x 31 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     object\n",
       "feature2     object\n",
       "feature3     object\n",
       "feature4     object\n",
       "feature5     object\n",
       "feature6     object\n",
       "feature7     object\n",
       "feature8     object\n",
       "feature9     object\n",
       "feature10    object\n",
       "feature11    object\n",
       "feature12    object\n",
       "feature13    object\n",
       "feature14    object\n",
       "feature15    object\n",
       "feature16    object\n",
       "feature17    object\n",
       "feature18    object\n",
       "feature19    object\n",
       "feature20    object\n",
       "feature21    object\n",
       "feature22    object\n",
       "feature23    object\n",
       "feature24    object\n",
       "feature25    object\n",
       "feature26    object\n",
       "feature27    object\n",
       "feature28    object\n",
       "feature29    object\n",
       "feature30    object\n",
       "label         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df)   \n",
    " \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['feature1'] = df['feature1'].astype(float) \n",
    "df['feature2'] = df['feature2'].astype(float) \n",
    "df['feature3'] = df['feature3'].astype(float) \n",
    "df['feature4'] = df['feature4'].astype(float) \n",
    "df['feature5'] = df['feature5'].astype(float) \n",
    "df['feature6'] = df['feature6'].astype(float) \n",
    "df['feature7'] = df['feature7'].astype(float) \n",
    "df['feature8'] = df['feature8'].astype(float) \n",
    "df['feature9'] = df['feature9'].astype(float) \n",
    "df['feature10'] = df['feature10'].astype(float) \n",
    "df['feature11'] = df['feature11'].astype(float) \n",
    "df['feature12'] = df['feature12'].astype(float) \n",
    "df['feature13'] = df['feature13'].astype(float) \n",
    "df['feature14'] = df['feature14'].astype(float) \n",
    "df['feature15'] = df['feature15'].astype(float) \n",
    "df['feature16'] = df['feature16'].astype(float) \n",
    "df['feature17'] = df['feature17'].astype(float) \n",
    "df['feature18'] = df['feature18'].astype(float) \n",
    "df['feature19'] = df['feature19'].astype(float) \n",
    "df['feature20'] = df['feature20'].astype(float) \n",
    "df['feature21'] = df['feature21'].astype(float) \n",
    "df['feature22'] = df['feature22'].astype(float) \n",
    "df['feature23'] = df['feature23'].astype(float) \n",
    "df['feature24'] = df['feature24'].astype(float) \n",
    "df['feature25'] = df['feature25'].astype(float) \n",
    "df['feature26'] = df['feature26'].astype(float) \n",
    "df['feature27'] = df['feature27'].astype(float) \n",
    "df['feature28'] = df['feature28'].astype(float)\n",
    "df['feature29'] = df['feature29'].astype(float) \n",
    "df['feature30'] = df['feature30'].astype(float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly sample 500 data point for training\n",
    "df_train=df[df.label==1].sample(275465)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275465, 31)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     float64\n",
       "feature2     float64\n",
       "feature3     float64\n",
       "feature4     float64\n",
       "feature5     float64\n",
       "feature6     float64\n",
       "feature7     float64\n",
       "feature8     float64\n",
       "feature9     float64\n",
       "feature10    float64\n",
       "feature11    float64\n",
       "feature12    float64\n",
       "feature13    float64\n",
       "feature14    float64\n",
       "feature15    float64\n",
       "feature16    float64\n",
       "feature17    float64\n",
       "feature18    float64\n",
       "feature19    float64\n",
       "feature20    float64\n",
       "feature21    float64\n",
       "feature22    float64\n",
       "feature23    float64\n",
       "feature24    float64\n",
       "feature25    float64\n",
       "feature26    float64\n",
       "feature27    float64\n",
       "feature28    float64\n",
       "feature29    float64\n",
       "feature30    float64\n",
       "label          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the item sampled from our dataset\n",
    "index_list=df_train.index\n",
    "df=df.drop(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     0.000000\n",
       "feature2     0.000000\n",
       "feature3     0.028014\n",
       "feature4     0.000000\n",
       "feature5     0.000000\n",
       "feature6     0.015670\n",
       "feature7     0.000000\n",
       "feature8     0.000000\n",
       "feature9     0.000000\n",
       "feature10    0.000000\n",
       "feature11    0.000000\n",
       "feature12    0.000000\n",
       "feature13    0.000000\n",
       "feature14    0.016695\n",
       "feature15    0.033749\n",
       "feature16    0.000000\n",
       "feature17    0.000000\n",
       "feature18    0.051203\n",
       "feature19    0.052126\n",
       "feature20    0.000000\n",
       "feature21    0.000000\n",
       "feature22    0.000000\n",
       "feature23    0.000000\n",
       "feature24    0.000000\n",
       "feature25    0.000000\n",
       "feature26    0.000000\n",
       "feature27    0.000000\n",
       "feature28    0.013512\n",
       "feature29    0.000000\n",
       "feature30    0.012734\n",
       "label        1.000000\n",
       "Name: 134425, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature22</th>\n",
       "      <th>feature23</th>\n",
       "      <th>feature24</th>\n",
       "      <th>feature25</th>\n",
       "      <th>feature26</th>\n",
       "      <th>feature27</th>\n",
       "      <th>feature28</th>\n",
       "      <th>feature29</th>\n",
       "      <th>feature30</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006962</td>\n",
       "      <td>0.055625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048416</td>\n",
       "      <td>0.014631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013936</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051723</td>\n",
       "      <td>0.013089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0       0.0  0.006962  0.055625       0.0       0.0       0.0  0.036116   \n",
       "1       0.0  0.052188  0.000000       0.0       0.0       0.0  0.047845   \n",
       "2       0.0  0.048416  0.014631       0.0       0.0       0.0  0.045899   \n",
       "3       0.0  0.000000  0.038806       0.0       0.0       0.0  0.000000   \n",
       "4       0.0  0.051723  0.013089       0.0       0.0       0.0  0.047847   \n",
       "\n",
       "   feature8  feature9  feature10  ...  feature22  feature23  feature24  \\\n",
       "0       0.0       0.0        0.0  ...        0.0   0.000000   0.000318   \n",
       "1       0.0       0.0        0.0  ...        0.0   0.000000   0.000000   \n",
       "2       0.0       0.0        0.0  ...        0.0   0.000000   0.000000   \n",
       "3       0.0       0.0        0.0  ...        0.0   0.027111   0.000000   \n",
       "4       0.0       0.0        0.0  ...        0.0   0.000000   0.000000   \n",
       "\n",
       "   feature25  feature26  feature27  feature28  feature29  feature30  label  \n",
       "0        0.0        0.0        0.0   0.021632        0.0   0.000000      0  \n",
       "1        0.0        0.0        0.0   0.057461        0.0   0.000000      0  \n",
       "2        0.0        0.0        0.0   0.059599        0.0   0.000000      0  \n",
       "3        0.0        0.0        0.0   0.044531        0.0   0.013936      0  \n",
       "4        0.0        0.0        0.0   0.060423        0.0   0.000000      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the label columns\n",
    "df_label=df_train.label\n",
    "df_train=df_train.drop('label',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label\n",
    "df_label.to_csv('abnormallabel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275465, 30)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     0.000000\n",
       "feature2     0.000000\n",
       "feature3     0.028014\n",
       "feature4     0.000000\n",
       "feature5     0.000000\n",
       "feature6     0.015670\n",
       "feature7     0.000000\n",
       "feature8     0.000000\n",
       "feature9     0.000000\n",
       "feature10    0.000000\n",
       "feature11    0.000000\n",
       "feature12    0.000000\n",
       "feature13    0.000000\n",
       "feature14    0.016695\n",
       "feature15    0.033749\n",
       "feature16    0.000000\n",
       "feature17    0.000000\n",
       "feature18    0.051203\n",
       "feature19    0.052126\n",
       "feature20    0.000000\n",
       "feature21    0.000000\n",
       "feature22    0.000000\n",
       "feature23    0.000000\n",
       "feature24    0.000000\n",
       "feature25    0.000000\n",
       "feature26    0.000000\n",
       "feature27    0.000000\n",
       "feature28    0.013512\n",
       "feature29    0.000000\n",
       "feature30    0.012734\n",
       "Name: 134425, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275465, 30)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.values.reshape((df_train.shape[0], 1, df_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275465, 1, 30)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature22</th>\n",
       "      <th>feature23</th>\n",
       "      <th>feature24</th>\n",
       "      <th>feature25</th>\n",
       "      <th>feature26</th>\n",
       "      <th>feature27</th>\n",
       "      <th>feature28</th>\n",
       "      <th>feature29</th>\n",
       "      <th>feature30</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006962</td>\n",
       "      <td>0.055625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048416</td>\n",
       "      <td>0.014631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013936</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051723</td>\n",
       "      <td>0.013089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0       0.0  0.006962  0.055625       0.0       0.0       0.0  0.036116   \n",
       "1       0.0  0.052188  0.000000       0.0       0.0       0.0  0.047845   \n",
       "2       0.0  0.048416  0.014631       0.0       0.0       0.0  0.045899   \n",
       "3       0.0  0.000000  0.038806       0.0       0.0       0.0  0.000000   \n",
       "4       0.0  0.051723  0.013089       0.0       0.0       0.0  0.047847   \n",
       "\n",
       "   feature8  feature9  feature10  ...  feature22  feature23  feature24  \\\n",
       "0       0.0       0.0        0.0  ...        0.0   0.000000   0.000318   \n",
       "1       0.0       0.0        0.0  ...        0.0   0.000000   0.000000   \n",
       "2       0.0       0.0        0.0  ...        0.0   0.000000   0.000000   \n",
       "3       0.0       0.0        0.0  ...        0.0   0.027111   0.000000   \n",
       "4       0.0       0.0        0.0  ...        0.0   0.000000   0.000000   \n",
       "\n",
       "   feature25  feature26  feature27  feature28  feature29  feature30  label  \n",
       "0        0.0        0.0        0.0   0.021632        0.0   0.000000      0  \n",
       "1        0.0        0.0        0.0   0.057461        0.0   0.000000      0  \n",
       "2        0.0        0.0        0.0   0.059599        0.0   0.000000      0  \n",
       "3        0.0        0.0        0.0   0.044531        0.0   0.013936      0  \n",
       "4        0.0        0.0        0.0   0.060423        0.0   0.000000      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "timesteps =  df_train.shape[1] # equal to the lookback\n",
    "n_features =  df_train.shape[2] # 59\n",
    "\n",
    "epochs = 15\n",
    "batch = 64\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=dict()\n",
    "history['gen']=[]\n",
    "history['dis']=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error\n",
    "                             ,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 1\n",
    "        self.img_cols = 30\n",
    "        self.img_shape = (self.img_rows, self.img_cols)\n",
    "        self.latent_dim = 30\n",
    "\n",
    "        optimizer = Adam(0.0001, 0.4)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates song\n",
    "        z = Input(shape=(1,30))\n",
    "    \n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(1, 30)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #specifying output to have 40 timesteps\n",
    "        model.add(RepeatVector(30))\n",
    "        #specifying 1 feature as the output\n",
    "        \n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.3))   \n",
    "        model.add(TimeDistributed(Dense(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(1,30))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "        #return Model(img)\n",
    "    \n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(64, use_bias=False, input_shape=(30, 1)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Convolution1D(32, 3, strides=2, padding='same', use_bias=False))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Convolution1D(1, 3, strides=2, padding='same', use_bias=False))\n",
    "        model.add(Flatten())        \n",
    "        model.add(RepeatVector(1))        \n",
    "        model.add(TimeDistributed(Dense(32, activation = 'relu')))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(1, activation = 'relu')))\n",
    "        #model.add(TimeDistributed(Dense(1)))\n",
    "        #model.add(Dense(1, activation='tanh'))\n",
    "        #model.add(Dense(1))\n",
    "        #model.add(Activation('sigmoid'))\n",
    "        model.summary()\n",
    "        #model.add(layers.BatchNormalization())\n",
    "        #model.add(sigmoid())\n",
    "       \n",
    "        img = Input(shape=(30,1))\n",
    "        validity = model(img)\n",
    "        print(\"exit\")\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs=15, batch_size=64, save_interval=50):\n",
    "        print(\"enter\")  \n",
    "        # Load the dataset\n",
    "        X_train = df_train\n",
    "       \n",
    "        # Rescale 0 to 1\n",
    "      #  X_train = X_train / 128\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size,1,1))\n",
    "        fake = np.zeros((batch_size,1,1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of songs\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            imgs = np.array(imgs)\n",
    "            imgs = imgs.reshape(len(imgs),30,1)\n",
    "\n",
    "            # Sample noise and generate a batch of new songs\n",
    "            noise = np.random.normal(0, 1, (batch_size,1,30))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake songs as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "            history['gen'].append(g_loss)\n",
    "            \n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            history['dis'].append(d_loss) \n",
    "            #return (g_loss,d_loss)\n",
    "            # If at save interval => save model\n",
    "#            if epoch % save_interval == 0:\n",
    " #               self.generator.save(\"LSTM_generator.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 30, 64)            64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 15, 32)            6144      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 15, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 8, 1)              96        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 1, 8)              0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 32)             288       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 1, 1)              33        \n",
      "=================================================================\n",
      "Total params: 6,625\n",
      "Trainable params: 6,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "exit\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 1, 256)            162816    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 1, 256)            394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 30, 256)           394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 30, 256)           394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 30, 256)           394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 30, 128)           32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 30, 128)           16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 30, 1)             129       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 30, 1)             0         \n",
      "=================================================================\n",
      "Total params: 2,183,553\n",
      "Trainable params: 2,183,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstmgan = LSTMGAN()\n",
    "#lstmgan.train(epochs=1000, batch_size=20, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 6.101748, acc.: 50.00%] [G loss: 7.514967]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 5.132453, acc.: 50.00%] [G loss: 6.740389]\n",
      "2 [D loss: 4.910282, acc.: 50.00%] [G loss: 6.310653]\n",
      "3 [D loss: 4.745870, acc.: 50.00%] [G loss: 6.013654]\n",
      "4 [D loss: 4.046610, acc.: 50.00%] [G loss: 5.735656]\n",
      "5 [D loss: 2.950385, acc.: 50.00%] [G loss: 5.653929]\n",
      "6 [D loss: 2.887361, acc.: 50.00%] [G loss: 5.367849]\n",
      "7 [D loss: 2.903033, acc.: 50.00%] [G loss: 5.507888]\n",
      "8 [D loss: 3.119380, acc.: 50.00%] [G loss: 5.201163]\n",
      "9 [D loss: 3.097181, acc.: 50.00%] [G loss: 5.288337]\n",
      "10 [D loss: 3.047255, acc.: 50.00%] [G loss: 5.212106]\n",
      "11 [D loss: 2.716779, acc.: 50.00%] [G loss: 5.229543]\n",
      "12 [D loss: 2.709215, acc.: 50.00%] [G loss: 5.125268]\n",
      "13 [D loss: 2.668502, acc.: 50.00%] [G loss: 4.935365]\n",
      "14 [D loss: 2.585671, acc.: 50.00%] [G loss: 4.929595]\n",
      "15 [D loss: 2.655252, acc.: 50.00%] [G loss: 4.966196]\n",
      "16 [D loss: 2.585852, acc.: 50.00%] [G loss: 5.004762]\n",
      "17 [D loss: 2.670637, acc.: 50.00%] [G loss: 4.934727]\n",
      "18 [D loss: 2.527434, acc.: 50.00%] [G loss: 4.886664]\n",
      "19 [D loss: 2.478618, acc.: 50.00%] [G loss: 4.804044]\n",
      "20 [D loss: 2.456681, acc.: 50.00%] [G loss: 4.774866]\n",
      "21 [D loss: 2.435376, acc.: 50.00%] [G loss: 4.945293]\n",
      "22 [D loss: 2.450924, acc.: 50.00%] [G loss: 4.888076]\n",
      "23 [D loss: 2.444735, acc.: 50.00%] [G loss: 4.723290]\n",
      "24 [D loss: 2.420016, acc.: 50.00%] [G loss: 4.714792]\n",
      "25 [D loss: 2.406358, acc.: 50.00%] [G loss: 4.637084]\n",
      "26 [D loss: 2.402122, acc.: 50.00%] [G loss: 4.617442]\n",
      "27 [D loss: 2.392889, acc.: 50.00%] [G loss: 4.725773]\n",
      "28 [D loss: 2.392859, acc.: 50.00%] [G loss: 4.519695]\n",
      "29 [D loss: 2.347537, acc.: 50.00%] [G loss: 4.589879]\n",
      "30 [D loss: 2.330416, acc.: 50.00%] [G loss: 4.609462]\n",
      "31 [D loss: 2.311893, acc.: 50.00%] [G loss: 4.550602]\n",
      "32 [D loss: 2.344553, acc.: 50.00%] [G loss: 4.570443]\n",
      "33 [D loss: 2.266775, acc.: 50.00%] [G loss: 4.553239]\n",
      "34 [D loss: 2.297054, acc.: 50.00%] [G loss: 4.459099]\n",
      "35 [D loss: 2.175951, acc.: 50.00%] [G loss: 4.462895]\n",
      "36 [D loss: 2.287070, acc.: 50.00%] [G loss: 4.473598]\n",
      "37 [D loss: 2.243917, acc.: 50.00%] [G loss: 4.422997]\n",
      "38 [D loss: 2.257818, acc.: 50.00%] [G loss: 4.368061]\n",
      "39 [D loss: 2.197390, acc.: 50.00%] [G loss: 4.281114]\n",
      "40 [D loss: 2.234202, acc.: 50.00%] [G loss: 4.373978]\n",
      "41 [D loss: 2.235343, acc.: 50.00%] [G loss: 4.266287]\n",
      "42 [D loss: 2.226559, acc.: 50.00%] [G loss: 4.350802]\n",
      "43 [D loss: 2.193010, acc.: 50.00%] [G loss: 4.276599]\n",
      "44 [D loss: 2.233248, acc.: 50.00%] [G loss: 4.344754]\n",
      "45 [D loss: 2.215014, acc.: 50.00%] [G loss: 4.321961]\n",
      "46 [D loss: 2.171088, acc.: 50.00%] [G loss: 4.206334]\n",
      "47 [D loss: 2.230133, acc.: 50.00%] [G loss: 4.311224]\n",
      "48 [D loss: 2.173673, acc.: 50.00%] [G loss: 4.310809]\n",
      "49 [D loss: 2.137142, acc.: 50.00%] [G loss: 4.221293]\n",
      "50 [D loss: 2.113770, acc.: 50.00%] [G loss: 4.242261]\n",
      "51 [D loss: 2.134032, acc.: 50.00%] [G loss: 4.114368]\n",
      "52 [D loss: 2.133289, acc.: 50.00%] [G loss: 4.079951]\n",
      "53 [D loss: 2.124341, acc.: 50.00%] [G loss: 4.111106]\n",
      "54 [D loss: 2.132795, acc.: 50.00%] [G loss: 4.107394]\n",
      "55 [D loss: 2.149701, acc.: 50.00%] [G loss: 4.059917]\n",
      "56 [D loss: 2.099026, acc.: 50.00%] [G loss: 4.142654]\n",
      "57 [D loss: 2.081643, acc.: 50.00%] [G loss: 4.114527]\n",
      "58 [D loss: 2.078851, acc.: 50.00%] [G loss: 4.138769]\n",
      "59 [D loss: 2.033524, acc.: 50.00%] [G loss: 4.093337]\n",
      "60 [D loss: 2.045406, acc.: 50.00%] [G loss: 4.040393]\n",
      "61 [D loss: 2.042364, acc.: 50.00%] [G loss: 3.994685]\n",
      "62 [D loss: 2.045798, acc.: 50.00%] [G loss: 3.942783]\n",
      "63 [D loss: 2.117194, acc.: 50.00%] [G loss: 3.953836]\n",
      "64 [D loss: 2.076812, acc.: 50.00%] [G loss: 4.136994]\n",
      "65 [D loss: 2.047774, acc.: 50.00%] [G loss: 4.100255]\n",
      "66 [D loss: 2.049948, acc.: 50.00%] [G loss: 3.978975]\n",
      "67 [D loss: 1.999806, acc.: 50.00%] [G loss: 3.913168]\n",
      "68 [D loss: 2.027941, acc.: 50.00%] [G loss: 3.921512]\n",
      "69 [D loss: 2.054580, acc.: 50.00%] [G loss: 4.063802]\n",
      "70 [D loss: 2.034390, acc.: 50.00%] [G loss: 3.925775]\n",
      "71 [D loss: 2.076354, acc.: 50.00%] [G loss: 3.968528]\n",
      "72 [D loss: 2.029265, acc.: 50.00%] [G loss: 3.980126]\n",
      "73 [D loss: 2.033358, acc.: 50.00%] [G loss: 3.941303]\n",
      "74 [D loss: 2.000486, acc.: 50.00%] [G loss: 3.909173]\n",
      "75 [D loss: 1.992006, acc.: 50.00%] [G loss: 3.921308]\n",
      "76 [D loss: 2.010124, acc.: 50.00%] [G loss: 3.853854]\n",
      "77 [D loss: 2.012484, acc.: 50.00%] [G loss: 3.872543]\n",
      "78 [D loss: 2.031966, acc.: 50.00%] [G loss: 3.820393]\n",
      "79 [D loss: 1.967330, acc.: 50.00%] [G loss: 3.766265]\n",
      "80 [D loss: 1.958751, acc.: 50.00%] [G loss: 3.818135]\n",
      "81 [D loss: 1.954861, acc.: 50.00%] [G loss: 3.871852]\n",
      "82 [D loss: 2.015558, acc.: 50.00%] [G loss: 3.760208]\n",
      "83 [D loss: 1.999811, acc.: 50.00%] [G loss: 3.808803]\n",
      "84 [D loss: 1.974550, acc.: 50.00%] [G loss: 3.792452]\n",
      "85 [D loss: 1.967613, acc.: 50.00%] [G loss: 3.801036]\n",
      "86 [D loss: 1.984840, acc.: 50.00%] [G loss: 3.757011]\n",
      "87 [D loss: 1.946396, acc.: 50.00%] [G loss: 3.707893]\n",
      "88 [D loss: 1.906033, acc.: 50.00%] [G loss: 3.780166]\n",
      "89 [D loss: 1.897317, acc.: 50.00%] [G loss: 3.663173]\n",
      "90 [D loss: 1.941326, acc.: 50.00%] [G loss: 3.842385]\n",
      "91 [D loss: 1.918459, acc.: 50.00%] [G loss: 3.733038]\n",
      "92 [D loss: 1.907847, acc.: 50.00%] [G loss: 3.767918]\n",
      "93 [D loss: 1.951761, acc.: 50.00%] [G loss: 3.689769]\n",
      "94 [D loss: 1.902591, acc.: 50.00%] [G loss: 3.637608]\n",
      "95 [D loss: 1.900023, acc.: 50.00%] [G loss: 3.559857]\n",
      "96 [D loss: 1.920837, acc.: 50.00%] [G loss: 3.631628]\n",
      "97 [D loss: 1.904712, acc.: 50.00%] [G loss: 3.691624]\n",
      "98 [D loss: 1.891019, acc.: 50.00%] [G loss: 3.605439]\n",
      "99 [D loss: 1.897872, acc.: 50.00%] [G loss: 3.580791]\n",
      "100 [D loss: 1.879701, acc.: 50.00%] [G loss: 3.497093]\n",
      "101 [D loss: 1.881811, acc.: 50.00%] [G loss: 3.531861]\n",
      "102 [D loss: 1.854183, acc.: 50.00%] [G loss: 3.384707]\n",
      "103 [D loss: 1.890241, acc.: 50.00%] [G loss: 4.729285]\n",
      "104 [D loss: 1.920943, acc.: 50.00%] [G loss: 3.299024]\n",
      "105 [D loss: 1.841626, acc.: 50.00%] [G loss: 4.086074]\n",
      "106 [D loss: 1.956845, acc.: 47.50%] [G loss: 4.437295]\n",
      "107 [D loss: 1.833046, acc.: 50.00%] [G loss: 5.638888]\n",
      "108 [D loss: 1.845732, acc.: 50.00%] [G loss: 7.607990]\n",
      "109 [D loss: 1.899950, acc.: 50.00%] [G loss: 5.875438]\n",
      "110 [D loss: 1.847862, acc.: 50.00%] [G loss: 5.321601]\n",
      "111 [D loss: 1.906618, acc.: 50.00%] [G loss: 5.689147]\n",
      "112 [D loss: 1.902093, acc.: 50.00%] [G loss: 6.728982]\n",
      "113 [D loss: 1.864837, acc.: 47.50%] [G loss: 7.001287]\n",
      "114 [D loss: 1.953810, acc.: 40.00%] [G loss: 5.311012]\n",
      "115 [D loss: 1.945762, acc.: 45.00%] [G loss: 5.404455]\n",
      "116 [D loss: 1.821447, acc.: 50.00%] [G loss: 7.319823]\n",
      "117 [D loss: 1.895584, acc.: 42.50%] [G loss: 9.267946]\n",
      "118 [D loss: 1.812794, acc.: 50.00%] [G loss: 9.244837]\n",
      "119 [D loss: 1.953697, acc.: 45.00%] [G loss: 5.671239]\n",
      "120 [D loss: 1.936348, acc.: 45.00%] [G loss: 9.553007]\n",
      "121 [D loss: 1.843446, acc.: 50.00%] [G loss: 8.902441]\n",
      "122 [D loss: 1.841142, acc.: 47.50%] [G loss: 8.294525]\n",
      "123 [D loss: 1.887226, acc.: 47.50%] [G loss: 5.829268]\n",
      "124 [D loss: 1.835311, acc.: 50.00%] [G loss: 8.884997]\n",
      "125 [D loss: 1.847406, acc.: 50.00%] [G loss: 8.207689]\n",
      "126 [D loss: 1.894288, acc.: 50.00%] [G loss: 10.654703]\n",
      "127 [D loss: 1.857775, acc.: 50.00%] [G loss: 9.639431]\n",
      "128 [D loss: 1.834608, acc.: 47.50%] [G loss: 8.090158]\n",
      "129 [D loss: 1.842708, acc.: 47.50%] [G loss: 8.347872]\n",
      "130 [D loss: 1.834993, acc.: 47.50%] [G loss: 7.691405]\n",
      "131 [D loss: 1.819988, acc.: 50.00%] [G loss: 10.559718]\n",
      "132 [D loss: 1.879521, acc.: 47.50%] [G loss: 9.649522]\n",
      "133 [D loss: 1.817994, acc.: 47.50%] [G loss: 8.111203]\n",
      "134 [D loss: 1.798562, acc.: 50.00%] [G loss: 7.279236]\n",
      "135 [D loss: 1.805034, acc.: 50.00%] [G loss: 9.741404]\n",
      "136 [D loss: 1.815884, acc.: 50.00%] [G loss: 10.235915]\n",
      "137 [D loss: 1.813515, acc.: 50.00%] [G loss: 10.293282]\n",
      "138 [D loss: 1.783053, acc.: 50.00%] [G loss: 5.439378]\n",
      "139 [D loss: 1.849047, acc.: 50.00%] [G loss: 8.956317]\n",
      "140 [D loss: 1.741607, acc.: 50.00%] [G loss: 8.363987]\n",
      "141 [D loss: 1.791990, acc.: 50.00%] [G loss: 7.636302]\n",
      "142 [D loss: 1.761331, acc.: 50.00%] [G loss: 8.372581]\n",
      "143 [D loss: 1.792789, acc.: 50.00%] [G loss: 6.184580]\n",
      "144 [D loss: 1.795130, acc.: 50.00%] [G loss: 8.416661]\n",
      "145 [D loss: 1.849275, acc.: 50.00%] [G loss: 8.599077]\n",
      "146 [D loss: 1.718380, acc.: 50.00%] [G loss: 8.284546]\n",
      "147 [D loss: 1.740517, acc.: 50.00%] [G loss: 8.658236]\n",
      "148 [D loss: 1.769921, acc.: 50.00%] [G loss: 7.222474]\n",
      "149 [D loss: 1.738736, acc.: 50.00%] [G loss: 9.110997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 [D loss: 1.735262, acc.: 50.00%] [G loss: 7.624818]\n",
      "151 [D loss: 1.749480, acc.: 50.00%] [G loss: 8.775644]\n",
      "152 [D loss: 1.734798, acc.: 50.00%] [G loss: 7.827512]\n",
      "153 [D loss: 1.750500, acc.: 50.00%] [G loss: 8.997293]\n",
      "154 [D loss: 1.752293, acc.: 50.00%] [G loss: 9.561164]\n",
      "155 [D loss: 1.780751, acc.: 50.00%] [G loss: 9.917506]\n",
      "156 [D loss: 1.711393, acc.: 50.00%] [G loss: 5.272920]\n",
      "157 [D loss: 1.772405, acc.: 50.00%] [G loss: 7.210542]\n",
      "158 [D loss: 1.703607, acc.: 50.00%] [G loss: 6.762200]\n",
      "159 [D loss: 1.702731, acc.: 50.00%] [G loss: 5.880547]\n",
      "160 [D loss: 1.723889, acc.: 50.00%] [G loss: 7.463267]\n",
      "161 [D loss: 1.694720, acc.: 50.00%] [G loss: 7.343556]\n",
      "162 [D loss: 1.741537, acc.: 50.00%] [G loss: 10.002405]\n",
      "163 [D loss: 1.701667, acc.: 50.00%] [G loss: 7.512979]\n",
      "164 [D loss: 1.696527, acc.: 50.00%] [G loss: 7.023363]\n",
      "165 [D loss: 1.746940, acc.: 50.00%] [G loss: 7.678383]\n",
      "166 [D loss: 1.685922, acc.: 50.00%] [G loss: 8.758777]\n",
      "167 [D loss: 1.657389, acc.: 50.00%] [G loss: 6.844204]\n",
      "168 [D loss: 1.686117, acc.: 50.00%] [G loss: 7.214269]\n",
      "169 [D loss: 1.690402, acc.: 50.00%] [G loss: 7.179448]\n",
      "170 [D loss: 1.740294, acc.: 50.00%] [G loss: 7.157758]\n",
      "171 [D loss: 1.665289, acc.: 50.00%] [G loss: 4.179426]\n",
      "172 [D loss: 1.692120, acc.: 50.00%] [G loss: 5.032138]\n",
      "173 [D loss: 1.698539, acc.: 50.00%] [G loss: 7.454835]\n",
      "174 [D loss: 1.732045, acc.: 50.00%] [G loss: 6.683280]\n",
      "175 [D loss: 1.689656, acc.: 50.00%] [G loss: 8.028712]\n",
      "176 [D loss: 1.685622, acc.: 50.00%] [G loss: 8.044891]\n",
      "177 [D loss: 1.637401, acc.: 50.00%] [G loss: 6.139109]\n",
      "178 [D loss: 1.687592, acc.: 50.00%] [G loss: 8.169266]\n",
      "179 [D loss: 1.647659, acc.: 50.00%] [G loss: 8.924765]\n",
      "180 [D loss: 1.718339, acc.: 50.00%] [G loss: 4.234093]\n",
      "181 [D loss: 1.720757, acc.: 50.00%] [G loss: 5.589158]\n",
      "182 [D loss: 1.648585, acc.: 50.00%] [G loss: 6.420965]\n",
      "183 [D loss: 1.619054, acc.: 50.00%] [G loss: 6.783425]\n",
      "184 [D loss: 1.621012, acc.: 50.00%] [G loss: 7.577482]\n",
      "185 [D loss: 1.662377, acc.: 50.00%] [G loss: 7.968814]\n",
      "186 [D loss: 1.651633, acc.: 50.00%] [G loss: 5.850801]\n",
      "187 [D loss: 1.648580, acc.: 50.00%] [G loss: 5.850082]\n",
      "188 [D loss: 1.643892, acc.: 50.00%] [G loss: 6.774126]\n",
      "189 [D loss: 1.647745, acc.: 50.00%] [G loss: 4.184165]\n",
      "190 [D loss: 1.685669, acc.: 50.00%] [G loss: 4.305251]\n",
      "191 [D loss: 1.586040, acc.: 50.00%] [G loss: 7.057739]\n",
      "192 [D loss: 1.629901, acc.: 50.00%] [G loss: 9.015985]\n",
      "193 [D loss: 1.655699, acc.: 50.00%] [G loss: 7.290938]\n",
      "194 [D loss: 1.682330, acc.: 50.00%] [G loss: 7.833804]\n",
      "195 [D loss: 1.641712, acc.: 50.00%] [G loss: 4.302937]\n",
      "196 [D loss: 1.621193, acc.: 50.00%] [G loss: 9.471555]\n",
      "197 [D loss: 1.609419, acc.: 50.00%] [G loss: 9.995626]\n",
      "198 [D loss: 1.627988, acc.: 50.00%] [G loss: 8.976230]\n",
      "199 [D loss: 1.632408, acc.: 50.00%] [G loss: 7.009947]\n",
      "200 [D loss: 1.616319, acc.: 50.00%] [G loss: 6.745771]\n",
      "201 [D loss: 1.583305, acc.: 50.00%] [G loss: 7.307603]\n",
      "202 [D loss: 1.609138, acc.: 50.00%] [G loss: 7.069846]\n",
      "203 [D loss: 1.613179, acc.: 50.00%] [G loss: 6.924719]\n",
      "204 [D loss: 1.626490, acc.: 50.00%] [G loss: 7.473709]\n",
      "205 [D loss: 1.601165, acc.: 50.00%] [G loss: 9.317488]\n",
      "206 [D loss: 1.583291, acc.: 50.00%] [G loss: 9.308165]\n",
      "207 [D loss: 1.656408, acc.: 50.00%] [G loss: 5.734771]\n",
      "208 [D loss: 1.645693, acc.: 50.00%] [G loss: 7.397674]\n",
      "209 [D loss: 1.564990, acc.: 50.00%] [G loss: 9.599179]\n",
      "210 [D loss: 1.636008, acc.: 50.00%] [G loss: 8.208045]\n",
      "211 [D loss: 1.610251, acc.: 50.00%] [G loss: 7.219850]\n",
      "212 [D loss: 1.578719, acc.: 50.00%] [G loss: 6.605535]\n",
      "213 [D loss: 1.584291, acc.: 50.00%] [G loss: 5.433331]\n",
      "214 [D loss: 1.568259, acc.: 50.00%] [G loss: 7.462440]\n",
      "215 [D loss: 1.642876, acc.: 50.00%] [G loss: 6.674825]\n",
      "216 [D loss: 1.540578, acc.: 50.00%] [G loss: 7.909022]\n",
      "217 [D loss: 1.574149, acc.: 50.00%] [G loss: 8.288299]\n",
      "218 [D loss: 1.568081, acc.: 50.00%] [G loss: 6.757877]\n",
      "219 [D loss: 1.631451, acc.: 50.00%] [G loss: 5.143672]\n",
      "220 [D loss: 1.507333, acc.: 50.00%] [G loss: 7.029078]\n",
      "221 [D loss: 1.610693, acc.: 50.00%] [G loss: 7.780051]\n",
      "222 [D loss: 1.562250, acc.: 50.00%] [G loss: 5.700007]\n",
      "223 [D loss: 1.566311, acc.: 50.00%] [G loss: 8.120371]\n",
      "224 [D loss: 1.548324, acc.: 50.00%] [G loss: 7.209887]\n",
      "225 [D loss: 1.580151, acc.: 50.00%] [G loss: 5.923778]\n",
      "226 [D loss: 1.578923, acc.: 50.00%] [G loss: 3.238292]\n",
      "227 [D loss: 1.519582, acc.: 50.00%] [G loss: 7.193805]\n",
      "228 [D loss: 1.537712, acc.: 50.00%] [G loss: 7.251270]\n",
      "229 [D loss: 1.505792, acc.: 50.00%] [G loss: 6.219581]\n",
      "230 [D loss: 1.557689, acc.: 50.00%] [G loss: 5.054700]\n",
      "231 [D loss: 1.544668, acc.: 50.00%] [G loss: 6.499222]\n",
      "232 [D loss: 1.606022, acc.: 50.00%] [G loss: 7.022532]\n",
      "233 [D loss: 1.587353, acc.: 50.00%] [G loss: 6.367910]\n",
      "234 [D loss: 1.591501, acc.: 50.00%] [G loss: 6.861743]\n",
      "235 [D loss: 1.542357, acc.: 50.00%] [G loss: 6.347633]\n",
      "236 [D loss: 1.540504, acc.: 50.00%] [G loss: 3.524441]\n",
      "237 [D loss: 1.525508, acc.: 50.00%] [G loss: 5.827567]\n",
      "238 [D loss: 1.479467, acc.: 50.00%] [G loss: 3.961023]\n",
      "239 [D loss: 1.549505, acc.: 50.00%] [G loss: 7.514618]\n",
      "240 [D loss: 1.531928, acc.: 50.00%] [G loss: 6.452631]\n",
      "241 [D loss: 1.563935, acc.: 50.00%] [G loss: 6.777896]\n",
      "242 [D loss: 1.517101, acc.: 50.00%] [G loss: 6.543281]\n",
      "243 [D loss: 1.564407, acc.: 50.00%] [G loss: 5.968957]\n",
      "244 [D loss: 1.531932, acc.: 50.00%] [G loss: 5.125396]\n",
      "245 [D loss: 1.517245, acc.: 50.00%] [G loss: 5.246782]\n",
      "246 [D loss: 1.549235, acc.: 50.00%] [G loss: 5.386411]\n",
      "247 [D loss: 1.526611, acc.: 50.00%] [G loss: 7.326694]\n",
      "248 [D loss: 1.480981, acc.: 50.00%] [G loss: 6.606873]\n",
      "249 [D loss: 1.541249, acc.: 50.00%] [G loss: 4.594037]\n",
      "250 [D loss: 1.499995, acc.: 50.00%] [G loss: 5.381742]\n",
      "251 [D loss: 1.499622, acc.: 50.00%] [G loss: 4.926229]\n",
      "252 [D loss: 1.525205, acc.: 50.00%] [G loss: 4.374124]\n",
      "253 [D loss: 1.555657, acc.: 50.00%] [G loss: 5.299951]\n",
      "254 [D loss: 1.524194, acc.: 50.00%] [G loss: 4.860222]\n",
      "255 [D loss: 1.544525, acc.: 50.00%] [G loss: 4.756709]\n",
      "256 [D loss: 1.539732, acc.: 50.00%] [G loss: 3.925297]\n",
      "257 [D loss: 1.536899, acc.: 50.00%] [G loss: 4.161222]\n",
      "258 [D loss: 1.507305, acc.: 50.00%] [G loss: 4.602122]\n",
      "259 [D loss: 1.471159, acc.: 50.00%] [G loss: 5.906708]\n",
      "260 [D loss: 1.481882, acc.: 50.00%] [G loss: 4.575067]\n",
      "261 [D loss: 1.441461, acc.: 50.00%] [G loss: 4.307026]\n",
      "262 [D loss: 1.450872, acc.: 50.00%] [G loss: 4.298371]\n",
      "263 [D loss: 1.476890, acc.: 50.00%] [G loss: 3.941547]\n",
      "264 [D loss: 1.479756, acc.: 50.00%] [G loss: 4.126299]\n",
      "265 [D loss: 1.503611, acc.: 50.00%] [G loss: 4.154655]\n",
      "266 [D loss: 1.521526, acc.: 50.00%] [G loss: 3.211014]\n",
      "267 [D loss: 1.452257, acc.: 50.00%] [G loss: 3.079101]\n",
      "268 [D loss: 1.499414, acc.: 50.00%] [G loss: 3.268182]\n",
      "269 [D loss: 1.495737, acc.: 50.00%] [G loss: 3.937133]\n",
      "270 [D loss: 1.502739, acc.: 50.00%] [G loss: 3.253577]\n",
      "271 [D loss: 1.491707, acc.: 50.00%] [G loss: 3.547574]\n",
      "272 [D loss: 1.468179, acc.: 50.00%] [G loss: 3.238797]\n",
      "273 [D loss: 1.505578, acc.: 50.00%] [G loss: 3.138823]\n",
      "274 [D loss: 1.460016, acc.: 50.00%] [G loss: 3.101053]\n",
      "275 [D loss: 1.486020, acc.: 50.00%] [G loss: 3.019688]\n",
      "276 [D loss: 1.500893, acc.: 50.00%] [G loss: 3.189951]\n",
      "277 [D loss: 1.472054, acc.: 50.00%] [G loss: 3.199204]\n",
      "278 [D loss: 1.478386, acc.: 50.00%] [G loss: 3.241946]\n",
      "279 [D loss: 1.469373, acc.: 50.00%] [G loss: 2.966547]\n",
      "280 [D loss: 1.514602, acc.: 50.00%] [G loss: 2.940600]\n",
      "281 [D loss: 1.455706, acc.: 50.00%] [G loss: 2.900892]\n",
      "282 [D loss: 1.480640, acc.: 50.00%] [G loss: 3.000550]\n",
      "283 [D loss: 1.466470, acc.: 50.00%] [G loss: 3.625083]\n",
      "284 [D loss: 1.478235, acc.: 50.00%] [G loss: 2.951931]\n",
      "285 [D loss: 1.463059, acc.: 50.00%] [G loss: 3.014793]\n",
      "286 [D loss: 1.482814, acc.: 50.00%] [G loss: 3.176537]\n",
      "287 [D loss: 1.487664, acc.: 50.00%] [G loss: 2.865028]\n",
      "288 [D loss: 1.468817, acc.: 50.00%] [G loss: 3.055035]\n",
      "289 [D loss: 1.485975, acc.: 50.00%] [G loss: 3.519768]\n",
      "290 [D loss: 1.513702, acc.: 50.00%] [G loss: 2.801671]\n",
      "291 [D loss: 1.464974, acc.: 50.00%] [G loss: 3.012252]\n",
      "292 [D loss: 1.436246, acc.: 50.00%] [G loss: 2.701748]\n",
      "293 [D loss: 1.420702, acc.: 50.00%] [G loss: 2.768204]\n",
      "294 [D loss: 1.451219, acc.: 50.00%] [G loss: 2.838260]\n",
      "295 [D loss: 1.437968, acc.: 50.00%] [G loss: 2.778221]\n",
      "296 [D loss: 1.453194, acc.: 50.00%] [G loss: 2.676536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 [D loss: 1.454435, acc.: 50.00%] [G loss: 2.782886]\n",
      "298 [D loss: 1.455850, acc.: 50.00%] [G loss: 2.801986]\n",
      "299 [D loss: 1.475430, acc.: 50.00%] [G loss: 2.945810]\n",
      "300 [D loss: 1.449221, acc.: 50.00%] [G loss: 2.882969]\n",
      "301 [D loss: 1.420448, acc.: 50.00%] [G loss: 2.786689]\n",
      "302 [D loss: 1.458453, acc.: 50.00%] [G loss: 2.622108]\n",
      "303 [D loss: 1.479187, acc.: 50.00%] [G loss: 3.477879]\n",
      "304 [D loss: 1.470952, acc.: 50.00%] [G loss: 3.408309]\n",
      "305 [D loss: 1.456607, acc.: 50.00%] [G loss: 2.795179]\n",
      "306 [D loss: 1.463131, acc.: 50.00%] [G loss: 2.934780]\n",
      "307 [D loss: 1.425228, acc.: 50.00%] [G loss: 2.813641]\n",
      "308 [D loss: 1.464312, acc.: 50.00%] [G loss: 2.738955]\n",
      "309 [D loss: 1.443177, acc.: 50.00%] [G loss: 2.879166]\n",
      "310 [D loss: 1.434201, acc.: 50.00%] [G loss: 3.436242]\n",
      "311 [D loss: 1.433874, acc.: 50.00%] [G loss: 2.955583]\n",
      "312 [D loss: 1.422921, acc.: 50.00%] [G loss: 2.826053]\n",
      "313 [D loss: 1.459755, acc.: 50.00%] [G loss: 2.739660]\n",
      "314 [D loss: 1.414135, acc.: 50.00%] [G loss: 2.721358]\n",
      "315 [D loss: 1.422880, acc.: 50.00%] [G loss: 2.809787]\n",
      "316 [D loss: 1.423362, acc.: 50.00%] [G loss: 2.800426]\n",
      "317 [D loss: 1.416246, acc.: 50.00%] [G loss: 2.873001]\n",
      "318 [D loss: 1.413856, acc.: 50.00%] [G loss: 2.723010]\n",
      "319 [D loss: 1.406534, acc.: 50.00%] [G loss: 2.778047]\n",
      "320 [D loss: 1.388812, acc.: 50.00%] [G loss: 2.888883]\n",
      "321 [D loss: 1.418038, acc.: 50.00%] [G loss: 2.868797]\n",
      "322 [D loss: 1.400643, acc.: 50.00%] [G loss: 2.869894]\n",
      "323 [D loss: 1.389011, acc.: 50.00%] [G loss: 2.763368]\n",
      "324 [D loss: 1.380908, acc.: 50.00%] [G loss: 2.741874]\n",
      "325 [D loss: 1.411222, acc.: 50.00%] [G loss: 2.794079]\n",
      "326 [D loss: 1.404791, acc.: 50.00%] [G loss: 2.755263]\n",
      "327 [D loss: 1.389459, acc.: 50.00%] [G loss: 2.837905]\n",
      "328 [D loss: 1.404345, acc.: 50.00%] [G loss: 2.840887]\n",
      "329 [D loss: 1.425232, acc.: 50.00%] [G loss: 2.708451]\n",
      "330 [D loss: 1.404036, acc.: 50.00%] [G loss: 2.653865]\n",
      "331 [D loss: 1.400991, acc.: 50.00%] [G loss: 2.829172]\n",
      "332 [D loss: 1.407004, acc.: 50.00%] [G loss: 2.742455]\n",
      "333 [D loss: 1.419330, acc.: 50.00%] [G loss: 2.815083]\n",
      "334 [D loss: 1.419196, acc.: 50.00%] [G loss: 2.651004]\n",
      "335 [D loss: 1.390072, acc.: 50.00%] [G loss: 2.777347]\n",
      "336 [D loss: 1.427708, acc.: 50.00%] [G loss: 2.723481]\n",
      "337 [D loss: 1.329886, acc.: 50.00%] [G loss: 2.860788]\n",
      "338 [D loss: 1.402105, acc.: 50.00%] [G loss: 2.755700]\n",
      "339 [D loss: 1.444463, acc.: 50.00%] [G loss: 2.679275]\n",
      "340 [D loss: 1.371908, acc.: 50.00%] [G loss: 2.580538]\n",
      "341 [D loss: 1.397676, acc.: 50.00%] [G loss: 2.752919]\n",
      "342 [D loss: 1.393622, acc.: 50.00%] [G loss: 2.681667]\n",
      "343 [D loss: 1.381412, acc.: 50.00%] [G loss: 2.734036]\n",
      "344 [D loss: 1.417826, acc.: 50.00%] [G loss: 2.570108]\n",
      "345 [D loss: 1.362724, acc.: 50.00%] [G loss: 2.689432]\n",
      "346 [D loss: 1.392422, acc.: 50.00%] [G loss: 2.641925]\n",
      "347 [D loss: 1.347004, acc.: 50.00%] [G loss: 2.695704]\n",
      "348 [D loss: 1.425892, acc.: 50.00%] [G loss: 2.689383]\n",
      "349 [D loss: 1.373710, acc.: 50.00%] [G loss: 2.715842]\n",
      "350 [D loss: 1.379967, acc.: 50.00%] [G loss: 2.630665]\n",
      "351 [D loss: 1.411354, acc.: 50.00%] [G loss: 2.625068]\n",
      "352 [D loss: 1.436553, acc.: 50.00%] [G loss: 2.755586]\n",
      "353 [D loss: 1.359283, acc.: 50.00%] [G loss: 2.459913]\n",
      "354 [D loss: 1.397296, acc.: 50.00%] [G loss: 2.684795]\n",
      "355 [D loss: 1.411438, acc.: 50.00%] [G loss: 2.635540]\n",
      "356 [D loss: 1.364093, acc.: 50.00%] [G loss: 2.647765]\n",
      "357 [D loss: 1.378228, acc.: 50.00%] [G loss: 2.449486]\n",
      "358 [D loss: 1.406484, acc.: 50.00%] [G loss: 2.547457]\n",
      "359 [D loss: 1.399385, acc.: 50.00%] [G loss: 3.271954]\n",
      "360 [D loss: 1.377056, acc.: 50.00%] [G loss: 2.365868]\n",
      "361 [D loss: 1.415600, acc.: 50.00%] [G loss: 2.623659]\n",
      "362 [D loss: 1.390264, acc.: 50.00%] [G loss: 2.822023]\n",
      "363 [D loss: 1.385892, acc.: 50.00%] [G loss: 2.599724]\n",
      "364 [D loss: 1.360236, acc.: 50.00%] [G loss: 2.590559]\n",
      "365 [D loss: 1.407030, acc.: 50.00%] [G loss: 2.517091]\n",
      "366 [D loss: 1.323337, acc.: 50.00%] [G loss: 2.366862]\n",
      "367 [D loss: 1.409963, acc.: 50.00%] [G loss: 2.557582]\n",
      "368 [D loss: 1.403094, acc.: 50.00%] [G loss: 2.704615]\n",
      "369 [D loss: 1.390150, acc.: 50.00%] [G loss: 2.504794]\n",
      "370 [D loss: 1.353544, acc.: 50.00%] [G loss: 2.748667]\n",
      "371 [D loss: 1.353806, acc.: 50.00%] [G loss: 2.640116]\n",
      "372 [D loss: 1.423807, acc.: 50.00%] [G loss: 2.624343]\n",
      "373 [D loss: 1.352672, acc.: 50.00%] [G loss: 2.496117]\n",
      "374 [D loss: 1.402554, acc.: 50.00%] [G loss: 3.218380]\n",
      "375 [D loss: 1.374643, acc.: 50.00%] [G loss: 2.443202]\n",
      "376 [D loss: 1.352907, acc.: 50.00%] [G loss: 2.766303]\n",
      "377 [D loss: 1.351640, acc.: 50.00%] [G loss: 2.730015]\n",
      "378 [D loss: 1.373349, acc.: 50.00%] [G loss: 2.563733]\n",
      "379 [D loss: 1.366193, acc.: 50.00%] [G loss: 2.672881]\n",
      "380 [D loss: 1.347609, acc.: 50.00%] [G loss: 3.805557]\n",
      "381 [D loss: 1.386763, acc.: 50.00%] [G loss: 3.402122]\n",
      "382 [D loss: 1.361483, acc.: 50.00%] [G loss: 3.141600]\n",
      "383 [D loss: 1.357300, acc.: 50.00%] [G loss: 2.724014]\n",
      "384 [D loss: 1.347087, acc.: 50.00%] [G loss: 2.469428]\n",
      "385 [D loss: 1.313545, acc.: 50.00%] [G loss: 2.737915]\n",
      "386 [D loss: 1.399565, acc.: 50.00%] [G loss: 2.592197]\n",
      "387 [D loss: 1.344480, acc.: 50.00%] [G loss: 2.572262]\n",
      "388 [D loss: 1.374623, acc.: 50.00%] [G loss: 2.556299]\n",
      "389 [D loss: 1.352745, acc.: 50.00%] [G loss: 2.597051]\n",
      "390 [D loss: 1.309902, acc.: 50.00%] [G loss: 2.522572]\n",
      "391 [D loss: 1.352593, acc.: 50.00%] [G loss: 2.365691]\n",
      "392 [D loss: 1.352963, acc.: 50.00%] [G loss: 2.587748]\n",
      "393 [D loss: 1.346125, acc.: 50.00%] [G loss: 2.826270]\n",
      "394 [D loss: 1.330199, acc.: 50.00%] [G loss: 2.675780]\n",
      "395 [D loss: 1.342843, acc.: 50.00%] [G loss: 2.617087]\n",
      "396 [D loss: 1.294274, acc.: 50.00%] [G loss: 2.707869]\n",
      "397 [D loss: 1.309706, acc.: 50.00%] [G loss: 2.451991]\n",
      "398 [D loss: 1.342239, acc.: 50.00%] [G loss: 2.537823]\n",
      "399 [D loss: 1.354300, acc.: 50.00%] [G loss: 2.507067]\n",
      "400 [D loss: 1.293617, acc.: 50.00%] [G loss: 2.635809]\n",
      "401 [D loss: 1.304809, acc.: 50.00%] [G loss: 2.588188]\n",
      "402 [D loss: 1.345224, acc.: 50.00%] [G loss: 2.570734]\n",
      "403 [D loss: 1.352541, acc.: 50.00%] [G loss: 2.404417]\n",
      "404 [D loss: 1.323136, acc.: 50.00%] [G loss: 2.581868]\n",
      "405 [D loss: 1.323705, acc.: 50.00%] [G loss: 2.551851]\n",
      "406 [D loss: 1.320878, acc.: 50.00%] [G loss: 2.605226]\n",
      "407 [D loss: 1.336908, acc.: 50.00%] [G loss: 2.651736]\n",
      "408 [D loss: 1.355186, acc.: 50.00%] [G loss: 2.517909]\n",
      "409 [D loss: 1.321055, acc.: 50.00%] [G loss: 2.553946]\n",
      "410 [D loss: 1.337291, acc.: 50.00%] [G loss: 2.559334]\n",
      "411 [D loss: 1.322544, acc.: 50.00%] [G loss: 2.455829]\n",
      "412 [D loss: 1.316193, acc.: 50.00%] [G loss: 2.439003]\n",
      "413 [D loss: 1.336010, acc.: 50.00%] [G loss: 2.501099]\n",
      "414 [D loss: 1.294321, acc.: 50.00%] [G loss: 2.532192]\n",
      "415 [D loss: 1.256815, acc.: 50.00%] [G loss: 2.486721]\n",
      "416 [D loss: 1.294847, acc.: 50.00%] [G loss: 2.438811]\n",
      "417 [D loss: 1.297935, acc.: 50.00%] [G loss: 2.577844]\n",
      "418 [D loss: 1.309527, acc.: 50.00%] [G loss: 2.437778]\n",
      "419 [D loss: 1.321201, acc.: 50.00%] [G loss: 2.485555]\n",
      "420 [D loss: 1.332639, acc.: 50.00%] [G loss: 2.660430]\n",
      "421 [D loss: 1.314812, acc.: 50.00%] [G loss: 2.553038]\n",
      "422 [D loss: 1.328905, acc.: 50.00%] [G loss: 2.518942]\n",
      "423 [D loss: 1.327410, acc.: 50.00%] [G loss: 2.512507]\n",
      "424 [D loss: 1.300829, acc.: 50.00%] [G loss: 2.612407]\n",
      "425 [D loss: 1.295531, acc.: 50.00%] [G loss: 2.474871]\n",
      "426 [D loss: 1.293234, acc.: 50.00%] [G loss: 2.559673]\n",
      "427 [D loss: 1.287567, acc.: 50.00%] [G loss: 2.546631]\n",
      "428 [D loss: 1.274702, acc.: 50.00%] [G loss: 2.644908]\n",
      "429 [D loss: 1.287675, acc.: 50.00%] [G loss: 2.463992]\n",
      "430 [D loss: 1.306472, acc.: 50.00%] [G loss: 2.609285]\n",
      "431 [D loss: 1.339892, acc.: 50.00%] [G loss: 2.420806]\n",
      "432 [D loss: 1.293686, acc.: 50.00%] [G loss: 2.771174]\n",
      "433 [D loss: 1.281884, acc.: 50.00%] [G loss: 2.506658]\n",
      "434 [D loss: 1.361797, acc.: 50.00%] [G loss: 2.413337]\n",
      "435 [D loss: 1.309870, acc.: 50.00%] [G loss: 2.511192]\n",
      "436 [D loss: 1.275436, acc.: 50.00%] [G loss: 2.477769]\n",
      "437 [D loss: 1.293738, acc.: 50.00%] [G loss: 2.505617]\n",
      "438 [D loss: 1.281085, acc.: 50.00%] [G loss: 2.566790]\n",
      "439 [D loss: 1.314767, acc.: 50.00%] [G loss: 2.495497]\n",
      "440 [D loss: 1.263755, acc.: 50.00%] [G loss: 2.519892]\n",
      "441 [D loss: 1.314761, acc.: 50.00%] [G loss: 2.453694]\n",
      "442 [D loss: 1.351285, acc.: 50.00%] [G loss: 2.522631]\n",
      "443 [D loss: 1.269533, acc.: 50.00%] [G loss: 2.417619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444 [D loss: 1.236346, acc.: 50.00%] [G loss: 2.483456]\n",
      "445 [D loss: 1.257088, acc.: 50.00%] [G loss: 2.450534]\n",
      "446 [D loss: 1.320565, acc.: 50.00%] [G loss: 2.522973]\n",
      "447 [D loss: 1.301645, acc.: 50.00%] [G loss: 2.489039]\n",
      "448 [D loss: 1.236365, acc.: 50.00%] [G loss: 2.515432]\n",
      "449 [D loss: 1.310158, acc.: 50.00%] [G loss: 2.585560]\n",
      "450 [D loss: 1.298217, acc.: 50.00%] [G loss: 2.492786]\n",
      "451 [D loss: 1.256022, acc.: 50.00%] [G loss: 2.481633]\n",
      "452 [D loss: 1.273717, acc.: 50.00%] [G loss: 2.503231]\n",
      "453 [D loss: 1.237851, acc.: 50.00%] [G loss: 2.306347]\n",
      "454 [D loss: 1.286718, acc.: 50.00%] [G loss: 2.556925]\n",
      "455 [D loss: 1.232192, acc.: 50.00%] [G loss: 2.586794]\n",
      "456 [D loss: 1.268362, acc.: 50.00%] [G loss: 2.423305]\n",
      "457 [D loss: 1.291200, acc.: 50.00%] [G loss: 2.424138]\n",
      "458 [D loss: 1.263758, acc.: 50.00%] [G loss: 2.471231]\n",
      "459 [D loss: 1.298960, acc.: 50.00%] [G loss: 2.358971]\n",
      "460 [D loss: 1.252558, acc.: 50.00%] [G loss: 2.330488]\n",
      "461 [D loss: 1.311602, acc.: 50.00%] [G loss: 2.371684]\n",
      "462 [D loss: 1.303396, acc.: 50.00%] [G loss: 2.462260]\n",
      "463 [D loss: 1.231805, acc.: 50.00%] [G loss: 2.330205]\n",
      "464 [D loss: 1.221536, acc.: 50.00%] [G loss: 2.418186]\n",
      "465 [D loss: 1.264723, acc.: 50.00%] [G loss: 2.501617]\n",
      "466 [D loss: 1.263409, acc.: 50.00%] [G loss: 2.406383]\n",
      "467 [D loss: 1.299355, acc.: 50.00%] [G loss: 2.474887]\n",
      "468 [D loss: 1.257876, acc.: 50.00%] [G loss: 2.442580]\n",
      "469 [D loss: 1.245149, acc.: 50.00%] [G loss: 2.498157]\n",
      "470 [D loss: 1.265776, acc.: 50.00%] [G loss: 2.434442]\n",
      "471 [D loss: 1.262384, acc.: 50.00%] [G loss: 2.567403]\n",
      "472 [D loss: 1.295909, acc.: 50.00%] [G loss: 2.414936]\n",
      "473 [D loss: 1.301754, acc.: 50.00%] [G loss: 2.363205]\n",
      "474 [D loss: 1.264169, acc.: 50.00%] [G loss: 2.467197]\n",
      "475 [D loss: 1.241419, acc.: 50.00%] [G loss: 2.436432]\n",
      "476 [D loss: 1.248629, acc.: 50.00%] [G loss: 2.377657]\n",
      "477 [D loss: 1.261785, acc.: 50.00%] [G loss: 2.366811]\n",
      "478 [D loss: 1.253424, acc.: 50.00%] [G loss: 2.410211]\n",
      "479 [D loss: 1.284317, acc.: 50.00%] [G loss: 2.300946]\n",
      "480 [D loss: 1.253053, acc.: 50.00%] [G loss: 2.408778]\n",
      "481 [D loss: 1.248386, acc.: 50.00%] [G loss: 2.371414]\n",
      "482 [D loss: 1.269750, acc.: 50.00%] [G loss: 2.491700]\n",
      "483 [D loss: 1.276965, acc.: 50.00%] [G loss: 2.397534]\n",
      "484 [D loss: 1.290152, acc.: 50.00%] [G loss: 2.389861]\n",
      "485 [D loss: 1.237893, acc.: 50.00%] [G loss: 2.280665]\n",
      "486 [D loss: 1.262062, acc.: 50.00%] [G loss: 2.309456]\n",
      "487 [D loss: 1.209553, acc.: 50.00%] [G loss: 2.462043]\n",
      "488 [D loss: 1.271564, acc.: 50.00%] [G loss: 2.506901]\n",
      "489 [D loss: 1.241757, acc.: 50.00%] [G loss: 2.384727]\n",
      "490 [D loss: 1.249091, acc.: 50.00%] [G loss: 2.450735]\n",
      "491 [D loss: 1.259618, acc.: 50.00%] [G loss: 2.354293]\n",
      "492 [D loss: 1.187746, acc.: 50.00%] [G loss: 2.419510]\n",
      "493 [D loss: 1.230499, acc.: 50.00%] [G loss: 2.322057]\n",
      "494 [D loss: 1.236521, acc.: 50.00%] [G loss: 2.509617]\n",
      "495 [D loss: 1.244423, acc.: 50.00%] [G loss: 2.373150]\n",
      "496 [D loss: 1.258868, acc.: 50.00%] [G loss: 2.327679]\n",
      "497 [D loss: 1.200132, acc.: 50.00%] [G loss: 2.373573]\n",
      "498 [D loss: 1.304940, acc.: 50.00%] [G loss: 2.433728]\n",
      "499 [D loss: 1.206104, acc.: 50.00%] [G loss: 2.312375]\n",
      "500 [D loss: 1.173849, acc.: 50.00%] [G loss: 2.292216]\n",
      "501 [D loss: 1.230939, acc.: 50.00%] [G loss: 2.244508]\n",
      "502 [D loss: 1.245239, acc.: 50.00%] [G loss: 2.254128]\n",
      "503 [D loss: 1.239734, acc.: 50.00%] [G loss: 2.350893]\n",
      "504 [D loss: 1.272641, acc.: 50.00%] [G loss: 2.324274]\n",
      "505 [D loss: 1.233110, acc.: 50.00%] [G loss: 2.355023]\n",
      "506 [D loss: 1.225407, acc.: 50.00%] [G loss: 2.386730]\n",
      "507 [D loss: 1.249365, acc.: 50.00%] [G loss: 2.381985]\n",
      "508 [D loss: 1.238386, acc.: 50.00%] [G loss: 2.327788]\n",
      "509 [D loss: 1.197090, acc.: 50.00%] [G loss: 2.341311]\n",
      "510 [D loss: 1.220285, acc.: 50.00%] [G loss: 2.389475]\n",
      "511 [D loss: 1.211939, acc.: 50.00%] [G loss: 2.326494]\n",
      "512 [D loss: 1.242182, acc.: 50.00%] [G loss: 2.286914]\n",
      "513 [D loss: 1.274040, acc.: 50.00%] [G loss: 2.334596]\n",
      "514 [D loss: 1.220990, acc.: 50.00%] [G loss: 2.333179]\n",
      "515 [D loss: 1.216679, acc.: 50.00%] [G loss: 2.368391]\n",
      "516 [D loss: 1.243758, acc.: 50.00%] [G loss: 2.319287]\n",
      "517 [D loss: 1.229714, acc.: 50.00%] [G loss: 2.268394]\n",
      "518 [D loss: 1.217972, acc.: 50.00%] [G loss: 2.971788]\n",
      "519 [D loss: 1.228422, acc.: 50.00%] [G loss: 2.199039]\n",
      "520 [D loss: 1.164917, acc.: 50.00%] [G loss: 2.408845]\n",
      "521 [D loss: 1.238226, acc.: 50.00%] [G loss: 2.324130]\n",
      "522 [D loss: 1.248895, acc.: 50.00%] [G loss: 2.269915]\n",
      "523 [D loss: 1.196332, acc.: 50.00%] [G loss: 2.367604]\n",
      "524 [D loss: 1.194092, acc.: 50.00%] [G loss: 2.236501]\n",
      "525 [D loss: 1.218976, acc.: 50.00%] [G loss: 2.340953]\n",
      "526 [D loss: 1.304754, acc.: 50.00%] [G loss: 2.362687]\n",
      "527 [D loss: 1.176586, acc.: 50.00%] [G loss: 2.454938]\n",
      "528 [D loss: 1.229437, acc.: 50.00%] [G loss: 2.312767]\n",
      "529 [D loss: 1.233096, acc.: 50.00%] [G loss: 2.262788]\n",
      "530 [D loss: 1.179869, acc.: 50.00%] [G loss: 2.476346]\n",
      "531 [D loss: 1.226832, acc.: 50.00%] [G loss: 2.295228]\n",
      "532 [D loss: 1.205517, acc.: 50.00%] [G loss: 2.349608]\n",
      "533 [D loss: 1.220987, acc.: 50.00%] [G loss: 2.351464]\n",
      "534 [D loss: 1.255402, acc.: 50.00%] [G loss: 2.383101]\n",
      "535 [D loss: 1.184047, acc.: 50.00%] [G loss: 2.261069]\n",
      "536 [D loss: 1.171898, acc.: 50.00%] [G loss: 2.309021]\n",
      "537 [D loss: 1.227898, acc.: 50.00%] [G loss: 2.319189]\n",
      "538 [D loss: 1.205146, acc.: 50.00%] [G loss: 2.188142]\n",
      "539 [D loss: 1.258699, acc.: 50.00%] [G loss: 2.296443]\n",
      "540 [D loss: 1.191595, acc.: 50.00%] [G loss: 2.278069]\n",
      "541 [D loss: 1.162757, acc.: 50.00%] [G loss: 2.345729]\n",
      "542 [D loss: 1.187487, acc.: 50.00%] [G loss: 2.278455]\n",
      "543 [D loss: 1.179304, acc.: 50.00%] [G loss: 2.294461]\n",
      "544 [D loss: 1.192152, acc.: 50.00%] [G loss: 2.371390]\n",
      "545 [D loss: 1.201618, acc.: 50.00%] [G loss: 2.249163]\n",
      "546 [D loss: 1.178076, acc.: 50.00%] [G loss: 2.354249]\n",
      "547 [D loss: 1.222211, acc.: 50.00%] [G loss: 2.303642]\n",
      "548 [D loss: 1.196442, acc.: 50.00%] [G loss: 2.276126]\n",
      "549 [D loss: 1.226279, acc.: 50.00%] [G loss: 2.313709]\n",
      "550 [D loss: 1.192158, acc.: 50.00%] [G loss: 2.345218]\n",
      "551 [D loss: 1.195787, acc.: 50.00%] [G loss: 2.372834]\n",
      "552 [D loss: 1.217779, acc.: 50.00%] [G loss: 2.300351]\n",
      "553 [D loss: 1.187250, acc.: 50.00%] [G loss: 2.257167]\n",
      "554 [D loss: 1.204167, acc.: 50.00%] [G loss: 2.193013]\n",
      "555 [D loss: 1.157116, acc.: 50.00%] [G loss: 2.164642]\n",
      "556 [D loss: 1.151087, acc.: 50.00%] [G loss: 2.376359]\n",
      "557 [D loss: 1.171542, acc.: 50.00%] [G loss: 2.190059]\n",
      "558 [D loss: 1.214924, acc.: 50.00%] [G loss: 2.210904]\n",
      "559 [D loss: 1.190660, acc.: 50.00%] [G loss: 2.241783]\n",
      "560 [D loss: 1.201869, acc.: 50.00%] [G loss: 2.319259]\n",
      "561 [D loss: 1.159151, acc.: 50.00%] [G loss: 2.356619]\n",
      "562 [D loss: 1.147417, acc.: 50.00%] [G loss: 2.171464]\n",
      "563 [D loss: 1.179708, acc.: 50.00%] [G loss: 2.196824]\n",
      "564 [D loss: 1.199672, acc.: 50.00%] [G loss: 2.327270]\n",
      "565 [D loss: 1.184753, acc.: 50.00%] [G loss: 2.247920]\n",
      "566 [D loss: 1.197981, acc.: 50.00%] [G loss: 2.136059]\n",
      "567 [D loss: 1.208486, acc.: 50.00%] [G loss: 2.219662]\n",
      "568 [D loss: 1.222459, acc.: 50.00%] [G loss: 2.289874]\n",
      "569 [D loss: 1.167416, acc.: 50.00%] [G loss: 2.183589]\n",
      "570 [D loss: 1.188976, acc.: 50.00%] [G loss: 2.235222]\n",
      "571 [D loss: 1.191841, acc.: 50.00%] [G loss: 2.184203]\n",
      "572 [D loss: 1.203129, acc.: 50.00%] [G loss: 2.217458]\n",
      "573 [D loss: 1.178418, acc.: 50.00%] [G loss: 2.264835]\n",
      "574 [D loss: 1.191909, acc.: 50.00%] [G loss: 2.156649]\n",
      "575 [D loss: 1.222589, acc.: 50.00%] [G loss: 2.261643]\n",
      "576 [D loss: 1.222869, acc.: 50.00%] [G loss: 2.191889]\n",
      "577 [D loss: 1.168023, acc.: 50.00%] [G loss: 2.202418]\n",
      "578 [D loss: 1.184289, acc.: 50.00%] [G loss: 2.277820]\n",
      "579 [D loss: 1.135743, acc.: 50.00%] [G loss: 2.292811]\n",
      "580 [D loss: 1.133719, acc.: 50.00%] [G loss: 2.112596]\n",
      "581 [D loss: 1.177439, acc.: 50.00%] [G loss: 2.094015]\n",
      "582 [D loss: 1.198029, acc.: 50.00%] [G loss: 2.193603]\n",
      "583 [D loss: 1.130768, acc.: 50.00%] [G loss: 2.320697]\n",
      "584 [D loss: 1.243562, acc.: 50.00%] [G loss: 2.138251]\n",
      "585 [D loss: 1.193665, acc.: 50.00%] [G loss: 2.151715]\n",
      "586 [D loss: 1.121659, acc.: 50.00%] [G loss: 2.278952]\n",
      "587 [D loss: 1.197862, acc.: 50.00%] [G loss: 2.304559]\n",
      "588 [D loss: 1.213930, acc.: 50.00%] [G loss: 2.183680]\n",
      "589 [D loss: 1.197022, acc.: 50.00%] [G loss: 2.200519]\n",
      "590 [D loss: 1.158806, acc.: 50.00%] [G loss: 2.250760]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591 [D loss: 1.157812, acc.: 50.00%] [G loss: 2.246437]\n",
      "592 [D loss: 1.211756, acc.: 50.00%] [G loss: 2.277275]\n",
      "593 [D loss: 1.157715, acc.: 50.00%] [G loss: 2.228818]\n",
      "594 [D loss: 1.166079, acc.: 50.00%] [G loss: 2.135110]\n",
      "595 [D loss: 1.181915, acc.: 50.00%] [G loss: 2.205032]\n",
      "596 [D loss: 1.131316, acc.: 50.00%] [G loss: 2.261668]\n",
      "597 [D loss: 1.188215, acc.: 50.00%] [G loss: 2.165972]\n",
      "598 [D loss: 1.156024, acc.: 50.00%] [G loss: 2.147977]\n",
      "599 [D loss: 1.156897, acc.: 50.00%] [G loss: 2.301651]\n",
      "600 [D loss: 1.151695, acc.: 50.00%] [G loss: 2.224119]\n",
      "601 [D loss: 1.121103, acc.: 50.00%] [G loss: 2.197582]\n",
      "602 [D loss: 1.119603, acc.: 50.00%] [G loss: 1.992325]\n",
      "603 [D loss: 1.216587, acc.: 50.00%] [G loss: 2.075628]\n",
      "604 [D loss: 1.155265, acc.: 50.00%] [G loss: 2.122125]\n",
      "605 [D loss: 1.138401, acc.: 50.00%] [G loss: 2.102549]\n",
      "606 [D loss: 1.128001, acc.: 50.00%] [G loss: 2.181101]\n",
      "607 [D loss: 1.110466, acc.: 50.00%] [G loss: 2.138766]\n",
      "608 [D loss: 1.114255, acc.: 50.00%] [G loss: 2.110094]\n",
      "609 [D loss: 1.102175, acc.: 50.00%] [G loss: 2.147706]\n",
      "610 [D loss: 1.132063, acc.: 50.00%] [G loss: 2.201623]\n",
      "611 [D loss: 1.177473, acc.: 50.00%] [G loss: 2.087355]\n",
      "612 [D loss: 1.159415, acc.: 50.00%] [G loss: 2.215286]\n",
      "613 [D loss: 1.161935, acc.: 50.00%] [G loss: 2.150789]\n",
      "614 [D loss: 1.191778, acc.: 50.00%] [G loss: 2.208877]\n",
      "615 [D loss: 1.212780, acc.: 50.00%] [G loss: 2.106899]\n",
      "616 [D loss: 1.144590, acc.: 50.00%] [G loss: 2.363276]\n",
      "617 [D loss: 1.116304, acc.: 50.00%] [G loss: 2.148636]\n",
      "618 [D loss: 1.129573, acc.: 50.00%] [G loss: 2.172611]\n",
      "619 [D loss: 1.135966, acc.: 50.00%] [G loss: 2.114650]\n",
      "620 [D loss: 1.132119, acc.: 50.00%] [G loss: 2.247079]\n",
      "621 [D loss: 1.116638, acc.: 50.00%] [G loss: 2.122943]\n",
      "622 [D loss: 1.173311, acc.: 50.00%] [G loss: 2.194980]\n",
      "623 [D loss: 1.191853, acc.: 50.00%] [G loss: 2.087252]\n",
      "624 [D loss: 1.116588, acc.: 50.00%] [G loss: 2.156386]\n",
      "625 [D loss: 1.082578, acc.: 50.00%] [G loss: 2.114620]\n",
      "626 [D loss: 1.169208, acc.: 50.00%] [G loss: 2.167247]\n",
      "627 [D loss: 1.153626, acc.: 50.00%] [G loss: 2.171237]\n",
      "628 [D loss: 1.147082, acc.: 50.00%] [G loss: 2.068604]\n",
      "629 [D loss: 1.144320, acc.: 50.00%] [G loss: 2.178510]\n",
      "630 [D loss: 1.099691, acc.: 50.00%] [G loss: 2.058051]\n",
      "631 [D loss: 1.137764, acc.: 50.00%] [G loss: 2.294807]\n",
      "632 [D loss: 1.143332, acc.: 50.00%] [G loss: 2.119796]\n",
      "633 [D loss: 1.153526, acc.: 50.00%] [G loss: 2.173760]\n",
      "634 [D loss: 1.137612, acc.: 50.00%] [G loss: 2.100872]\n",
      "635 [D loss: 1.175038, acc.: 50.00%] [G loss: 2.086532]\n",
      "636 [D loss: 1.129526, acc.: 50.00%] [G loss: 2.158837]\n",
      "637 [D loss: 1.119178, acc.: 50.00%] [G loss: 2.113368]\n",
      "638 [D loss: 1.161603, acc.: 50.00%] [G loss: 2.205791]\n",
      "639 [D loss: 1.163168, acc.: 50.00%] [G loss: 2.105233]\n",
      "640 [D loss: 1.168456, acc.: 50.00%] [G loss: 2.129441]\n",
      "641 [D loss: 1.137445, acc.: 50.00%] [G loss: 2.207743]\n",
      "642 [D loss: 1.148021, acc.: 50.00%] [G loss: 2.110223]\n",
      "643 [D loss: 1.158620, acc.: 50.00%] [G loss: 2.108615]\n",
      "644 [D loss: 1.122323, acc.: 50.00%] [G loss: 2.092510]\n",
      "645 [D loss: 1.151242, acc.: 50.00%] [G loss: 2.135973]\n",
      "646 [D loss: 1.142415, acc.: 50.00%] [G loss: 2.187982]\n",
      "647 [D loss: 1.111586, acc.: 50.00%] [G loss: 2.146838]\n",
      "648 [D loss: 1.116263, acc.: 50.00%] [G loss: 2.060387]\n",
      "649 [D loss: 1.197293, acc.: 50.00%] [G loss: 2.078257]\n",
      "650 [D loss: 1.118856, acc.: 50.00%] [G loss: 2.098474]\n",
      "651 [D loss: 1.072694, acc.: 50.00%] [G loss: 2.032158]\n",
      "652 [D loss: 1.158515, acc.: 50.00%] [G loss: 2.179889]\n",
      "653 [D loss: 1.134572, acc.: 50.00%] [G loss: 2.124515]\n",
      "654 [D loss: 1.125381, acc.: 50.00%] [G loss: 2.030430]\n",
      "655 [D loss: 1.148904, acc.: 50.00%] [G loss: 2.043169]\n",
      "656 [D loss: 1.137832, acc.: 50.00%] [G loss: 2.141692]\n",
      "657 [D loss: 1.068024, acc.: 50.00%] [G loss: 2.204348]\n",
      "658 [D loss: 1.127364, acc.: 50.00%] [G loss: 2.143349]\n",
      "659 [D loss: 1.167808, acc.: 50.00%] [G loss: 2.089261]\n",
      "660 [D loss: 1.142409, acc.: 50.00%] [G loss: 2.102609]\n",
      "661 [D loss: 1.175123, acc.: 50.00%] [G loss: 2.145020]\n",
      "662 [D loss: 1.102378, acc.: 50.00%] [G loss: 2.048731]\n",
      "663 [D loss: 1.095266, acc.: 50.00%] [G loss: 2.047805]\n",
      "664 [D loss: 1.066967, acc.: 50.00%] [G loss: 2.051893]\n",
      "665 [D loss: 1.135756, acc.: 50.00%] [G loss: 2.071711]\n",
      "666 [D loss: 1.118568, acc.: 50.00%] [G loss: 2.089690]\n",
      "667 [D loss: 1.086472, acc.: 50.00%] [G loss: 2.036002]\n",
      "668 [D loss: 1.050827, acc.: 50.00%] [G loss: 2.090701]\n",
      "669 [D loss: 1.101308, acc.: 50.00%] [G loss: 2.003015]\n",
      "670 [D loss: 1.092681, acc.: 50.00%] [G loss: 2.089546]\n",
      "671 [D loss: 1.149500, acc.: 50.00%] [G loss: 1.976046]\n",
      "672 [D loss: 1.133056, acc.: 50.00%] [G loss: 2.039436]\n",
      "673 [D loss: 1.078120, acc.: 50.00%] [G loss: 2.024204]\n",
      "674 [D loss: 1.146974, acc.: 50.00%] [G loss: 2.143267]\n",
      "675 [D loss: 1.084414, acc.: 50.00%] [G loss: 2.141579]\n",
      "676 [D loss: 1.091547, acc.: 50.00%] [G loss: 2.052918]\n",
      "677 [D loss: 1.051222, acc.: 50.00%] [G loss: 1.984970]\n",
      "678 [D loss: 1.144355, acc.: 50.00%] [G loss: 2.067261]\n",
      "679 [D loss: 1.108443, acc.: 50.00%] [G loss: 2.087904]\n",
      "680 [D loss: 1.131588, acc.: 50.00%] [G loss: 2.071118]\n",
      "681 [D loss: 1.141362, acc.: 50.00%] [G loss: 2.053059]\n",
      "682 [D loss: 1.112941, acc.: 50.00%] [G loss: 2.054976]\n",
      "683 [D loss: 1.106938, acc.: 50.00%] [G loss: 2.030702]\n",
      "684 [D loss: 1.082676, acc.: 50.00%] [G loss: 2.045239]\n",
      "685 [D loss: 1.058222, acc.: 50.00%] [G loss: 2.142902]\n",
      "686 [D loss: 1.089481, acc.: 50.00%] [G loss: 2.054903]\n",
      "687 [D loss: 1.107311, acc.: 50.00%] [G loss: 2.044031]\n",
      "688 [D loss: 1.112767, acc.: 50.00%] [G loss: 2.050723]\n",
      "689 [D loss: 1.052737, acc.: 50.00%] [G loss: 2.069403]\n",
      "690 [D loss: 1.177920, acc.: 50.00%] [G loss: 2.011394]\n",
      "691 [D loss: 1.089084, acc.: 50.00%] [G loss: 2.076484]\n",
      "692 [D loss: 1.103707, acc.: 50.00%] [G loss: 2.118189]\n",
      "693 [D loss: 1.080784, acc.: 50.00%] [G loss: 2.113618]\n",
      "694 [D loss: 1.032444, acc.: 50.00%] [G loss: 2.183164]\n",
      "695 [D loss: 1.119200, acc.: 50.00%] [G loss: 2.091033]\n",
      "696 [D loss: 1.048478, acc.: 50.00%] [G loss: 2.120385]\n",
      "697 [D loss: 1.107387, acc.: 50.00%] [G loss: 2.196466]\n",
      "698 [D loss: 1.106090, acc.: 50.00%] [G loss: 2.057023]\n",
      "699 [D loss: 1.098549, acc.: 50.00%] [G loss: 1.979664]\n",
      "700 [D loss: 1.077524, acc.: 50.00%] [G loss: 2.082388]\n",
      "701 [D loss: 1.083733, acc.: 50.00%] [G loss: 1.975948]\n",
      "702 [D loss: 1.126566, acc.: 50.00%] [G loss: 2.040975]\n",
      "703 [D loss: 1.134088, acc.: 50.00%] [G loss: 2.008377]\n",
      "704 [D loss: 1.089431, acc.: 50.00%] [G loss: 2.160347]\n",
      "705 [D loss: 1.118116, acc.: 50.00%] [G loss: 2.011973]\n",
      "706 [D loss: 1.072810, acc.: 50.00%] [G loss: 2.109658]\n",
      "707 [D loss: 1.058223, acc.: 50.00%] [G loss: 2.052500]\n",
      "708 [D loss: 1.109941, acc.: 50.00%] [G loss: 1.959100]\n",
      "709 [D loss: 1.073903, acc.: 50.00%] [G loss: 2.129145]\n",
      "710 [D loss: 1.064098, acc.: 50.00%] [G loss: 2.009203]\n",
      "711 [D loss: 1.107728, acc.: 50.00%] [G loss: 2.020183]\n",
      "712 [D loss: 1.094874, acc.: 50.00%] [G loss: 1.949269]\n",
      "713 [D loss: 1.068944, acc.: 50.00%] [G loss: 1.992191]\n",
      "714 [D loss: 1.072536, acc.: 50.00%] [G loss: 1.993746]\n",
      "715 [D loss: 1.097676, acc.: 50.00%] [G loss: 2.031644]\n",
      "716 [D loss: 1.090538, acc.: 50.00%] [G loss: 2.057094]\n",
      "717 [D loss: 1.086637, acc.: 50.00%] [G loss: 2.074689]\n",
      "718 [D loss: 1.096043, acc.: 50.00%] [G loss: 1.958729]\n",
      "719 [D loss: 1.148665, acc.: 50.00%] [G loss: 2.012135]\n",
      "720 [D loss: 1.037491, acc.: 50.00%] [G loss: 1.967072]\n",
      "721 [D loss: 1.062673, acc.: 50.00%] [G loss: 2.019893]\n",
      "722 [D loss: 1.096875, acc.: 50.00%] [G loss: 2.075649]\n",
      "723 [D loss: 1.095281, acc.: 50.00%] [G loss: 2.005613]\n",
      "724 [D loss: 1.061009, acc.: 50.00%] [G loss: 2.133781]\n",
      "725 [D loss: 1.061720, acc.: 50.00%] [G loss: 2.068933]\n",
      "726 [D loss: 1.100170, acc.: 50.00%] [G loss: 1.991012]\n",
      "727 [D loss: 1.082071, acc.: 50.00%] [G loss: 2.076710]\n",
      "728 [D loss: 1.045282, acc.: 50.00%] [G loss: 1.977204]\n",
      "729 [D loss: 1.031717, acc.: 50.00%] [G loss: 2.010669]\n",
      "730 [D loss: 1.082322, acc.: 50.00%] [G loss: 2.096240]\n",
      "731 [D loss: 1.049728, acc.: 50.00%] [G loss: 1.955301]\n",
      "732 [D loss: 1.055638, acc.: 50.00%] [G loss: 2.044046]\n",
      "733 [D loss: 1.091500, acc.: 50.00%] [G loss: 2.067807]\n",
      "734 [D loss: 1.042567, acc.: 50.00%] [G loss: 2.025721]\n",
      "735 [D loss: 1.110068, acc.: 50.00%] [G loss: 1.957608]\n",
      "736 [D loss: 1.102867, acc.: 50.00%] [G loss: 1.969530]\n",
      "737 [D loss: 1.152366, acc.: 50.00%] [G loss: 1.956866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738 [D loss: 1.114393, acc.: 50.00%] [G loss: 1.977756]\n",
      "739 [D loss: 1.058720, acc.: 50.00%] [G loss: 2.028972]\n",
      "740 [D loss: 1.046037, acc.: 50.00%] [G loss: 1.864431]\n",
      "741 [D loss: 1.041017, acc.: 50.00%] [G loss: 2.055896]\n",
      "742 [D loss: 1.080278, acc.: 50.00%] [G loss: 2.046293]\n",
      "743 [D loss: 1.082512, acc.: 50.00%] [G loss: 1.945320]\n",
      "744 [D loss: 1.077734, acc.: 50.00%] [G loss: 1.951711]\n",
      "745 [D loss: 1.073427, acc.: 50.00%] [G loss: 1.937019]\n",
      "746 [D loss: 1.070985, acc.: 50.00%] [G loss: 1.951421]\n",
      "747 [D loss: 1.052947, acc.: 50.00%] [G loss: 2.013065]\n",
      "748 [D loss: 1.048823, acc.: 50.00%] [G loss: 2.083098]\n",
      "749 [D loss: 1.124036, acc.: 50.00%] [G loss: 1.984745]\n",
      "750 [D loss: 1.043216, acc.: 50.00%] [G loss: 2.147992]\n",
      "751 [D loss: 1.008830, acc.: 50.00%] [G loss: 2.035893]\n",
      "752 [D loss: 1.046096, acc.: 50.00%] [G loss: 2.007818]\n",
      "753 [D loss: 1.071433, acc.: 50.00%] [G loss: 1.832910]\n",
      "754 [D loss: 1.059043, acc.: 50.00%] [G loss: 2.013340]\n",
      "755 [D loss: 1.065100, acc.: 50.00%] [G loss: 2.043334]\n",
      "756 [D loss: 1.103940, acc.: 50.00%] [G loss: 2.059735]\n",
      "757 [D loss: 1.054402, acc.: 50.00%] [G loss: 2.051595]\n",
      "758 [D loss: 1.040875, acc.: 50.00%] [G loss: 1.948490]\n",
      "759 [D loss: 1.021586, acc.: 50.00%] [G loss: 1.928515]\n",
      "760 [D loss: 1.026487, acc.: 50.00%] [G loss: 2.000466]\n",
      "761 [D loss: 1.031166, acc.: 50.00%] [G loss: 2.012285]\n",
      "762 [D loss: 1.056522, acc.: 50.00%] [G loss: 1.965562]\n",
      "763 [D loss: 0.989540, acc.: 50.00%] [G loss: 1.913580]\n",
      "764 [D loss: 1.074054, acc.: 50.00%] [G loss: 1.897103]\n",
      "765 [D loss: 1.004187, acc.: 50.00%] [G loss: 2.021920]\n",
      "766 [D loss: 1.073422, acc.: 50.00%] [G loss: 1.859238]\n",
      "767 [D loss: 1.053064, acc.: 50.00%] [G loss: 1.999251]\n",
      "768 [D loss: 1.029062, acc.: 50.00%] [G loss: 2.045812]\n",
      "769 [D loss: 1.026805, acc.: 50.00%] [G loss: 2.003598]\n",
      "770 [D loss: 1.016292, acc.: 50.00%] [G loss: 1.909207]\n",
      "771 [D loss: 1.084021, acc.: 50.00%] [G loss: 1.935693]\n",
      "772 [D loss: 1.060435, acc.: 50.00%] [G loss: 1.921906]\n",
      "773 [D loss: 1.050831, acc.: 50.00%] [G loss: 1.937800]\n",
      "774 [D loss: 1.001015, acc.: 50.00%] [G loss: 1.983780]\n",
      "775 [D loss: 1.035897, acc.: 50.00%] [G loss: 1.839800]\n",
      "776 [D loss: 1.069540, acc.: 50.00%] [G loss: 1.939109]\n",
      "777 [D loss: 1.077087, acc.: 50.00%] [G loss: 1.888452]\n",
      "778 [D loss: 1.046273, acc.: 50.00%] [G loss: 1.982903]\n",
      "779 [D loss: 1.056393, acc.: 50.00%] [G loss: 1.995375]\n",
      "780 [D loss: 1.068627, acc.: 50.00%] [G loss: 1.966557]\n",
      "781 [D loss: 1.054981, acc.: 50.00%] [G loss: 1.854677]\n",
      "782 [D loss: 1.049561, acc.: 50.00%] [G loss: 1.932317]\n",
      "783 [D loss: 0.982297, acc.: 50.00%] [G loss: 1.969463]\n",
      "784 [D loss: 1.012345, acc.: 50.00%] [G loss: 1.844022]\n",
      "785 [D loss: 1.037348, acc.: 50.00%] [G loss: 1.879726]\n",
      "786 [D loss: 1.043397, acc.: 50.00%] [G loss: 1.917919]\n",
      "787 [D loss: 1.013477, acc.: 50.00%] [G loss: 1.833590]\n",
      "788 [D loss: 1.053732, acc.: 50.00%] [G loss: 1.839396]\n",
      "789 [D loss: 1.054158, acc.: 50.00%] [G loss: 1.943213]\n",
      "790 [D loss: 0.993677, acc.: 50.00%] [G loss: 1.875568]\n",
      "791 [D loss: 1.026920, acc.: 50.00%] [G loss: 1.854208]\n",
      "792 [D loss: 1.034160, acc.: 50.00%] [G loss: 1.877401]\n",
      "793 [D loss: 1.075736, acc.: 50.00%] [G loss: 1.863610]\n",
      "794 [D loss: 1.051808, acc.: 50.00%] [G loss: 1.931354]\n",
      "795 [D loss: 1.033550, acc.: 50.00%] [G loss: 2.026660]\n",
      "796 [D loss: 1.054936, acc.: 50.00%] [G loss: 1.896462]\n",
      "797 [D loss: 1.022048, acc.: 50.00%] [G loss: 1.853446]\n",
      "798 [D loss: 1.022090, acc.: 50.00%] [G loss: 1.965486]\n",
      "799 [D loss: 1.036556, acc.: 50.00%] [G loss: 1.841537]\n",
      "800 [D loss: 1.070362, acc.: 50.00%] [G loss: 1.942366]\n",
      "801 [D loss: 1.011218, acc.: 50.00%] [G loss: 1.956873]\n",
      "802 [D loss: 1.045409, acc.: 50.00%] [G loss: 1.849168]\n",
      "803 [D loss: 1.005755, acc.: 50.00%] [G loss: 1.925484]\n",
      "804 [D loss: 1.035954, acc.: 50.00%] [G loss: 1.831969]\n",
      "805 [D loss: 1.017302, acc.: 50.00%] [G loss: 1.939197]\n",
      "806 [D loss: 1.002708, acc.: 50.00%] [G loss: 1.876528]\n",
      "807 [D loss: 0.999318, acc.: 50.00%] [G loss: 1.920615]\n",
      "808 [D loss: 0.979879, acc.: 50.00%] [G loss: 1.995522]\n",
      "809 [D loss: 1.057588, acc.: 50.00%] [G loss: 1.785484]\n",
      "810 [D loss: 1.039456, acc.: 50.00%] [G loss: 1.854725]\n",
      "811 [D loss: 1.023705, acc.: 50.00%] [G loss: 1.828797]\n",
      "812 [D loss: 0.951871, acc.: 50.00%] [G loss: 1.940699]\n",
      "813 [D loss: 1.043512, acc.: 50.00%] [G loss: 1.808622]\n",
      "814 [D loss: 1.019776, acc.: 50.00%] [G loss: 2.009412]\n",
      "815 [D loss: 1.002905, acc.: 50.00%] [G loss: 1.957951]\n",
      "816 [D loss: 1.015599, acc.: 50.00%] [G loss: 1.886487]\n",
      "817 [D loss: 1.041530, acc.: 50.00%] [G loss: 1.965987]\n",
      "818 [D loss: 1.059553, acc.: 50.00%] [G loss: 1.828747]\n",
      "819 [D loss: 1.024365, acc.: 50.00%] [G loss: 1.863907]\n",
      "820 [D loss: 0.995536, acc.: 50.00%] [G loss: 1.793157]\n",
      "821 [D loss: 1.023177, acc.: 50.00%] [G loss: 1.842751]\n",
      "822 [D loss: 1.034212, acc.: 50.00%] [G loss: 1.902209]\n",
      "823 [D loss: 1.005746, acc.: 50.00%] [G loss: 1.888727]\n",
      "824 [D loss: 1.009938, acc.: 50.00%] [G loss: 1.817051]\n",
      "825 [D loss: 1.036142, acc.: 50.00%] [G loss: 1.862167]\n",
      "826 [D loss: 1.028687, acc.: 50.00%] [G loss: 1.945627]\n",
      "827 [D loss: 0.979685, acc.: 50.00%] [G loss: 1.887706]\n",
      "828 [D loss: 1.023939, acc.: 50.00%] [G loss: 1.912630]\n",
      "829 [D loss: 1.009348, acc.: 50.00%] [G loss: 1.913441]\n",
      "830 [D loss: 1.001603, acc.: 50.00%] [G loss: 1.833302]\n",
      "831 [D loss: 1.004397, acc.: 50.00%] [G loss: 1.803150]\n",
      "832 [D loss: 1.039180, acc.: 50.00%] [G loss: 1.801479]\n",
      "833 [D loss: 1.067935, acc.: 50.00%] [G loss: 1.913009]\n",
      "834 [D loss: 1.035948, acc.: 50.00%] [G loss: 1.796078]\n",
      "835 [D loss: 1.049223, acc.: 50.00%] [G loss: 1.737244]\n",
      "836 [D loss: 0.985394, acc.: 50.00%] [G loss: 1.840073]\n",
      "837 [D loss: 1.017419, acc.: 50.00%] [G loss: 1.887779]\n",
      "838 [D loss: 1.024744, acc.: 50.00%] [G loss: 1.858048]\n",
      "839 [D loss: 1.029924, acc.: 50.00%] [G loss: 1.852696]\n",
      "840 [D loss: 0.982506, acc.: 50.00%] [G loss: 1.887182]\n",
      "841 [D loss: 1.030243, acc.: 50.00%] [G loss: 1.840770]\n",
      "842 [D loss: 1.013407, acc.: 50.00%] [G loss: 1.885820]\n",
      "843 [D loss: 0.992282, acc.: 50.00%] [G loss: 1.910621]\n",
      "844 [D loss: 0.997172, acc.: 50.00%] [G loss: 1.707270]\n",
      "845 [D loss: 1.002334, acc.: 50.00%] [G loss: 1.799326]\n",
      "846 [D loss: 0.976595, acc.: 50.00%] [G loss: 1.870375]\n",
      "847 [D loss: 1.051516, acc.: 50.00%] [G loss: 1.863383]\n",
      "848 [D loss: 1.008616, acc.: 50.00%] [G loss: 1.898521]\n",
      "849 [D loss: 1.023733, acc.: 50.00%] [G loss: 1.932907]\n",
      "850 [D loss: 0.983045, acc.: 50.00%] [G loss: 1.871666]\n",
      "851 [D loss: 0.997187, acc.: 50.00%] [G loss: 1.866278]\n",
      "852 [D loss: 1.065682, acc.: 50.00%] [G loss: 1.831228]\n",
      "853 [D loss: 0.985619, acc.: 50.00%] [G loss: 1.852564]\n",
      "854 [D loss: 1.000838, acc.: 50.00%] [G loss: 1.809892]\n",
      "855 [D loss: 0.963236, acc.: 50.00%] [G loss: 1.786133]\n",
      "856 [D loss: 0.978188, acc.: 50.00%] [G loss: 1.866111]\n",
      "857 [D loss: 0.964758, acc.: 50.00%] [G loss: 1.959067]\n",
      "858 [D loss: 0.985350, acc.: 50.00%] [G loss: 1.872546]\n",
      "859 [D loss: 1.018909, acc.: 50.00%] [G loss: 1.901252]\n",
      "860 [D loss: 1.020054, acc.: 50.00%] [G loss: 1.822791]\n",
      "861 [D loss: 1.002292, acc.: 50.00%] [G loss: 1.883228]\n",
      "862 [D loss: 0.975332, acc.: 50.00%] [G loss: 1.792904]\n",
      "863 [D loss: 0.998698, acc.: 50.00%] [G loss: 1.816224]\n",
      "864 [D loss: 1.009195, acc.: 50.00%] [G loss: 1.840088]\n",
      "865 [D loss: 0.970761, acc.: 50.00%] [G loss: 1.858261]\n",
      "866 [D loss: 0.962634, acc.: 50.00%] [G loss: 1.889983]\n",
      "867 [D loss: 1.002712, acc.: 50.00%] [G loss: 1.859484]\n",
      "868 [D loss: 1.015563, acc.: 50.00%] [G loss: 1.736418]\n",
      "869 [D loss: 0.952462, acc.: 50.00%] [G loss: 1.726308]\n",
      "870 [D loss: 1.025013, acc.: 50.00%] [G loss: 1.797304]\n",
      "871 [D loss: 0.960576, acc.: 50.00%] [G loss: 1.756537]\n",
      "872 [D loss: 0.977646, acc.: 50.00%] [G loss: 1.872920]\n",
      "873 [D loss: 0.994366, acc.: 50.00%] [G loss: 1.808874]\n",
      "874 [D loss: 0.974497, acc.: 50.00%] [G loss: 1.924480]\n",
      "875 [D loss: 1.023494, acc.: 50.00%] [G loss: 1.800017]\n",
      "876 [D loss: 0.958642, acc.: 50.00%] [G loss: 1.721633]\n",
      "877 [D loss: 0.993167, acc.: 50.00%] [G loss: 1.836514]\n",
      "878 [D loss: 0.991772, acc.: 50.00%] [G loss: 1.832831]\n",
      "879 [D loss: 1.025196, acc.: 50.00%] [G loss: 1.748816]\n",
      "880 [D loss: 0.991816, acc.: 50.00%] [G loss: 1.837367]\n",
      "881 [D loss: 0.972417, acc.: 50.00%] [G loss: 1.833185]\n",
      "882 [D loss: 1.017529, acc.: 50.00%] [G loss: 1.713374]\n",
      "883 [D loss: 0.887838, acc.: 50.00%] [G loss: 1.891115]\n",
      "884 [D loss: 0.952809, acc.: 50.00%] [G loss: 1.827577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885 [D loss: 0.974805, acc.: 50.00%] [G loss: 1.858271]\n",
      "886 [D loss: 1.043130, acc.: 50.00%] [G loss: 1.883716]\n",
      "887 [D loss: 0.947986, acc.: 50.00%] [G loss: 1.874329]\n",
      "888 [D loss: 1.007625, acc.: 50.00%] [G loss: 1.861916]\n",
      "889 [D loss: 0.962049, acc.: 50.00%] [G loss: 1.783398]\n",
      "890 [D loss: 0.992775, acc.: 50.00%] [G loss: 1.862859]\n",
      "891 [D loss: 1.011950, acc.: 50.00%] [G loss: 1.776298]\n",
      "892 [D loss: 1.035751, acc.: 50.00%] [G loss: 1.757991]\n",
      "893 [D loss: 0.969102, acc.: 50.00%] [G loss: 1.682660]\n",
      "894 [D loss: 0.987290, acc.: 50.00%] [G loss: 1.744010]\n",
      "895 [D loss: 0.973401, acc.: 50.00%] [G loss: 1.860072]\n",
      "896 [D loss: 1.001217, acc.: 50.00%] [G loss: 1.890404]\n",
      "897 [D loss: 0.936121, acc.: 50.00%] [G loss: 1.764523]\n",
      "898 [D loss: 1.011581, acc.: 50.00%] [G loss: 1.702995]\n",
      "899 [D loss: 1.007098, acc.: 50.00%] [G loss: 1.721196]\n",
      "900 [D loss: 0.962941, acc.: 50.00%] [G loss: 1.796669]\n",
      "901 [D loss: 0.900580, acc.: 50.00%] [G loss: 1.759636]\n",
      "902 [D loss: 0.942268, acc.: 50.00%] [G loss: 1.692022]\n",
      "903 [D loss: 0.968117, acc.: 50.00%] [G loss: 1.826581]\n",
      "904 [D loss: 0.941498, acc.: 50.00%] [G loss: 1.827055]\n",
      "905 [D loss: 1.003442, acc.: 50.00%] [G loss: 1.962916]\n",
      "906 [D loss: 0.986640, acc.: 50.00%] [G loss: 1.725847]\n",
      "907 [D loss: 0.929251, acc.: 50.00%] [G loss: 1.789552]\n",
      "908 [D loss: 0.961742, acc.: 50.00%] [G loss: 1.831552]\n",
      "909 [D loss: 0.962380, acc.: 50.00%] [G loss: 1.678235]\n",
      "910 [D loss: 0.992592, acc.: 50.00%] [G loss: 1.727717]\n",
      "911 [D loss: 0.933708, acc.: 50.00%] [G loss: 1.896419]\n",
      "912 [D loss: 0.970163, acc.: 50.00%] [G loss: 1.777308]\n",
      "913 [D loss: 0.985577, acc.: 50.00%] [G loss: 1.670218]\n",
      "914 [D loss: 0.990759, acc.: 50.00%] [G loss: 1.735695]\n",
      "915 [D loss: 0.924193, acc.: 50.00%] [G loss: 1.752635]\n",
      "916 [D loss: 0.945420, acc.: 50.00%] [G loss: 1.728995]\n",
      "917 [D loss: 0.970111, acc.: 50.00%] [G loss: 1.771674]\n",
      "918 [D loss: 0.998601, acc.: 50.00%] [G loss: 1.730901]\n",
      "919 [D loss: 0.980932, acc.: 50.00%] [G loss: 1.772588]\n",
      "920 [D loss: 0.987154, acc.: 50.00%] [G loss: 1.714525]\n",
      "921 [D loss: 0.918643, acc.: 50.00%] [G loss: 1.713417]\n",
      "922 [D loss: 1.020543, acc.: 50.00%] [G loss: 1.788344]\n",
      "923 [D loss: 0.939592, acc.: 50.00%] [G loss: 1.791204]\n",
      "924 [D loss: 0.997536, acc.: 50.00%] [G loss: 1.782898]\n",
      "925 [D loss: 1.008167, acc.: 50.00%] [G loss: 1.786996]\n",
      "926 [D loss: 0.925621, acc.: 50.00%] [G loss: 1.702534]\n",
      "927 [D loss: 1.003513, acc.: 50.00%] [G loss: 1.722445]\n",
      "928 [D loss: 0.944815, acc.: 50.00%] [G loss: 1.781821]\n",
      "929 [D loss: 0.915603, acc.: 50.00%] [G loss: 1.762298]\n",
      "930 [D loss: 0.989739, acc.: 50.00%] [G loss: 1.688204]\n",
      "931 [D loss: 0.937527, acc.: 50.00%] [G loss: 1.699924]\n",
      "932 [D loss: 0.962647, acc.: 50.00%] [G loss: 1.780439]\n",
      "933 [D loss: 0.935811, acc.: 50.00%] [G loss: 1.747004]\n",
      "934 [D loss: 0.941059, acc.: 50.00%] [G loss: 1.608746]\n",
      "935 [D loss: 1.020300, acc.: 50.00%] [G loss: 1.698425]\n",
      "936 [D loss: 0.986154, acc.: 50.00%] [G loss: 1.658510]\n",
      "937 [D loss: 0.931355, acc.: 50.00%] [G loss: 1.810467]\n",
      "938 [D loss: 0.980325, acc.: 50.00%] [G loss: 1.718023]\n",
      "939 [D loss: 0.991832, acc.: 50.00%] [G loss: 1.717877]\n",
      "940 [D loss: 0.970619, acc.: 50.00%] [G loss: 1.712958]\n",
      "941 [D loss: 0.940595, acc.: 50.00%] [G loss: 1.772742]\n",
      "942 [D loss: 0.965117, acc.: 50.00%] [G loss: 1.753752]\n",
      "943 [D loss: 0.914846, acc.: 50.00%] [G loss: 1.748922]\n",
      "944 [D loss: 0.944553, acc.: 50.00%] [G loss: 1.864209]\n",
      "945 [D loss: 0.927025, acc.: 50.00%] [G loss: 1.725213]\n",
      "946 [D loss: 0.947580, acc.: 50.00%] [G loss: 1.712690]\n",
      "947 [D loss: 0.966731, acc.: 50.00%] [G loss: 1.751092]\n",
      "948 [D loss: 0.945413, acc.: 50.00%] [G loss: 1.706430]\n",
      "949 [D loss: 0.949445, acc.: 50.00%] [G loss: 1.564682]\n",
      "950 [D loss: 0.963242, acc.: 50.00%] [G loss: 1.732871]\n",
      "951 [D loss: 0.981373, acc.: 50.00%] [G loss: 1.716883]\n",
      "952 [D loss: 0.922237, acc.: 50.00%] [G loss: 1.712427]\n",
      "953 [D loss: 0.963463, acc.: 50.00%] [G loss: 1.849735]\n",
      "954 [D loss: 0.937825, acc.: 50.00%] [G loss: 1.712110]\n",
      "955 [D loss: 0.956769, acc.: 50.00%] [G loss: 1.763911]\n",
      "956 [D loss: 0.992428, acc.: 50.00%] [G loss: 1.679964]\n",
      "957 [D loss: 0.956892, acc.: 50.00%] [G loss: 1.757726]\n",
      "958 [D loss: 0.987374, acc.: 50.00%] [G loss: 1.724786]\n",
      "959 [D loss: 0.964773, acc.: 50.00%] [G loss: 1.839917]\n",
      "960 [D loss: 0.915257, acc.: 50.00%] [G loss: 1.712057]\n",
      "961 [D loss: 0.921493, acc.: 50.00%] [G loss: 1.697484]\n",
      "962 [D loss: 0.927554, acc.: 50.00%] [G loss: 1.761778]\n",
      "963 [D loss: 0.962955, acc.: 50.00%] [G loss: 1.681666]\n",
      "964 [D loss: 0.918567, acc.: 50.00%] [G loss: 1.627816]\n",
      "965 [D loss: 0.929277, acc.: 50.00%] [G loss: 1.636509]\n",
      "966 [D loss: 0.938946, acc.: 50.00%] [G loss: 1.747113]\n",
      "967 [D loss: 0.937240, acc.: 50.00%] [G loss: 1.824448]\n",
      "968 [D loss: 0.961183, acc.: 50.00%] [G loss: 1.813174]\n",
      "969 [D loss: 0.949476, acc.: 50.00%] [G loss: 1.657986]\n",
      "970 [D loss: 0.941439, acc.: 50.00%] [G loss: 1.829205]\n",
      "971 [D loss: 0.978621, acc.: 50.00%] [G loss: 1.634337]\n",
      "972 [D loss: 1.004093, acc.: 50.00%] [G loss: 1.656681]\n",
      "973 [D loss: 0.950092, acc.: 50.00%] [G loss: 1.639609]\n",
      "974 [D loss: 0.984747, acc.: 50.00%] [G loss: 1.904437]\n",
      "975 [D loss: 0.990773, acc.: 50.00%] [G loss: 1.659814]\n",
      "976 [D loss: 0.933405, acc.: 50.00%] [G loss: 1.703517]\n",
      "977 [D loss: 1.004863, acc.: 50.00%] [G loss: 1.615122]\n",
      "978 [D loss: 0.914877, acc.: 50.00%] [G loss: 1.646777]\n",
      "979 [D loss: 0.932001, acc.: 50.00%] [G loss: 1.741099]\n",
      "980 [D loss: 0.972820, acc.: 50.00%] [G loss: 1.766670]\n",
      "981 [D loss: 0.903820, acc.: 50.00%] [G loss: 1.667928]\n",
      "982 [D loss: 0.929309, acc.: 50.00%] [G loss: 1.626065]\n",
      "983 [D loss: 0.983590, acc.: 50.00%] [G loss: 1.671985]\n",
      "984 [D loss: 0.912833, acc.: 50.00%] [G loss: 1.685369]\n",
      "985 [D loss: 0.934784, acc.: 50.00%] [G loss: 1.779301]\n",
      "986 [D loss: 0.911009, acc.: 50.00%] [G loss: 1.619812]\n",
      "987 [D loss: 0.935257, acc.: 50.00%] [G loss: 1.678062]\n",
      "988 [D loss: 0.958686, acc.: 50.00%] [G loss: 1.752357]\n",
      "989 [D loss: 0.917821, acc.: 50.00%] [G loss: 1.707989]\n",
      "990 [D loss: 0.968237, acc.: 50.00%] [G loss: 1.638520]\n",
      "991 [D loss: 0.979751, acc.: 50.00%] [G loss: 1.721418]\n",
      "992 [D loss: 0.887677, acc.: 50.00%] [G loss: 1.817032]\n",
      "993 [D loss: 0.901882, acc.: 50.00%] [G loss: 1.732378]\n",
      "994 [D loss: 0.981936, acc.: 50.00%] [G loss: 1.712902]\n",
      "995 [D loss: 0.895447, acc.: 50.00%] [G loss: 1.668584]\n",
      "996 [D loss: 0.895304, acc.: 50.00%] [G loss: 1.638990]\n",
      "997 [D loss: 0.946895, acc.: 50.00%] [G loss: 1.650211]\n",
      "998 [D loss: 0.943967, acc.: 50.00%] [G loss: 1.740943]\n",
      "999 [D loss: 0.956219, acc.: 50.00%] [G loss: 1.630338]\n"
     ]
    }
   ],
   "source": [
    "lstmgan.train(epochs=1000, batch_size=20, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0h 9m 4s\n"
     ]
    }
   ],
   "source": [
    "end = datetime.datetime.now()\n",
    "\n",
    "diff = (end - start)\n",
    "\n",
    "datetime.timedelta(seconds=10, microseconds=885206)\n",
    "\n",
    "diff_seconds = int(diff.total_seconds())\n",
    "\n",
    "minute_seconds, seconds = divmod(diff_seconds, 60)\n",
    "hours, minutes = divmod(minute_seconds, 60)\n",
    "hms = f\"{hours}h {minutes}m {seconds}s\"\n",
    "\n",
    "'0h 0m 10s'\n",
    "print(hms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-2.78866738e-01,  2.83506215e-01, -1.06550917e-01,\n",
      "         1.61859661e-01,  1.62954316e-01,  8.77741352e-02,\n",
      "        -1.96943209e-01, -5.59760034e-02, -8.64504427e-02,\n",
      "        -5.17245862e-05, -3.00759226e-01,  8.92397463e-02,\n",
      "        -2.83573151e-01,  8.61801207e-02,  2.82437772e-01,\n",
      "        -1.87183321e-01,  2.50819862e-01, -1.80072740e-01,\n",
      "         2.58226752e-01,  2.65989065e-01, -1.39630333e-01,\n",
      "         6.40792623e-02,  8.18407461e-02, -7.16285631e-02,\n",
      "        -2.65613720e-02, -1.55572770e-02,  2.97948048e-02,\n",
      "        -2.55733550e-01, -1.64589018e-01, -1.69833109e-01,\n",
      "        -2.20689863e-01,  4.57774699e-02, -2.78150678e-01,\n",
      "         5.65863140e-02,  1.20947488e-01, -2.97073096e-01,\n",
      "        -2.95010597e-01, -6.73216507e-02, -1.16186395e-01,\n",
      "        -1.68868214e-01, -2.76158959e-01,  2.67015785e-01,\n",
      "         1.84346125e-01, -2.45254993e-01,  1.00093924e-01,\n",
      "         3.43628898e-02,  3.24788690e-02,  1.27949685e-01,\n",
      "        -2.27176771e-01, -2.81905830e-01, -1.37723356e-01,\n",
      "         9.30028036e-02,  1.79384425e-01, -2.45575719e-02,\n",
      "        -8.90655816e-02,  1.94691390e-01, -8.21965262e-02,\n",
      "        -1.68489709e-01, -2.62059033e-01, -1.62160382e-01,\n",
      "        -2.59082794e-01,  1.72543488e-02, -2.95672536e-01,\n",
      "        -1.20335892e-01]], dtype=float32), array([[[-0.12313719, -0.08124908, -0.00316828, ...,  0.06035103,\n",
      "         -0.11175594,  0.1264087 ],\n",
      "        [ 0.10073981, -0.0759462 , -0.01726039, ..., -0.11775515,\n",
      "         -0.07923767, -0.0690694 ],\n",
      "        [-0.05063504, -0.12204383, -0.06711538, ..., -0.01640587,\n",
      "         -0.02066356,  0.05646789],\n",
      "        ...,\n",
      "        [-0.05774032, -0.00056427,  0.12456629, ...,  0.05717741,\n",
      "         -0.0359724 ,  0.06798477],\n",
      "        [ 0.05100884,  0.14707568,  0.12322652, ...,  0.06156936,\n",
      "         -0.03206755,  0.02544641],\n",
      "        [ 0.06239838,  0.05007846,  0.08371139, ..., -0.10390768,\n",
      "          0.0861886 , -0.12081388]],\n",
      "\n",
      "       [[ 0.04779289,  0.12740257,  0.08503211, ...,  0.00285138,\n",
      "          0.06211638, -0.13554764],\n",
      "        [-0.00377207,  0.02895015,  0.01601446, ...,  0.08499721,\n",
      "          0.11148497, -0.07961614],\n",
      "        [-0.02565111,  0.0033609 , -0.103553  , ...,  0.08335488,\n",
      "          0.03736453, -0.11101925],\n",
      "        ...,\n",
      "        [-0.11835789, -0.09664588,  0.02437326, ..., -0.07352361,\n",
      "         -0.13409038, -0.01388799],\n",
      "        [-0.06985535,  0.10465169,  0.13760121, ..., -0.05403566,\n",
      "         -0.08558682, -0.04546708],\n",
      "        [ 0.0241964 , -0.0164942 ,  0.10413057, ..., -0.00411449,\n",
      "          0.07051248, -0.1400877 ]],\n",
      "\n",
      "       [[ 0.12068694, -0.136546  , -0.05484776, ...,  0.04467317,\n",
      "         -0.00834506, -0.0764456 ],\n",
      "        [ 0.11877466, -0.06501933,  0.14683792, ...,  0.05626849,\n",
      "          0.019115  , -0.12451216],\n",
      "        [ 0.00699247,  0.04700482,  0.02231567, ..., -0.05457843,\n",
      "         -0.14176856, -0.10788088],\n",
      "        ...,\n",
      "        [-0.01684937, -0.01874204,  0.04579106, ..., -0.06338003,\n",
      "         -0.12505245, -0.02763687],\n",
      "        [ 0.04809695,  0.07789724,  0.09252308, ..., -0.06162484,\n",
      "         -0.07604492, -0.10416643],\n",
      "        [-0.11927941,  0.00212064,  0.11526848, ...,  0.14124875,\n",
      "          0.01640064, -0.12172858]]], dtype=float32), array([[[-0.21869223],\n",
      "        [ 0.0383219 ],\n",
      "        [-0.1763463 ],\n",
      "        [-0.1537412 ],\n",
      "        [ 0.01790863],\n",
      "        [ 0.07689851],\n",
      "        [-0.14239949],\n",
      "        [ 0.02421924],\n",
      "        [-0.04663595],\n",
      "        [-0.05330269],\n",
      "        [-0.03580647],\n",
      "        [-0.10054111],\n",
      "        [-0.03495303],\n",
      "        [-0.14357135],\n",
      "        [ 0.21910383],\n",
      "        [ 0.13569248],\n",
      "        [ 0.07865497],\n",
      "        [ 0.1986157 ],\n",
      "        [ 0.34497473],\n",
      "        [-0.18280087],\n",
      "        [ 0.16744438],\n",
      "        [ 0.22991373],\n",
      "        [ 0.19492938],\n",
      "        [ 0.13162205],\n",
      "        [-0.1241364 ],\n",
      "        [ 0.219479  ],\n",
      "        [-0.17717418],\n",
      "        [ 0.13128024],\n",
      "        [ 0.05939839],\n",
      "        [-0.17631985],\n",
      "        [ 0.17880599],\n",
      "        [-0.07357717]],\n",
      "\n",
      "       [[-0.00133615],\n",
      "        [ 0.11589272],\n",
      "        [-0.10998135],\n",
      "        [-0.23949464],\n",
      "        [ 0.0758671 ],\n",
      "        [ 0.05137442],\n",
      "        [-0.06818953],\n",
      "        [-0.1345003 ],\n",
      "        [ 0.05240603],\n",
      "        [ 0.18876114],\n",
      "        [-0.04520919],\n",
      "        [ 0.22174962],\n",
      "        [ 0.2139586 ],\n",
      "        [ 0.07927381],\n",
      "        [-0.23581985],\n",
      "        [ 0.24833398],\n",
      "        [-0.17184734],\n",
      "        [ 0.02808161],\n",
      "        [ 0.13963608],\n",
      "        [-0.1553445 ],\n",
      "        [-0.14370431],\n",
      "        [-0.1841695 ],\n",
      "        [-0.21027508],\n",
      "        [ 0.0692741 ],\n",
      "        [-0.18355407],\n",
      "        [-0.14858478],\n",
      "        [ 0.1805623 ],\n",
      "        [ 0.10774381],\n",
      "        [-0.22145258],\n",
      "        [ 0.09353184],\n",
      "        [ 0.03927552],\n",
      "        [-0.18579535]],\n",
      "\n",
      "       [[ 0.08098438],\n",
      "        [ 0.02549266],\n",
      "        [ 0.08951731],\n",
      "        [-0.04088915],\n",
      "        [ 0.00560779],\n",
      "        [ 0.10982049],\n",
      "        [ 0.2086978 ],\n",
      "        [-0.01125562],\n",
      "        [-0.22713369],\n",
      "        [-0.08354639],\n",
      "        [-0.10967761],\n",
      "        [ 0.10686663],\n",
      "        [-0.08876471],\n",
      "        [ 0.12515685],\n",
      "        [-0.1349767 ],\n",
      "        [ 0.01414729],\n",
      "        [ 0.2728065 ],\n",
      "        [ 0.2207734 ],\n",
      "        [ 0.10886986],\n",
      "        [-0.2329796 ],\n",
      "        [-0.21805158],\n",
      "        [ 0.08571608],\n",
      "        [-0.18781236],\n",
      "        [-0.06207295],\n",
      "        [-0.14933318],\n",
      "        [ 0.15507542],\n",
      "        [ 0.22683348],\n",
      "        [-0.09350505],\n",
      "        [ 0.24098493],\n",
      "        [ 0.21827982],\n",
      "        [ 0.23807935],\n",
      "        [-0.17146966]]], dtype=float32), array([[-0.14068484,  0.24847823,  0.31029257, -0.05403705,  0.31676573,\n",
      "        -0.10127545, -0.22356898, -0.03265057, -0.11911102, -0.00784103,\n",
      "         0.16476928,  0.13305745,  0.05055276,  0.33231217,  0.30603555,\n",
      "        -0.1289914 , -0.26439276,  0.24778637,  0.08782111, -0.06688742,\n",
      "         0.18282838, -0.11402553, -0.2732955 , -0.11491207, -0.22611728,\n",
      "        -0.19219139, -0.04955963, -0.12278747, -0.05488454,  0.2049908 ,\n",
      "        -0.02534324, -0.32797346],\n",
      "       [ 0.16629726, -0.16307166,  0.38024905,  0.3680419 ,  0.23836192,\n",
      "         0.09254768,  0.33759195, -0.02702721,  0.03306437,  0.19499186,\n",
      "        -0.34112155,  0.35076347, -0.18591802,  0.13158253, -0.10127562,\n",
      "         0.25111592, -0.17628896,  0.40749165, -0.11316959,  0.24489866,\n",
      "        -0.16627531,  0.10878647, -0.07415857,  0.30087486,  0.32637313,\n",
      "         0.22371517, -0.18667139,  0.1787489 ,  0.25320965, -0.18553811,\n",
      "         0.02233249, -0.08561339],\n",
      "       [ 0.33201388, -0.08985857,  0.35779148,  0.02501781,  0.06064598,\n",
      "        -0.09908191,  0.27389842,  0.08963823,  0.17268345, -0.1923762 ,\n",
      "        -0.15753   , -0.21709976,  0.09660019, -0.1773664 ,  0.04856801,\n",
      "         0.2560154 ,  0.36232737, -0.12687209, -0.18372832, -0.3420723 ,\n",
      "        -0.228787  , -0.14379753,  0.12578015, -0.18680057,  0.03897893,\n",
      "        -0.01695504, -0.15310846,  0.24594325,  0.18198618, -0.28697646,\n",
      "        -0.15250862, -0.09774678],\n",
      "       [-0.31932136,  0.3661146 ,  0.30605578, -0.04555698,  0.15904208,\n",
      "         0.2141099 ,  0.28941703,  0.06985442, -0.24642113, -0.15515503,\n",
      "         0.24974282, -0.26513442,  0.37470755, -0.3443131 , -0.24076828,\n",
      "        -0.1946114 ,  0.20994267,  0.27108896,  0.06237738, -0.0425908 ,\n",
      "         0.10294841,  0.39242032,  0.11290854,  0.16510467,  0.26006255,\n",
      "         0.429183  ,  0.22123216, -0.19972429, -0.22005999,  0.3147392 ,\n",
      "         0.03651376, -0.37546414],\n",
      "       [-0.27326372,  0.3798536 ,  0.12072943,  0.19527957, -0.19897556,\n",
      "         0.13086513,  0.23292723,  0.32494622, -0.00304681,  0.34991384,\n",
      "        -0.12322975,  0.26528934,  0.04986116,  0.2597485 ,  0.28234893,\n",
      "         0.18050452,  0.24788935,  0.3346665 ,  0.167556  ,  0.06918873,\n",
      "        -0.16667686,  0.07365084,  0.28338808, -0.3708365 , -0.31970218,\n",
      "        -0.05256297,  0.0615797 ,  0.06708167,  0.22128482, -0.2548341 ,\n",
      "        -0.26158676,  0.33710566],\n",
      "       [ 0.17598823,  0.13810511, -0.22645237,  0.16954309,  0.29472232,\n",
      "        -0.09436852, -0.11611088,  0.39785895,  0.03733833, -0.00426715,\n",
      "        -0.01582812, -0.23413046,  0.05477455, -0.14741899,  0.22318543,\n",
      "         0.06345329,  0.17378122,  0.25321865, -0.32939097, -0.07337824,\n",
      "        -0.12714572, -0.33229998,  0.19570778, -0.07129247, -0.13603193,\n",
      "         0.00119371,  0.08550234,  0.12549733, -0.26020575,  0.05267756,\n",
      "        -0.08825178,  0.26475132],\n",
      "       [-0.34554026, -0.23735087,  0.04315738, -0.08860417,  0.08130573,\n",
      "         0.349737  ,  0.19379738, -0.10389599, -0.19476283, -0.31843454,\n",
      "        -0.04991332,  0.2517424 ,  0.03710046,  0.02955574,  0.18062685,\n",
      "         0.2603008 , -0.15021066, -0.27326506,  0.23955986,  0.26293418,\n",
      "        -0.3203969 , -0.15765221,  0.21586892, -0.00729143, -0.00943329,\n",
      "         0.16491547,  0.29148585,  0.18446127,  0.04635831, -0.17598261,\n",
      "         0.22679459, -0.14805453],\n",
      "       [-0.13870507,  0.21587002,  0.24096243, -0.12178608, -0.19779809,\n",
      "         0.27674496,  0.14899053, -0.31956214,  0.17327398,  0.06749925,\n",
      "         0.3352488 , -0.00750767,  0.2949305 , -0.22041051,  0.35493734,\n",
      "        -0.37482965,  0.14657104, -0.09491617,  0.2923883 , -0.3153817 ,\n",
      "        -0.17941159,  0.04286318, -0.04169156,  0.10658218, -0.2964135 ,\n",
      "        -0.14039479,  0.22669543,  0.01678074,  0.12896061,  0.1327462 ,\n",
      "         0.05456632,  0.01945377]], dtype=float32), array([-0.00328068,  0.03761856,  0.01125767,  0.01199334,  0.0680972 ,\n",
      "        0.01365515,  0.01114062,  0.03682293,  0.01248894, -0.00580327,\n",
      "        0.04172552,  0.01261725, -0.00522208, -0.00402162, -0.00333596,\n",
      "       -0.00303076,  0.0384214 ,  0.07549302, -0.00196473, -0.00237162,\n",
      "        0.05188874,  0.05652273, -0.00480576, -0.00417987, -0.00456507,\n",
      "        0.03579393, -0.00393054, -0.00229961, -0.00216977,  0.0681061 ,\n",
      "       -0.00266569, -0.00351465], dtype=float32), array([[-0.27369836],\n",
      "       [ 0.47126588],\n",
      "       [ 0.22977166],\n",
      "       [ 0.4919288 ],\n",
      "       [ 0.16123052],\n",
      "       [ 0.11506204],\n",
      "       [ 0.3889906 ],\n",
      "       [ 0.47888726],\n",
      "       [ 0.49486455],\n",
      "       [-0.16235381],\n",
      "       [ 0.2834776 ],\n",
      "       [ 0.11790773],\n",
      "       [-0.15843818],\n",
      "       [-0.35102037],\n",
      "       [-0.15693772],\n",
      "       [-0.14749905],\n",
      "       [ 0.23528536],\n",
      "       [ 0.18425113],\n",
      "       [-0.02924288],\n",
      "       [-0.23839939],\n",
      "       [ 0.4885407 ],\n",
      "       [ 0.3138537 ],\n",
      "       [-0.1501916 ],\n",
      "       [-0.05797588],\n",
      "       [-0.39134574],\n",
      "       [ 0.35049033],\n",
      "       [-0.23743188],\n",
      "       [-0.29161173],\n",
      "       [-0.31755826],\n",
      "       [ 0.20876038],\n",
      "       [-0.39905494],\n",
      "       [-0.2523886 ]], dtype=float32), array([0.01424705], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(lstmgan.discriminator.layers[1].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[2.7087027e-02, 2.8347531e-02, 1.5393603e-02, ...,\n",
      "         3.4542847e-02, 0.0000000e+00, 0.0000000e+00]],\n",
      "\n",
      "       [[0.0000000e+00, 0.0000000e+00, 2.8014038e-02, ...,\n",
      "         1.3511876e-02, 0.0000000e+00, 1.2734290e-02]],\n",
      "\n",
      "       [[0.0000000e+00, 4.1976795e-02, 1.6932707e-02, ...,\n",
      "         2.2560477e-02, 0.0000000e+00, 0.0000000e+00]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.0000000e+00, 0.0000000e+00, 5.6570794e-02, ...,\n",
      "         9.2335045e-05, 3.8696453e-05, 0.0000000e+00]],\n",
      "\n",
      "       [[1.1414906e-02, 1.6699830e-02, 3.4302339e-02, ...,\n",
      "         3.3625364e-03, 0.0000000e+00, 0.0000000e+00]],\n",
      "\n",
      "       [[2.4491664e-02, 2.7264195e-02, 1.5420880e-02, ...,\n",
      "         3.4583785e-02, 7.6724887e-03, 0.0000000e+00]]], dtype=float32)]\n",
      "(1, 275465, 1, 30)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "hidden_layers = keras.backend.function(\n",
    "[lstmgan.discriminator.layers[0].input],  # we will feed the function with the input of the first layer  \n",
    "[lstmgan.discriminator.layers[0].output,] # we want to get the output of the first layer\n",
    ")\n",
    "h=hidden_layers([df_train])\n",
    "print(h)\n",
    "print(np.shape(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=np.array(h)  \n",
    "\n",
    "arr_reshaped = np.array(h).reshape(275465, 30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275465, 30)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(arr_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('abnormal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.         0.         0.02801404 0.         0.         0.01566972\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.01669469 0.03374875 0.         0.         0.05120272\n",
      "   0.05212622 0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.01351188 0.         0.01273429]]]\n"
     ]
    }
   ],
   "source": [
    "#To check 1st row\n",
    "print(h[:,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_1 = pd.read_csv('abnormal.csv')\n",
    "csv_2 = pd.read_csv('abnormallabel.csv')\n",
    "\n",
    "result = pd.concat([csv_1, csv_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"lgcabnormalresult.csv\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16ccad24c48>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['gen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16ccae907c8>,\n",
       " <matplotlib.lines.Line2D at 0x16ccaed2a08>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAfAUlEQVR4nO3deXxU9b3/8ddnJvtCQkhYAwRlUVTWuCCIqLhbbW/t7eZef9aHXfRaa7Ve+6u3V1vtvli7uNQdrVZrqaJo0boCQWQRkB3CEpIQA2Rf5nv/mMkwQwKZQIY5Ce/n48EjM2fOmfl8A7xz8j3f8/2acw4REfEuX6ILEBGRA1NQi4h4nIJaRMTjFNQiIh6noBYR8bikeLxpfn6+Kyoqisdbi4j0SosWLap0zhV09FpcgrqoqIiSkpJ4vLWISK9kZpv295q6PkREPE5BLSLicQpqERGPU1CLiHicglpExOMU1CIiHqegFhHxOE8F9W/eWMNbqysSXYaIiKd4KqgfeHMd766tTHQZIiKeElNQm1mumT1nZqvMbKWZTYlLMQaBgBYyEBGJFOst5L8G5jjnLjWzFCAjHsX4zGjVijMiIlE6DWoz6wNMB64CcM41AU3xKMbnM5TTIiLRYun6OAqoAB4xs8Vm9qCZZe67k5ldZ2YlZlZSUXFwFwR9BgEltYhIlFiCOgmYBDzgnJsI1AK37buTc+5Pzrli51xxQUGHM/V1XowZreqjFhGJEktQbwG2OOfmh54/RzC4u78Yn6GcFhGJ1mlQO+fKgFIzGxPadBawIi7FGDh1fYiIRIl11Me3gCdDIz7WA1fHoxh1fYiItBdTUDvnPgKK41wLPlPXh4jIvjx1Z6LPp64PEZF9eSuodcOLiEg7ngpqv7o+RETa8VRQm254ERFpx1NB7TPTpEwiIvvwVFD7faYzahGRfXgqqE191CIi7XgqqDUftYhIe54KanV9iIi056mgVteHiEh7ngpqzUctItKep4I6eMOLglpEJJKngjo4jjrRVYiIeIungtoMzfUhIrIPTwW132eaPU9EZB+eCmrNRy0i0p6ngtoMrfAiIrIPTwW1uj5ERNrzVFCr60NEpD2PBbW6PkRE9uWpoDbd8CIi0o6ngtpvhnJaRCSap4La59MNLyIi+/JUUJuW4hIRacdTQZ2W5KexRZN9iIhESoplJzPbCOwBWoEW51xxPIrJSPFT19QSj7cWEemxYgrqkDOcc5VxqwTISPVT29Qaz48QEelxPNX1kZmSRFNLgJZWdX+IiLSJNagd8JqZLTKz6zrawcyuM7MSMyupqKg4qGIyUvwA1DXrrFpEpE2sQT3VOTcJOB/4hplN33cH59yfnHPFzrnigoKCgyomIyXYE1PXqKAWEWkTU1A757aFvpYDLwAnxaOYzNTQGbUuKIqIhHUa1GaWaWbZbY+Bc4Dl8SgmPbktqHVGLSLSJpZRHwOAF8ysbf+nnHNz4lFMZmqwnNpGnVGLiLTpNKidc+uB8YehFtJ1MVFEpB3PDc8DXUwUEYnkqaBuG55Xq4uJIiJhngzqel1MFBEJ81RQt/VR16uPWkQkzFNB7QuOLNFyXCIiETwV1H5fMKg1J7WIyF7eCuq2M2qt8iIiEuapoPb5DNNK5CIiUTwV1BA8q1ZQi4js5bmg9vlMXR8iIhE8F9R+LXArIhLFc0Gd5DO0wIuIyF6eC2qfz2gNKKlFRNp4Lqj96qMWEYniuaD2mbo+REQieS6o/T7dmSgiEslzQZ3k89GioBYRCfNcUPt8EFAftYhImOeCWncmiohE81xQ685EEZFongvqJJ/uTBQRieS5oPaZ6WKiiEgEzwW1X2fUIiJRPBnU6qMWEdnLc0Ht06gPEZEongvq4Ox5CmoRkTYxB7WZ+c1ssZnNjmtBCmoRkShdOaO+EVgZr0La+M10Z6KISISYgtrMCoELgQfjW05o1IdyWkQkLNYz6l8BtwL7nYDUzK4zsxIzK6moqDjogsw014eISKROg9rMLgLKnXOLDrSfc+5Pzrli51xxQUHBwRdkOqMWEYkUyxn1VOBiM9sIzALONLMn4laQgdMZtYhIWKdB7Zy73TlX6JwrAr4E/Ms5d1ncCtLFRBGRKJ4bR21maG1bEZG9krqys3PuTeDNuFQS4tPFRBGRKJ47o/aZoZwWEdnLe0GtpbhERKJ4LqhNFxNFRKJ4LqjV9SEiEs2DQa2uDxGRSB4Mat2ZKCISyXNBrbk+RESieS6o1UctIhLNg0GtM2oRkUgeDGoNzxMRieS5oDYzWjXXh4hImOeCWtOciohE81xQB5fiUlCLiLTxXFBrHLWISDTPBbXGUYuIRPNcUGsctYhINA8Gtc6oRUQieTCodTFRRCSS54LadDFRRCSK54Ja46hFRKJ5MKh1Ri0iEsmDQa2LiSIikTwX1BYanqfuDxGRIM8Ftc8MgMYWzcwkIgKeDOrg12PunENdU0tiixER8QDvBXVbUgPvrKlMYCUiIt7QaVCbWZqZLTCzJWb2sZndFc+CbG9Os6psTzw/SkSkR0iKYZ9G4EznXI2ZJQPvmNkrzrkP4lGQLyKpt+9qiMdHiIj0KJ0GtQsOv6gJPU0O/YnbkIyIng+adEFRRCS2Pmoz85vZR0A5MNc5N7+Dfa4zsxIzK6moqDj4giLOqJu0JpeISGxB7Zxrdc5NAAqBk8zs+A72+ZNzrtg5V1xQUHDQBSX795bU1NJ60O8jItJbdGnUh3OuGngTOC8u1QCDc9PDj9X1ISIS26iPAjPLDT1OB2YCq+JV0Ij8zPBjdX2IiMR2Rj0ImGdmS4GFBPuoZ8eroJH9s/jj5ZMZX5hDc4tuIxcRiWXUx1Jg4mGoJezc4wby5PzN7KpvPpwfKyLiSZ67M7FNit+nPmoRETwc1KlJPo36EBHBw0Gd7DddTBQRwcNB3Sc9mdKqesr36DZyETmyeTaoL5kwGIBfvLaaRZuqElyNiEjieDaoxxfmAjBrYSmff+D9BFcjIpI4ng3qJL9nSxMROayUhiIiHufpoP72mSMTXYKISMLFsnBAwtx8zhhaneMPb63HOYdFLv8iInKE8PQZNUD/7DRaA47yPY2JLkVEJCE8H9RjBmYD8PG2XQmuREQkMTwf1BOGBofpXfOXEt5affArx4iI9FSeD+q0ZD83zDgagB+/vJKGZs3/ISJHFs8HNcAt54zhns+dwKqyPZz187cU1iJyROkRQe3zGV85eRhXTy1ia3U9c5aXEVwcXUSk9+sRQd3m69ODXSA3PfMRc5aXJbgaEZHDo0cF9cCcNLJSg0O/b31+KY2ar1pEjgA9KqgB/n3rGQDsaWjhwt+8k+BqRETir8cFdV5mSvjx2vIa7puzijc/KWf7rvoEViUiEj89LqgBLjtlWPjx799cx1WPLGTKj/+VwIpEROKnRwZ1YD8DPqrrmg5vISIih0GPDOrzjhsIwE/+4wQevKI4vH3Jll08s3CzxlmLSK9i8RiPXFxc7EpKSrr9fSMFAg6fLzib3vKtu7jot3svLN7zuROYNjIfMxialxHXOkREuoOZLXLOFXf0Wo88owbCIQ1w/JAcBvRJDT/fUFnD9J/O47T75iWiNBGRbtVpUJvZUDObZ2YrzexjM7vxcBTWVa/eNJ1F/z2TU4/ux1/e2xjePnvpNib/aK5WMxeRHiuWM+oW4DvOuWOBU4BvmNnY+JbVdbkZKfTLSuU754ymuXVvd843n1rMztom7v/XWl5fsYPm1kACqxQR6bpOV3hxzm0Htoce7zGzlcAQYEWcazsoE4f2ZVheBk0tAcp27z2LfvT9TTz6/iYAfv/VSVxwwqBElSgi0iVd6qM2syJgIjC/g9euM7MSMyupqEjcvNE+n/HWd2fwwffP4guTCwG48axRUfvc8OSHPPTOhkSUJyLSZTGP+jCzLOAt4G7n3N8OtO/hGPURi6aWADWNLeSkJzN76TZunPVR1Ou/+uIEmlsDfKF4KAB/+3ALWz+t58qpRdzzz5UU5WeSmZrE5acMT0T5InIEOdCoj5gWtzWzZOB54MnOQtpLUpJ85CUFbzm/ZMIQ+mak8GxJKQ3NAV5fuYObngkGd25GCtNG5nPzs0sAGDc0l1kLS8Pv85lxg8jNSGn/ASIih0GnZ9QWXPr7UaDKOXdTLG/qlTPq/fmotJrP3v9uzPtfOrmQcYU5fOWkYTgg2d9jRzWKiEcd6Iw6lqCeBrwNLAPahkx83zn38v6O8XpQO+f4+Wur2bG7gbUVNSzeXB3TcdlpSeRmJPP2rWeG3yf4c0xE5NAcUlAfDK8H9b4+rW1i4o/mxrz/JRMG88qyMgqyUxmSm86z10+JY3UiciTolXcmdqe+mSks+P5ZTBuZD8B3zx1Dkm//Z8p//2gbTa0BtlbXs2Bj1eEqU0SOUDFdTDwS9O+TxhPXnhx+/oXJhfzP7BV888yRHDOwDwAPvLmOe+esanfszppG3llbybxV5fzk8+NIS/YftrpFpPdT10cXddZNcsmEwZwxpj/9s1Ppm5nCmAHZUfOSiIh0RH3U3ez1FTu49rHY23freWMIBBwThvZl2qj8OFYmIj2VgjoO9jQ088H6Km57fik3zhzF22sqmbtixwGPKchOZeEdM3lvXSUFWakUZKdqfLaIAArqw6I14JizvIw7XlxGdV3zfve7emoRj7y7EYBR/bOYe/PpACwpraZ/n1QG5aQfjnJFxGMU1IdZWz/24Jw0Li0eym/eWLPffb933jHMGFPA+b9+G4AbZhzNy8u2c9WpRXz1lOG6uUbkCKGgToC2FWgCAcctf13Clup6Fmzo2lC+7NQklt11LrsbmvnV3DXccMbR5Geldn6giPQ4CmoPcC7YNVJclMeJd78OwNenH8Uf/73+gMdNG5nPcYP7hPdbc/f5bP20nkG5aaQmaRigSG+hoPaY+et34vcZxUV5vL5iB1W1Tfzy9dVs39W1VWi+f8ExzF9fxYadtTx2zUkU9tX6kCI9lYK6B3DOsa6iBr/Pxxk/e7PLxw/NS+ftW8/k16+vobioLxOG5uL3GVur66mqbeLEojxKq+oo293AiUV53d8AETkkhzzNqcSfmTGyfzYAq350HsfcOYfxQ3MJBBzLtu7i7s8dzx0vLN/v8aVV9dw/by2/fH11h6//4bJJXP/EhwBs+PEFNLUGaGoJ0NgSUL+3iMfpjNqjtlXXk5OeTGZqEos3f8r4wlx+8NJynvhgM8t+eA6Pvb+Jn776Cd8+axSXTBjMWT9/K+b3vvOisfxodvRKar/98kQAPjN+cLe2Q0Rio66PXqKlNUD5nkYG56bjnKOqtol+Wak45zj2B3NoaN67cO+4whyWbtlFWrIvvH1Ibjpbq+sP+Bmv33w6g3LSyExNYtPOWppbHTtrGrn+iUUMzEnnhRtOZfWOPby9ppKMFD9XTx0RdfyntU3sbmhmeL/M7v8GiPRiCuojwLbqel5ZXsZXThpGSyBAdlpy+LW2ebOdc4y4PTiN+DVTR/Dwuxs49eh+vLdu50F/blZqEjWNLfTNSObTiBt95t0ygxH5mdQ1tTB3xQ4uHj9Yc3eLHICCWsKqaptobg3QPzuVVWV7OGZgNo0tAXbXN/PSkm38+e317Njd2O2fe9WpRfzw4uO6dExDcyvOQXqKhiFK76eglpjtaWjmjZXlLN+6i+KiPM49bgDXPlrCG6vKD/m9M1P8TDm6H++sreTYQX0YlJPG+opafvaF8Rw7qA/1za1kJPvx+YzSqjpOu28eqUk+5t0ygx/8fTk//88J5KQnd/5BIj2QgloOySPvbuCuf6zgt1+eyIj8TO54cTm//uIENlTWMmZgNoNz0ym67Z8AnDYqn63V9QzJTWfpll3sqt//vCeRvnTi0PCCwn+8fDJff3xRu31Sknz818zRXD21iLRkP+V7Glhauouzju0f7tpxjv1OK1u+p4H+2Wn7raG+qZVW58hK1WAoOfwU1HJIWgOON1bu4OyxA/bbz7yxspbZS7fxjTNGRu1z2/NLWbChivWVte2OOWFIDsu27mq3PcXvo6k10G57mz5pSYwZmM3CjZ8CMCI/k9QkH9t3NfAfk4ZwUlEeZrB0yy6+e+4YzIz31lbylQfn8+crijnzmP74fUZza4CAc+E7PIv/dy6VNU1s/MmFXfr+iHQHBbUkXENzK7WNLfxu3lrqGlu58tQixg4OrpzTdjb+5yuK6ZeVwnWPlVBZ09Rtnz19dAGlVXVsqKwl2W80tzpe+uZUfjR7Bat31LDov2dStruBaffOA4K36b+4eCv52akYMGNM/y59Xtv/KV08la5QUIunPVtSyl/e3cjsb03D5zP2NDSzfVcDRf0yWVW2m9c+3sHv5q1NWH3r77mApxZsZlheBsVFfUlN8uOzYBCvKttNQVYq1fXNVO5pZNLwvoy64xVuPns03zoz+reLuqYWMlLUrSIdU1BLjzd3xQ7mr99Jn/Rkahtb+MeSbfzz26fx5T9/QGVNIw9eeSKjB2Txz6XbOX1MAbvrm5n5i393y2ePHdSHFdt3t9v+uYlDeGHx1qhtv//qJG548sPw8+OH9OHJa09hzY49XPqH93niayczbVQ+97y8kvfWVTL7W6exesceji7Iwt9B33og4HhpyTYuGjeIJE1526spqKXXam4NUNPQQt/M9ivlLN+6iwUbqviffe7CXHfPBcz8xVtsq67noStP5LKH5se9zrZ+96tOLaIgO5WfvvoJAH+4bDLXP7GIicNyefr/ncKaHTW8sHgr44fmcPyQHBZvruaWvy7hO2eP5ptnjuTOvy8nPyuVm2aO3u9nPfHBJppaAlwzbcR+9xHvUVDLEe2j0mqOGZjN7KXbWbl9N3deNDbq9e89t5RnSkp58RtTufeVVUwfXUBdUwsfrN8ZvmB5xwXHcvqYAs75ZfRZ+sxjB/D6yh3hr505e+yAqCXbMlP81Da1drlNz10/hZrGFm5+dgkXjRtETWML00cV8NmJQ8J9/m0XRStrGvGZkRfxw+y+OauYOKwvZ48dEN72wfqd3PPySp65borGrieAglrkAFoDjqaWQIfhtLW6nsE5aeG+5rfXVLChspamlgBJPuOqqSPCd37+8KWP+ct7G8PHfufs0Tz07oYOl2Yb2T+LteU13d6WyMm3phzVjwvGDeLOF4OTed163hiOH5zDyUflMea/5wCw9u7zcUCy3xcO+Me/dhKnjSrge88tJTnJuP38Y6lvbtXkXXGmoBY5DFpaA2yorGVNeQ3rymv45pkjCThYVbab+eurWFtRw1PzNwPw5i0z+Nlrn7B6xx5W79gb2MXD+zK5qC+Th/WlNeD4xlMfEuj+/6JRstOSOLEoj3+FbmqaMaaAC44fxK3PLwXgqPxM1lfWsvEnF7Ktup7M1CReXradGWMK+LS2mWMHZYd/kAUCjo07azmqICv8A0xic0hBbWYPAxcB5c6542P5QAW1SMcamlv5pGwP44fmhrf99o01PLuolJ9dOp5jBvYhJyP67svxd70W041Dl04u5LlFW7q95jZXThnOo+9v6vC1566fQtnuBmYtKOWdtZWM6p/FmvIaHrnqRIbmZfDqx2VcMWU4c1fsIOCCtQJs31XPtHvn8cOLj+PyU4YDwa6q0qo6fjl3NS99a1qnNyA1NLeSmuTr8T8UDjWopwM1wGMKapHDb2dNI9X1zVTXNZGa5CfgHPfOWcW7a3ey8I6ZXPvoQnY3tPDYNSdx46zFfLi5OnzsuMIcZl13CnOWl3HXP1Zw0bhBnHPcQG6ctZjqumZy0pOjfgi0Ta51YlHfcP98vAzNS2d0/+zw9ASFfdO5YcZIvv/CsvA+z359Ci2tAcYO7kOftGT+uWw7Rxdksbmqluq6Zo4bnMNnfvcOX5hcyKadddx36TiK8vc/c+PyrbsY2T+LtORgN9eS0mqOHdSHlKTEj6g55K4PMysCZiuoRbzBOUdLwJHs9xEIOMyib7BZtKmKFxdv486LxnYYQqvKdrO2vIYLTxjET1/9hKF5GQztm8Gk4bm8sbKc844fyMvLtnPjrI/4z+JCynY38u/VFe3eZ0CfVMYX5vLais4vpB6M/KxUKmu6PknY/V+ZxKsfl3H5lOEk+30Y8PyHW3gs9BvBCUNy+N/PHs8l97/LpGG5PHzVieRm7L3YurW6ntKqOk4ekRf+vt4/by0bK2vZ8mk9f7xiMn3SunfemcMS1GZ2HXAdwLBhwyZv2tTxr0gi0jO0Bhx/eW8jl04qJCPVz83PLuEfS7bx5yuKyc1I5oE313H7+cfw2PubePyDTUwd2Y931+7khRtOZWT/LN78pIJ756xiy6d750C/ePxg/rF0G7npe6fFPWfsgLgFfVe8etN0/lpSyoPvbAhvu3LKcL588jAM49xftR+X//K3T+OKhxdQWdPITy8dx6kj8xmSm35Qn68zahE5ZE0tAarrm9pNbLWrvpm311Rw4QmDOpwUa94n5Vz9yEIguAxc2xnqsyWlnDwij2F5Gby3bid1Ta0My8tg1sLNjCvM4en5pSzYWMWpR/dj0866The98IL8rBTe+u4ZZB7ExF4KahFJqA2Vtawtr4kat92ZQMBR09RCdmoSZkZNYwt/LSnlnOMGUtvYwoA+aYy/6zUA7rr4OK6YMpyrHlnIWx100XQHM+gsLq+ZOoIffGbsgXfa7/trcVsRSaAR+ZmMOMBFvo74fBbVD5yVmhS19FvkSeaVpxYB8Og1J3H735by9IJSXr/5dJL9Rm56Cq+uKOPkEXk8/v4mLhw3iPvnreOLJw5l+uh8qmqbeGdNJU/M38zjXzuJd9ZUcnRBFjWNLfxy7mrM4N7Pj2NwbjpNLQGufHgB76/veFWk75yz/ztGD0Usoz6eBmYA+cAO4P875x460DE6oxaRw+EXc1dz8og8po7MD29raG5lc1Udowdkx+1zy/c0UJCVipnxzMLNzFlexucmFXLxISwOrRteREQ87kBBnfjBgyIickAKahERj1NQi4h4nIJaRMTjFNQiIh6noBYR8TgFtYiIxymoRUQ8TkEtIuJxCmoREY/z1qRMq18D1/UVmbtNoBUCzZCUBv1Gwq5SSO0DTbXB1zMLoK5y73MRkUj+FBh5Vre/rbeC+q9XQnNdoqsQETk4mf3hu2u6/W29FdTXzAEXSNznz7kddq6FIZNh9Zx9XjQgNIHV5S9Aet/DXZ2IeJ0vPpHqraAeND6xn3/1K8GvCx+MDupbN0BKVrBbxPyQnNbx8SIiceCtoE60tsVBI38q3r4FUtvmtU1pd4iISLwpqDsy7ovBLpDTb40IaRGRxFBQdyQlA869O9FViIgAGkctIuJ5CmoREY9TUIuIeJyCWkTE4xTUIiIep6AWEfE4BbWIiMcpqEVEPM6cc93/pmYVwKaDPDwfqOzGcnoCtfnIoDb3fofS3uHOuYKOXohLUB8KMytxzhUnuo7DSW0+MqjNvV+82quuDxERj1NQi4h4nBeD+k+JLiAB1OYjg9rc+8WlvZ7roxYRkWhePKMWEZEICmoREY/zTFCb2Xlm9omZrTWz2xJdT3cxs6FmNs/MVprZx2Z2Y2h7npnNNbM1oa99I465PfR9+MTMzk1c9YfGzPxmttjMZoee9+o2m1mumT1nZqtCf99TjoA2/1fo3/VyM3vazNJ6W5vN7GEzKzez5RHbutxGM5tsZstCr/3GrG3tvxg45xL+B/AD64CjCC5MuAQYm+i6uqltg4BJocfZwGpgLHAfcFto+23AvaHHY0PtTwVGhL4v/kS34yDbfjPwFDA79LxXtxl4FLg29DgFyO3NbQaGABuA9NDzZ4GrelubgenAJGB5xLYutxFYAEwBDHgFOD/WGrxyRn0SsNY5t9451wTMAi5JcE3dwjm33Tn3YejxHmAlwX/glxD8j03o62dDjy8BZjnnGp1zG4C1BL8/PYqZFQIXAg9GbO61bTazPgT/Qz8E4Jxrcs5V04vbHJIEpJtZEpABbKOXtdk592+gap/NXWqjmQ0C+jjn3nfB1H4s4phOeSWohwClEc+3hLb1KmZWBEwE5gMDnHPbIRjmQP/Qbr3le/Er4FYgELGtN7f5KKACeCTU3fOgmWXSi9vsnNsK/AzYDGwHdjnnXqMXtzlCV9s4JPR43+0x8UpQd9RX06vGDZpZFvA8cJNzbveBdu1gW4/6XpjZRUC5c25RrId0sK1HtZngmeUk4AHn3ESgluCvxPvT49sc6pe9hOCv+IOBTDO77ECHdLCtR7U5Bvtr4yG13StBvQUYGvG8kOCvUL2CmSUTDOknnXN/C23eEfp1iNDX8tD23vC9mApcbGYbCXZjnWlmT9C727wF2OKcmx96/hzB4O7NbZ4JbHDOVTjnmoG/AafSu9vcpqtt3BJ6vO/2mHglqBcCo8xshJmlAF8CXkpwTd0idGX3IWClc+4XES+9BFwZenwl8PeI7V8ys1QzGwGMIngRosdwzt3unCt0zhUR/Lv8l3PuMnp3m8uAUjMbE9p0FrCCXtxmgl0ep5hZRujf+VkEr8H05ja36VIbQ90je8zslND36oqIYzqX6CuqEVdRLyA4ImIdcEei6+nGdk0j+CvOUuCj0J8LgH7AG8Ca0Ne8iGPuCH0fPqELV4a9+AeYwd5RH726zcAEoCT0d/0i0PcIaPNdwCpgOfA4wdEOvarNwNME++CbCZ4Zf+1g2ggUh75P64DfEbozPJY/uoVcRMTjvNL1ISIi+6GgFhHxOAW1iIjHKahFRDxOQS0i4nEKahERj1NQi4h43P8BeMjQHEEe/UUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['dis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature22</th>\n",
       "      <th>feature23</th>\n",
       "      <th>feature24</th>\n",
       "      <th>feature25</th>\n",
       "      <th>feature26</th>\n",
       "      <th>feature27</th>\n",
       "      <th>feature28</th>\n",
       "      <th>feature29</th>\n",
       "      <th>feature30</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006962</td>\n",
       "      <td>0.055625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048416</td>\n",
       "      <td>0.014631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013936</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051723</td>\n",
       "      <td>0.013089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0       0.0  0.006962  0.055625       0.0       0.0       0.0  0.036116   \n",
       "1       0.0  0.052188  0.000000       0.0       0.0       0.0  0.047845   \n",
       "2       0.0  0.048416  0.014631       0.0       0.0       0.0  0.045899   \n",
       "3       0.0  0.000000  0.038806       0.0       0.0       0.0  0.000000   \n",
       "4       0.0  0.051723  0.013089       0.0       0.0       0.0  0.047847   \n",
       "\n",
       "   feature8  feature9  feature10  ...  feature22  feature23  feature24  \\\n",
       "0       0.0       0.0        0.0  ...        0.0   0.000000   0.000318   \n",
       "1       0.0       0.0        0.0  ...        0.0   0.000000   0.000000   \n",
       "2       0.0       0.0        0.0  ...        0.0   0.000000   0.000000   \n",
       "3       0.0       0.0        0.0  ...        0.0   0.027111   0.000000   \n",
       "4       0.0       0.0        0.0  ...        0.0   0.000000   0.000000   \n",
       "\n",
       "   feature25  feature26  feature27  feature28  feature29  feature30  label  \n",
       "0        0.0        0.0        0.0   0.021632        0.0   0.000000      0  \n",
       "1        0.0        0.0        0.0   0.057461        0.0   0.000000      0  \n",
       "2        0.0        0.0        0.0   0.059599        0.0   0.000000      0  \n",
       "3        0.0        0.0        0.0   0.044531        0.0   0.013936      0  \n",
       "4        0.0        0.0        0.0   0.060423        0.0   0.000000      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test=df1.label\n",
    "\n",
    "x_test=df1.drop('label',axis=1).values.reshape(-1,30,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.1866512 ]]\n",
      "\n",
      " [[0.19458453]]\n",
      "\n",
      " [[0.19500764]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.19296129]]\n",
      "\n",
      " [[0.19483964]]\n",
      "\n",
      " [[0.19457051]]]\n"
     ]
    }
   ],
   "source": [
    "#y_pred=lstmgan.generator.predict(x_test)\n",
    "y_pred=lstmgan.discriminator.predict(x_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "yhat_probs = lstmgan.discriminator.predict(x_test, verbose=0)\n",
    "yhat_classes=np.argmax(yhat_probs,axis=1)\n",
    "# predict crisp classes for test set\n",
    "#yhat_classes = model.predict_classes(x_val, verbose=0)\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "#yhat_classes = yhat_classes[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-0ee39934b722>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# We set the threshold equal to the training loss of the autoencoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtesting_set_predictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlstmgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "# Helper function that calculates the reconstruction loss of each data sample\n",
    "def calculate_losses(x,preds):\n",
    "    losses=np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        losses[i]=((preds[i] - x[i]) ** 2).mean(axis=None)\n",
    "        \n",
    "    return losses\n",
    "\n",
    "# We set the threshold equal to the training loss of the autoencoder\n",
    "threshold=history.history[\"loss\"][-1]\n",
    "\n",
    "testing_set_predictions=lstmgan.discriminator.predict(x_test)\n",
    "test_losses=calculate_losses(x_test,testing_set_predictions)\n",
    "testing_set_predictions=np.zeros(len(test_losses))\n",
    "testing_set_predictions[np.where(test_losses>threshold)]=1\n",
    "\n",
    "\n",
    "error_df = pd.DataFrame({'Reconstruction_error': testing_set_predictions, 'True_class': y_test})\n",
    "error_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,precision_score,f1_score\n",
    "accuracy=accuracy_score(y_val,testing_set_predictions)\n",
    "recall=recall_score(y_val,testing_set_predictions)\n",
    "precision=precision_score(y_val,testing_set_predictions)\n",
    "f1=f1_score(y_val,testing_set_predictions)\n",
    "print(\"Performance over the testing data set \\n\")\n",
    "print(\"Accuracy : {} , Recall : {} , Precision : {} , F1 : {}\\n\".format(accuracy,recall,precision,f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/#:~:text=Optimal%20Threshold%20for%20Precision%2DRecall%20Curve,-Unlike%20the%20ROC&text=Recall%20is%20calculated%20as%20the,positives%20and%20the%20false%20negatives.\n",
    "#Optimal Threshold Tuning\n",
    "# search thresholds for imbalanced classification\n",
    "from numpy import arange\n",
    "from numpy import argmax\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')\n",
    " \n",
    "\n",
    "# predict probabilities\n",
    "#yhat = model.predict_proba(x_val)\n",
    "# keep probabilities for the positive outcome only\n",
    "#probs = yhat[:, 1]\n",
    "# define thresholds\n",
    "thresholds = arange(0, 1, 0.001)\n",
    "# evaluate each threshold\n",
    "scores = [f1_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each threshold\n",
    "scores = [accuracy_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, Accuracy-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each threshold\n",
    "scores = [precision_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, Precision-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each threshold\n",
    "scores = [recall_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, recall-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# ROC AUC\n",
    "auc = roc_auc_score(y_test, yhat_probs)\n",
    "print('ROC AUC: %f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=8\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(df_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "#x_test = keras.preprocessing.sequence.pad_sequences(x_test , maxlen=maxlen)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "#https://www.kaggle.com/suicaokhoailang/10-fold-lstm-with-attention-0-991-lb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import sklearn.metrics as sk\n",
    "from sklearn.metrics import roc_curve,f1_score\n",
    "from sklearn.metrics import auc\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=42069)\n",
    "preds = []\n",
    "fold = 0\n",
    "aucs = 0\n",
    "for train_idx, val_idx in kf.split(x_train):\n",
    "    x_train_f = x_train[train_idx]\n",
    "    y_train_f = y_test[train_idx]\n",
    "    x_val_f = x_train[val_idx]\n",
    "    y_val_f = y_test[val_idx]\n",
    "    lstmgan.discriminator.compile(optimizer, \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    #model = get_model()\n",
    "    lstmgan.discriminator.fit(x_train_f, y_train_f,\n",
    "              batch_size=256,\n",
    "              epochs=10,\n",
    "              verbose = 1,\n",
    "              validation_data=(x_val_f, y_val_f))\n",
    "\n",
    "    # Get accuracy of model on validation data. It's not AUC but it's something at least!\n",
    "    preds_val = lstmgan.discriminator.predict([x_val_f], batch_size=512)\n",
    "    preds.append(lstmgan.discriminator.predict(x_test))\n",
    "    fold+=1\n",
    "    fpr, tpr, thresholds = roc_curve(y_val_f, preds_val, pos_label=1)\n",
    "    # calculate scores\n",
    "    #lr_f1, lr_auc = f1_score(testy, yhat), auc(lr_recall, lr_precision)\n",
    "    aucs += auc(fpr,tpr)\n",
    "    print('Fold {}, AUC = {}'.format(fold,auc(fpr, tpr)))\n",
    "print(\"Cross Validation AUC = {}\".format(aucs/10))\n",
    "#print(sk.confusion_matrix(y_val_f,preds_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, accuracy_score ,recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#range of output value\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(range(len(y_pred)),y_pred.reshape(-1,1))\n",
    "plt.ylabel('y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for the optimal probability threshold\n",
    "#prob=[0.5,0.55,0.6,0.65,0.75,0.8,0.85,0.9,0.95,1,2,2.5,5]\n",
    "prob=[-0.2,-0.1,-0.05,0,0.005,0.1,0.157,0.2,0.3,0.5,1,1.5,2]\n",
    "for p in prob:\n",
    "    pred_value =[1 if i<p   else 0 for i in y_pred]\n",
    "    f1=f1_score(y_test,pred_value)\n",
    "    acc=accuracy_score(y_test,pred_value)\n",
    "    precision=average_precision_score(y_test,pred_value)\n",
    "    recall= recall_score(y_test,pred_value)\n",
    "    print(f'prob ={p} and f1 score ={f1} : accuracy={acc} : precision={precision} : recall={recall}')\n",
    "    print(pred_value.count(0),pred_value.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=dict()\n",
    "history['gen']=[]\n",
    "history['dis']=[]\n",
    "def train1(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for batch in dataset:\n",
    "\n",
    "           gen_loss,dis_loss= lstmgan.train(batch)\n",
    "        history['gen'].append(gen_loss)\n",
    "        history['dis'].append(dis_loss)\n",
    "        print ('Time for epoch {} is {} sec '.format(epoch + 1, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = lstmgan.discriminator.evaluate(x_test, y_test)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = lstmgan.generator.predict_classes(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average=\"binary\")\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"binary\")\n",
    "#np.savetxt('results/expected1.txt', y_test, fmt='%01d')\n",
    "#np.savetxt('results/predicted1.txt', y_pred, fmt='%01d')\n",
    "\n",
    "print(\"confusion matrix\")\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"accuracy\")\n",
    "print(\"%.6f\" %accuracy)\n",
    "print(\"racall\")\n",
    "print(\"%.6f\" %recall)\n",
    "print(\"precision\")\n",
    "print(\"%.6f\" %precision)\n",
    "print(\"f1score\")\n",
    "print(\"%.6f\" %f1)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"==============================================\")\n",
    "print (cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
