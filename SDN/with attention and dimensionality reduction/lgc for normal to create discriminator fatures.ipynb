{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/function9/bidirectional-lstm-gan-music-generation\n",
    "#https://www.kaggle.com/abhisheksinha28/bidirectional-lstm/data\n",
    "#https://www.kaggle.com/sekfook97/gan-for-anomaly-detection\n",
    "# importing libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Bidirectional, LSTM, Reshape, RepeatVector, TimeDistributed\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "\"feature1\": np.float16,\n",
    "\"feature2\": np.float16,\n",
    "\"feature3\": np.float16,\n",
    "\"feature4\": np.float16,\n",
    "\"feature5\": np.float16,\n",
    "\"feature6\": np.float16,\n",
    "\"feature7\": np.float16,\n",
    "\"feature8\": np.float16,\n",
    "\"feature9\": np.float16,\n",
    "\"feature10\": np.float16,\n",
    "\"feature11\": np.float16,\n",
    "\"feature12\": np.float16,\n",
    "\"feature13\": np.float16,\n",
    "\"feature14\": np.float16,\n",
    "\"feature15\": np.float16,\n",
    "\"feature16\": np.float16,\n",
    "\"feature17\": np.float16,\n",
    "\"feature18\": np.float16,\n",
    "\"feature19\": np.float16,\n",
    "\"feature20\": np.float16,\n",
    "\"feature21\": np.float16,\n",
    "\"feature22\": np.float16,\n",
    "\"feature23\": np.float16,\n",
    "\"feature24\": np.float16,\n",
    "\"feature25\": np.float16,    \n",
    "\"feature26\": np.float16,\n",
    "\"feature27\": np.float16,\n",
    "\"feature28\": np.float16,\n",
    "\"feature29\": np.float16,\n",
    "\"feature30\": np.float16,    \n",
    "\"label\": np.object}    \n",
    "columns = [\"feature1\",\"feature2\",\"feature3\",\"feature4\",\"feature5\",\"feature6\",\"feature7\",\"feature8\",\"feature9\",\"feature10\",\"feature11\",\"feature12\",\"feature13\",\"feature14\",\"feature15\",\"feature16\",\"feature17\",\"feature18\",\"feature19\",\"feature20\",\"feature21\",\"feature22\",\"feature23\",\"feature24\",\"feature25\",\"feature26\",\"feature27\",\"feature28\",\"feature29\",\"feature30\",\"label\"]\n",
    "#df = pd.read_csv(\"/kaggle/input/kdd-cup-1999-data/kddcup.data_10_percent_corrected\", sep=\",\", names=columns, dtype=dtypes, index_col=None)\n",
    "df = pd.read_csv(r\"C:\\Users\\admin\\SDN\\afterdimensionalityreduction\",sep=\",\", names=columns, dtype=str, index_col=None)\n",
    "#df_read = pd.read_csv(savefile, dtype=str, index_col=0)\n",
    "#print(type(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Abnormal    275465\n",
       "Normal       68424\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        feature1              feature2              feature3 feature4 feature5  \\\n",
       "0           0.0  0.006962014399999999            0.05562538      0.0      0.0   \n",
       "1           0.0            0.05218774                   0.0      0.0      0.0   \n",
       "2           0.0           0.048415996           0.014630627      0.0      0.0   \n",
       "3           0.0                   0.0           0.038806356      0.0      0.0   \n",
       "4           0.0            0.05172342            0.01308862      0.0      0.0   \n",
       "...         ...                   ...                   ...      ...      ...   \n",
       "343884      0.0            0.04297894           0.017960861      0.0      0.0   \n",
       "343885      0.0            0.04324762           0.014465368      0.0      0.0   \n",
       "343886      0.0           0.047583852                   0.0      0.0      0.0   \n",
       "343887      0.0            0.04297537  0.017960499999999997      0.0      0.0   \n",
       "343888      0.0            0.04319027           0.014465845      0.0      0.0   \n",
       "\n",
       "       feature6              feature7 feature8              feature9  \\\n",
       "0           0.0           0.036115788      0.0                   0.0   \n",
       "1           0.0           0.047844697      0.0                   0.0   \n",
       "2           0.0            0.04589876      0.0                   0.0   \n",
       "3           0.0                   0.0      0.0                   0.0   \n",
       "4           0.0           0.047847077      0.0                   0.0   \n",
       "...         ...                   ...      ...                   ...   \n",
       "343884      0.0           0.021094704      0.0            0.04364106   \n",
       "343885      0.0             0.0216345      0.0  0.043688737000000005   \n",
       "343886      0.0            0.02868637      0.0            0.03592718   \n",
       "343887      0.0  0.021132400000000003      0.0           0.043612387   \n",
       "343888      0.0            0.02155185      0.0            0.04372048   \n",
       "\n",
       "       feature10  ... feature22    feature23      feature24 feature25  \\\n",
       "0            0.0  ...       0.0          0.0  0.00031781942       0.0   \n",
       "1            0.0  ...       0.0          0.0            0.0       0.0   \n",
       "2            0.0  ...       0.0          0.0            0.0       0.0   \n",
       "3            0.0  ...       0.0  0.027110513            0.0       0.0   \n",
       "4            0.0  ...       0.0          0.0            0.0       0.0   \n",
       "...          ...  ...       ...          ...            ...       ...   \n",
       "343884       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "343885       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "343886       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "343887       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "343888       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "\n",
       "       feature26 feature27    feature28 feature29    feature30     label  \n",
       "0            0.0       0.0  0.021631505       0.0          0.0    Normal  \n",
       "1            0.0       0.0  0.057460763       0.0          0.0    Normal  \n",
       "2            0.0       0.0   0.05959917       0.0          0.0    Normal  \n",
       "3            0.0       0.0  0.044530995       0.0  0.013936298    Normal  \n",
       "4            0.0       0.0  0.060422726       0.0          0.0    Normal  \n",
       "...          ...       ...          ...       ...          ...       ...  \n",
       "343884       0.0       0.0  0.023048477       0.0          0.0  Abnormal  \n",
       "343885       0.0       0.0  0.023150168       0.0          0.0  Abnormal  \n",
       "343886       0.0       0.0  0.021780338       0.0          0.0  Abnormal  \n",
       "343887       0.0       0.0  0.023061449       0.0          0.0  Abnormal  \n",
       "343888       0.0       0.0  0.023140473       0.0          0.0  Abnormal  \n",
       "\n",
       "[343889 rows x 31 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 for normal data and 1 for abnormalities\n",
    "df.label=df.label.apply(lambda x: 0 if x == 'Normal' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(343889, 31)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        feature1              feature2              feature3 feature4 feature5  \\\n",
       "0           0.0  0.006962014399999999            0.05562538      0.0      0.0   \n",
       "1           0.0            0.05218774                   0.0      0.0      0.0   \n",
       "2           0.0           0.048415996           0.014630627      0.0      0.0   \n",
       "3           0.0                   0.0           0.038806356      0.0      0.0   \n",
       "4           0.0            0.05172342            0.01308862      0.0      0.0   \n",
       "...         ...                   ...                   ...      ...      ...   \n",
       "343884      0.0            0.04297894           0.017960861      0.0      0.0   \n",
       "343885      0.0            0.04324762           0.014465368      0.0      0.0   \n",
       "343886      0.0           0.047583852                   0.0      0.0      0.0   \n",
       "343887      0.0            0.04297537  0.017960499999999997      0.0      0.0   \n",
       "343888      0.0            0.04319027           0.014465845      0.0      0.0   \n",
       "\n",
       "       feature6              feature7 feature8              feature9  \\\n",
       "0           0.0           0.036115788      0.0                   0.0   \n",
       "1           0.0           0.047844697      0.0                   0.0   \n",
       "2           0.0            0.04589876      0.0                   0.0   \n",
       "3           0.0                   0.0      0.0                   0.0   \n",
       "4           0.0           0.047847077      0.0                   0.0   \n",
       "...         ...                   ...      ...                   ...   \n",
       "343884      0.0           0.021094704      0.0            0.04364106   \n",
       "343885      0.0             0.0216345      0.0  0.043688737000000005   \n",
       "343886      0.0            0.02868637      0.0            0.03592718   \n",
       "343887      0.0  0.021132400000000003      0.0           0.043612387   \n",
       "343888      0.0            0.02155185      0.0            0.04372048   \n",
       "\n",
       "       feature10  ... feature22    feature23      feature24 feature25  \\\n",
       "0            0.0  ...       0.0          0.0  0.00031781942       0.0   \n",
       "1            0.0  ...       0.0          0.0            0.0       0.0   \n",
       "2            0.0  ...       0.0          0.0            0.0       0.0   \n",
       "3            0.0  ...       0.0  0.027110513            0.0       0.0   \n",
       "4            0.0  ...       0.0          0.0            0.0       0.0   \n",
       "...          ...  ...       ...          ...            ...       ...   \n",
       "343884       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "343885       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "343886       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "343887       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "343888       0.0  ...       0.0          0.0            0.0       0.0   \n",
       "\n",
       "       feature26 feature27    feature28 feature29    feature30 label  \n",
       "0            0.0       0.0  0.021631505       0.0          0.0     0  \n",
       "1            0.0       0.0  0.057460763       0.0          0.0     0  \n",
       "2            0.0       0.0   0.05959917       0.0          0.0     0  \n",
       "3            0.0       0.0  0.044530995       0.0  0.013936298     0  \n",
       "4            0.0       0.0  0.060422726       0.0          0.0     0  \n",
       "...          ...       ...          ...       ...          ...   ...  \n",
       "343884       0.0       0.0  0.023048477       0.0          0.0     1  \n",
       "343885       0.0       0.0  0.023150168       0.0          0.0     1  \n",
       "343886       0.0       0.0  0.021780338       0.0          0.0     1  \n",
       "343887       0.0       0.0  0.023061449       0.0          0.0     1  \n",
       "343888       0.0       0.0  0.023140473       0.0          0.0     1  \n",
       "\n",
       "[343889 rows x 31 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     object\n",
       "feature2     object\n",
       "feature3     object\n",
       "feature4     object\n",
       "feature5     object\n",
       "feature6     object\n",
       "feature7     object\n",
       "feature8     object\n",
       "feature9     object\n",
       "feature10    object\n",
       "feature11    object\n",
       "feature12    object\n",
       "feature13    object\n",
       "feature14    object\n",
       "feature15    object\n",
       "feature16    object\n",
       "feature17    object\n",
       "feature18    object\n",
       "feature19    object\n",
       "feature20    object\n",
       "feature21    object\n",
       "feature22    object\n",
       "feature23    object\n",
       "feature24    object\n",
       "feature25    object\n",
       "feature26    object\n",
       "feature27    object\n",
       "feature28    object\n",
       "feature29    object\n",
       "feature30    object\n",
       "label         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df)   \n",
    " \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['feature1'] = df['feature1'].astype(float) \n",
    "df['feature2'] = df['feature2'].astype(float) \n",
    "df['feature3'] = df['feature3'].astype(float) \n",
    "df['feature4'] = df['feature4'].astype(float) \n",
    "df['feature5'] = df['feature5'].astype(float) \n",
    "df['feature6'] = df['feature6'].astype(float) \n",
    "df['feature7'] = df['feature7'].astype(float) \n",
    "df['feature8'] = df['feature8'].astype(float) \n",
    "df['feature9'] = df['feature9'].astype(float) \n",
    "df['feature10'] = df['feature10'].astype(float) \n",
    "df['feature11'] = df['feature11'].astype(float) \n",
    "df['feature12'] = df['feature12'].astype(float) \n",
    "df['feature13'] = df['feature13'].astype(float) \n",
    "df['feature14'] = df['feature14'].astype(float) \n",
    "df['feature15'] = df['feature15'].astype(float) \n",
    "df['feature16'] = df['feature16'].astype(float) \n",
    "df['feature17'] = df['feature17'].astype(float) \n",
    "df['feature18'] = df['feature18'].astype(float) \n",
    "df['feature19'] = df['feature19'].astype(float) \n",
    "df['feature20'] = df['feature20'].astype(float) \n",
    "df['feature21'] = df['feature21'].astype(float) \n",
    "df['feature22'] = df['feature22'].astype(float) \n",
    "df['feature23'] = df['feature23'].astype(float) \n",
    "df['feature24'] = df['feature24'].astype(float) \n",
    "df['feature25'] = df['feature25'].astype(float) \n",
    "df['feature26'] = df['feature26'].astype(float) \n",
    "df['feature27'] = df['feature27'].astype(float) \n",
    "df['feature28'] = df['feature28'].astype(float)\n",
    "df['feature29'] = df['feature29'].astype(float) \n",
    "df['feature30'] = df['feature30'].astype(float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly sample 500 data point for training\n",
    "df_train=df[df.label==0].sample(68424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68424, 31)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     float64\n",
       "feature2     float64\n",
       "feature3     float64\n",
       "feature4     float64\n",
       "feature5     float64\n",
       "feature6     float64\n",
       "feature7     float64\n",
       "feature8     float64\n",
       "feature9     float64\n",
       "feature10    float64\n",
       "feature11    float64\n",
       "feature12    float64\n",
       "feature13    float64\n",
       "feature14    float64\n",
       "feature15    float64\n",
       "feature16    float64\n",
       "feature17    float64\n",
       "feature18    float64\n",
       "feature19    float64\n",
       "feature20    float64\n",
       "feature21    float64\n",
       "feature22    float64\n",
       "feature23    float64\n",
       "feature24    float64\n",
       "feature25    float64\n",
       "feature26    float64\n",
       "feature27    float64\n",
       "feature28    float64\n",
       "feature29    float64\n",
       "feature30    float64\n",
       "label          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the item sampled from our dataset\n",
    "index_list=df_train.index\n",
    "df=df.drop(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     0.000000\n",
       "feature2     0.000000\n",
       "feature3     0.058928\n",
       "feature4     0.000000\n",
       "feature5     0.000000\n",
       "feature6     0.000000\n",
       "feature7     0.014441\n",
       "feature8     0.000000\n",
       "feature9     0.000000\n",
       "feature10    0.000000\n",
       "feature11    0.000000\n",
       "feature12    0.071845\n",
       "feature13    0.000000\n",
       "feature14    0.000000\n",
       "feature15    0.000000\n",
       "feature16    0.000000\n",
       "feature17    0.000000\n",
       "feature18    0.063959\n",
       "feature19    0.000000\n",
       "feature20    0.000000\n",
       "feature21    0.000000\n",
       "feature22    0.000000\n",
       "feature23    0.099892\n",
       "feature24    0.000000\n",
       "feature25    0.000000\n",
       "feature26    0.000000\n",
       "feature27    0.000000\n",
       "feature28    0.000000\n",
       "feature29    0.000000\n",
       "feature30    0.000000\n",
       "label        0.000000\n",
       "Name: 53142, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature22</th>\n",
       "      <th>feature23</th>\n",
       "      <th>feature24</th>\n",
       "      <th>feature25</th>\n",
       "      <th>feature26</th>\n",
       "      <th>feature27</th>\n",
       "      <th>feature28</th>\n",
       "      <th>feature29</th>\n",
       "      <th>feature30</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68424</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044392</td>\n",
       "      <td>0.009810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68425</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024184</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68426</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044602</td>\n",
       "      <td>0.009867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68427</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024209</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68428</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044153</td>\n",
       "      <td>0.009751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "68424       0.0  0.044392  0.009810       0.0       0.0       0.0  0.029762   \n",
       "68425       0.0  0.000000  0.017692       0.0       0.0       0.0  0.018432   \n",
       "68426       0.0  0.044602  0.009867       0.0       0.0       0.0  0.029689   \n",
       "68427       0.0  0.000000  0.017742       0.0       0.0       0.0  0.017482   \n",
       "68428       0.0  0.044153  0.009751       0.0       0.0       0.0  0.029908   \n",
       "\n",
       "       feature8  feature9  feature10  ...  feature22  feature23  feature24  \\\n",
       "68424       0.0  0.043584        0.0  ...        0.0        0.0        0.0   \n",
       "68425       0.0  0.000000        0.0  ...        0.0        0.0        0.0   \n",
       "68426       0.0  0.043520        0.0  ...        0.0        0.0        0.0   \n",
       "68427       0.0  0.000000        0.0  ...        0.0        0.0        0.0   \n",
       "68428       0.0  0.043653        0.0  ...        0.0        0.0        0.0   \n",
       "\n",
       "       feature25  feature26  feature27  feature28  feature29  feature30  label  \n",
       "68424        0.0   0.000000        0.0   0.024066        0.0   0.000000      1  \n",
       "68425        0.0   0.007034        0.0   0.000000        0.0   0.024184      1  \n",
       "68426        0.0   0.000000        0.0   0.024401        0.0   0.000000      1  \n",
       "68427        0.0   0.007084        0.0   0.000000        0.0   0.024209      1  \n",
       "68428        0.0   0.000000        0.0   0.023642        0.0   0.000000      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the label columns\n",
    "df_label=df_train.label\n",
    "df_train=df_train.drop('label',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label\n",
    "df_label.to_csv('normallabel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68424, 30)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     0.000000\n",
       "feature2     0.000000\n",
       "feature3     0.058928\n",
       "feature4     0.000000\n",
       "feature5     0.000000\n",
       "feature6     0.000000\n",
       "feature7     0.014441\n",
       "feature8     0.000000\n",
       "feature9     0.000000\n",
       "feature10    0.000000\n",
       "feature11    0.000000\n",
       "feature12    0.071845\n",
       "feature13    0.000000\n",
       "feature14    0.000000\n",
       "feature15    0.000000\n",
       "feature16    0.000000\n",
       "feature17    0.000000\n",
       "feature18    0.063959\n",
       "feature19    0.000000\n",
       "feature20    0.000000\n",
       "feature21    0.000000\n",
       "feature22    0.000000\n",
       "feature23    0.099892\n",
       "feature24    0.000000\n",
       "feature25    0.000000\n",
       "feature26    0.000000\n",
       "feature27    0.000000\n",
       "feature28    0.000000\n",
       "feature29    0.000000\n",
       "feature30    0.000000\n",
       "Name: 53142, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68424, 30)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.values.reshape((df_train.shape[0], 1, df_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68424, 1, 30)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature22</th>\n",
       "      <th>feature23</th>\n",
       "      <th>feature24</th>\n",
       "      <th>feature25</th>\n",
       "      <th>feature26</th>\n",
       "      <th>feature27</th>\n",
       "      <th>feature28</th>\n",
       "      <th>feature29</th>\n",
       "      <th>feature30</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68424</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044392</td>\n",
       "      <td>0.009810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68425</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024184</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68426</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044602</td>\n",
       "      <td>0.009867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68427</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024209</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68428</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044153</td>\n",
       "      <td>0.009751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "68424       0.0  0.044392  0.009810       0.0       0.0       0.0  0.029762   \n",
       "68425       0.0  0.000000  0.017692       0.0       0.0       0.0  0.018432   \n",
       "68426       0.0  0.044602  0.009867       0.0       0.0       0.0  0.029689   \n",
       "68427       0.0  0.000000  0.017742       0.0       0.0       0.0  0.017482   \n",
       "68428       0.0  0.044153  0.009751       0.0       0.0       0.0  0.029908   \n",
       "\n",
       "       feature8  feature9  feature10  ...  feature22  feature23  feature24  \\\n",
       "68424       0.0  0.043584        0.0  ...        0.0        0.0        0.0   \n",
       "68425       0.0  0.000000        0.0  ...        0.0        0.0        0.0   \n",
       "68426       0.0  0.043520        0.0  ...        0.0        0.0        0.0   \n",
       "68427       0.0  0.000000        0.0  ...        0.0        0.0        0.0   \n",
       "68428       0.0  0.043653        0.0  ...        0.0        0.0        0.0   \n",
       "\n",
       "       feature25  feature26  feature27  feature28  feature29  feature30  label  \n",
       "68424        0.0   0.000000        0.0   0.024066        0.0   0.000000      1  \n",
       "68425        0.0   0.007034        0.0   0.000000        0.0   0.024184      1  \n",
       "68426        0.0   0.000000        0.0   0.024401        0.0   0.000000      1  \n",
       "68427        0.0   0.007084        0.0   0.000000        0.0   0.024209      1  \n",
       "68428        0.0   0.000000        0.0   0.023642        0.0   0.000000      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "timesteps =  df_train.shape[1] # equal to the lookback\n",
    "n_features =  df_train.shape[2] # 59\n",
    "\n",
    "epochs = 15\n",
    "batch = 64\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=dict()\n",
    "history['gen']=[]\n",
    "history['dis']=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error\n",
    "                             ,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 1\n",
    "        self.img_cols = 30\n",
    "        self.img_shape = (self.img_rows, self.img_cols)\n",
    "        self.latent_dim = 30\n",
    "\n",
    "        optimizer = Adam(0.0001, 0.4)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates song\n",
    "        z = Input(shape=(1,30))\n",
    "    \n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(1, 30)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #specifying output to have 40 timesteps\n",
    "        model.add(RepeatVector(30))\n",
    "        #specifying 1 feature as the output\n",
    "        \n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.3))   \n",
    "        model.add(TimeDistributed(Dense(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(1,30))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "        #return Model(img)\n",
    "    \n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(64, use_bias=False, input_shape=(30, 1)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Convolution1D(32, 3, strides=2, padding='same', use_bias=False))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Convolution1D(1, 3, strides=2, padding='same', use_bias=False))\n",
    "        model.add(Flatten())        \n",
    "        model.add(RepeatVector(1))        \n",
    "        model.add(TimeDistributed(Dense(32, activation = 'relu')))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(1, activation = 'relu')))\n",
    "        #model.add(TimeDistributed(Dense(1)))\n",
    "        #model.add(Dense(1, activation='tanh'))\n",
    "        #model.add(Dense(1))\n",
    "        #model.add(Activation('sigmoid'))\n",
    "        model.summary()\n",
    "        #model.add(layers.BatchNormalization())\n",
    "        #model.add(sigmoid())\n",
    "       \n",
    "        img = Input(shape=(30,1))\n",
    "        validity = model(img)\n",
    "        print(\"exit\")\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs=15, batch_size=64, save_interval=50):\n",
    "        print(\"enter\")  \n",
    "        # Load the dataset\n",
    "        X_train = df_train\n",
    "       \n",
    "        # Rescale 0 to 1\n",
    "      #  X_train = X_train / 128\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size,1,1))\n",
    "        fake = np.zeros((batch_size,1,1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of songs\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            imgs = np.array(imgs)\n",
    "            imgs = imgs.reshape(len(imgs),30,1)\n",
    "\n",
    "            # Sample noise and generate a batch of new songs\n",
    "            noise = np.random.normal(0, 1, (batch_size,1,30))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake songs as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "            history['gen'].append(g_loss)\n",
    "            \n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            history['dis'].append(d_loss) \n",
    "            #return (g_loss,d_loss)\n",
    "            # If at save interval => save model\n",
    "#            if epoch % save_interval == 0:\n",
    " #               self.generator.save(\"LSTM_generator.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 30, 64)            64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 15, 32)            6144      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 15, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 8, 1)              96        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 1, 8)              0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 32)             288       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 1, 1)              33        \n",
      "=================================================================\n",
      "Total params: 6,625\n",
      "Trainable params: 6,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "exit\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 1, 256)            162816    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 1, 256)            394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 30, 256)           394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 30, 256)           394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 30, 256)           394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 30, 128)           32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 30, 128)           16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 30, 1)             129       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 30, 1)             0         \n",
      "=================================================================\n",
      "Total params: 2,183,553\n",
      "Trainable params: 2,183,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstmgan = LSTMGAN()\n",
    "#lstmgan.train(epochs=1000, batch_size=20, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 6.956025, acc.: 50.00%] [G loss: 7.959528]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 5.499847, acc.: 50.00%] [G loss: 6.836017]\n",
      "2 [D loss: 4.569423, acc.: 50.00%] [G loss: 6.259305]\n",
      "3 [D loss: 3.905200, acc.: 50.00%] [G loss: 5.968812]\n",
      "4 [D loss: 3.385165, acc.: 50.00%] [G loss: 5.922042]\n",
      "5 [D loss: 2.995248, acc.: 50.00%] [G loss: 5.845067]\n",
      "6 [D loss: 3.047131, acc.: 50.00%] [G loss: 5.668993]\n",
      "7 [D loss: 2.917993, acc.: 50.00%] [G loss: 5.710380]\n",
      "8 [D loss: 2.985773, acc.: 50.00%] [G loss: 5.497611]\n",
      "9 [D loss: 2.841527, acc.: 50.00%] [G loss: 5.590598]\n",
      "10 [D loss: 2.855016, acc.: 50.00%] [G loss: 5.453340]\n",
      "11 [D loss: 3.044448, acc.: 50.00%] [G loss: 5.452754]\n",
      "12 [D loss: 3.048119, acc.: 50.00%] [G loss: 5.348230]\n",
      "13 [D loss: 2.779248, acc.: 50.00%] [G loss: 5.203269]\n",
      "14 [D loss: 2.690141, acc.: 50.00%] [G loss: 5.186970]\n",
      "15 [D loss: 2.756208, acc.: 50.00%] [G loss: 5.186081]\n",
      "16 [D loss: 2.647058, acc.: 50.00%] [G loss: 5.203384]\n",
      "17 [D loss: 2.679193, acc.: 50.00%] [G loss: 5.106551]\n",
      "18 [D loss: 2.639110, acc.: 50.00%] [G loss: 5.105025]\n",
      "19 [D loss: 2.618743, acc.: 50.00%] [G loss: 5.010975]\n",
      "20 [D loss: 2.552436, acc.: 50.00%] [G loss: 4.988352]\n",
      "21 [D loss: 2.479603, acc.: 50.00%] [G loss: 5.162709]\n",
      "22 [D loss: 2.592467, acc.: 50.00%] [G loss: 5.064118]\n",
      "23 [D loss: 2.509380, acc.: 50.00%] [G loss: 4.953490]\n",
      "24 [D loss: 2.501901, acc.: 50.00%] [G loss: 4.851570]\n",
      "25 [D loss: 2.491480, acc.: 50.00%] [G loss: 4.852551]\n",
      "26 [D loss: 2.483866, acc.: 50.00%] [G loss: 4.800343]\n",
      "27 [D loss: 2.503292, acc.: 50.00%] [G loss: 4.889618]\n",
      "28 [D loss: 2.498467, acc.: 50.00%] [G loss: 4.698723]\n",
      "29 [D loss: 2.399005, acc.: 50.00%] [G loss: 4.786532]\n",
      "30 [D loss: 2.474460, acc.: 50.00%] [G loss: 4.807482]\n",
      "31 [D loss: 2.431763, acc.: 50.00%] [G loss: 4.719363]\n",
      "32 [D loss: 2.413805, acc.: 50.00%] [G loss: 4.722144]\n",
      "33 [D loss: 2.381606, acc.: 50.00%] [G loss: 4.782218]\n",
      "34 [D loss: 2.411924, acc.: 50.00%] [G loss: 4.662198]\n",
      "35 [D loss: 2.320310, acc.: 50.00%] [G loss: 4.668535]\n",
      "36 [D loss: 2.390791, acc.: 50.00%] [G loss: 4.658835]\n",
      "37 [D loss: 2.586136, acc.: 50.00%] [G loss: 4.650817]\n",
      "38 [D loss: 2.344362, acc.: 50.00%] [G loss: 4.509806]\n",
      "39 [D loss: 2.290011, acc.: 50.00%] [G loss: 4.466361]\n",
      "40 [D loss: 2.329588, acc.: 50.00%] [G loss: 4.609035]\n",
      "41 [D loss: 2.340195, acc.: 50.00%] [G loss: 4.442095]\n",
      "42 [D loss: 2.348707, acc.: 50.00%] [G loss: 4.562987]\n",
      "43 [D loss: 2.242141, acc.: 50.00%] [G loss: 4.497103]\n",
      "44 [D loss: 2.348155, acc.: 50.00%] [G loss: 4.527988]\n",
      "45 [D loss: 2.323482, acc.: 50.00%] [G loss: 4.543231]\n",
      "46 [D loss: 2.246690, acc.: 50.00%] [G loss: 4.397429]\n",
      "47 [D loss: 2.324428, acc.: 50.00%] [G loss: 4.510640]\n",
      "48 [D loss: 2.271777, acc.: 50.00%] [G loss: 4.544473]\n",
      "49 [D loss: 2.233645, acc.: 50.00%] [G loss: 4.406167]\n",
      "50 [D loss: 2.201489, acc.: 50.00%] [G loss: 4.438820]\n",
      "51 [D loss: 2.194679, acc.: 50.00%] [G loss: 4.319008]\n",
      "52 [D loss: 2.217777, acc.: 50.00%] [G loss: 4.232451]\n",
      "53 [D loss: 2.235005, acc.: 50.00%] [G loss: 4.313700]\n",
      "54 [D loss: 2.242161, acc.: 50.00%] [G loss: 4.319987]\n",
      "55 [D loss: 2.236288, acc.: 50.00%] [G loss: 4.236655]\n",
      "56 [D loss: 2.222567, acc.: 50.00%] [G loss: 4.234408]\n",
      "57 [D loss: 2.182549, acc.: 50.00%] [G loss: 4.323590]\n",
      "58 [D loss: 2.163740, acc.: 50.00%] [G loss: 4.336511]\n",
      "59 [D loss: 2.149680, acc.: 50.00%] [G loss: 4.235528]\n",
      "60 [D loss: 2.189146, acc.: 50.00%] [G loss: 4.199126]\n",
      "61 [D loss: 2.172878, acc.: 50.00%] [G loss: 4.258344]\n",
      "62 [D loss: 2.153640, acc.: 50.00%] [G loss: 4.074983]\n",
      "63 [D loss: 2.186022, acc.: 50.00%] [G loss: 4.036628]\n",
      "64 [D loss: 2.179519, acc.: 50.00%] [G loss: 4.354258]\n",
      "65 [D loss: 2.158553, acc.: 50.00%] [G loss: 4.232682]\n",
      "66 [D loss: 2.173927, acc.: 50.00%] [G loss: 4.163167]\n",
      "67 [D loss: 2.109523, acc.: 50.00%] [G loss: 4.138700]\n",
      "68 [D loss: 2.133869, acc.: 50.00%] [G loss: 4.147935]\n",
      "69 [D loss: 2.153253, acc.: 50.00%] [G loss: 4.246787]\n",
      "70 [D loss: 2.117144, acc.: 50.00%] [G loss: 4.088578]\n",
      "71 [D loss: 2.164974, acc.: 50.00%] [G loss: 4.117597]\n",
      "72 [D loss: 2.132578, acc.: 50.00%] [G loss: 4.181157]\n",
      "73 [D loss: 2.112019, acc.: 50.00%] [G loss: 4.099935]\n",
      "74 [D loss: 2.106832, acc.: 50.00%] [G loss: 4.114366]\n",
      "75 [D loss: 2.090828, acc.: 50.00%] [G loss: 4.099822]\n",
      "76 [D loss: 2.092193, acc.: 50.00%] [G loss: 4.037065]\n",
      "77 [D loss: 2.069378, acc.: 50.00%] [G loss: 4.065240]\n",
      "78 [D loss: 2.098563, acc.: 50.00%] [G loss: 3.970467]\n",
      "79 [D loss: 2.063156, acc.: 50.00%] [G loss: 3.901012]\n",
      "80 [D loss: 2.057491, acc.: 50.00%] [G loss: 4.013226]\n",
      "81 [D loss: 2.089268, acc.: 50.00%] [G loss: 4.105971]\n",
      "82 [D loss: 2.070422, acc.: 50.00%] [G loss: 3.872258]\n",
      "83 [D loss: 2.075620, acc.: 50.00%] [G loss: 4.074983]\n",
      "84 [D loss: 2.056509, acc.: 50.00%] [G loss: 3.859042]\n",
      "85 [D loss: 2.058270, acc.: 50.00%] [G loss: 3.895914]\n",
      "86 [D loss: 2.095468, acc.: 50.00%] [G loss: 4.374359]\n",
      "87 [D loss: 2.023786, acc.: 50.00%] [G loss: 3.823908]\n",
      "88 [D loss: 2.012181, acc.: 50.00%] [G loss: 4.920583]\n",
      "89 [D loss: 2.003163, acc.: 50.00%] [G loss: 3.688349]\n",
      "90 [D loss: 2.061186, acc.: 50.00%] [G loss: 4.442050]\n",
      "91 [D loss: 2.024889, acc.: 50.00%] [G loss: 4.345967]\n",
      "92 [D loss: 2.014617, acc.: 50.00%] [G loss: 3.826188]\n",
      "93 [D loss: 2.102659, acc.: 50.00%] [G loss: 4.518917]\n",
      "94 [D loss: 1.970432, acc.: 50.00%] [G loss: 3.909297]\n",
      "95 [D loss: 1.986805, acc.: 50.00%] [G loss: 3.686813]\n",
      "96 [D loss: 2.043899, acc.: 50.00%] [G loss: 3.812084]\n",
      "97 [D loss: 2.015807, acc.: 50.00%] [G loss: 3.909713]\n",
      "98 [D loss: 1.994282, acc.: 50.00%] [G loss: 3.801227]\n",
      "99 [D loss: 1.975721, acc.: 50.00%] [G loss: 3.726190]\n",
      "100 [D loss: 2.006900, acc.: 50.00%] [G loss: 3.655674]\n",
      "101 [D loss: 1.959082, acc.: 50.00%] [G loss: 3.657053]\n",
      "102 [D loss: 1.969670, acc.: 50.00%] [G loss: 3.592559]\n",
      "103 [D loss: 2.003026, acc.: 50.00%] [G loss: 3.866909]\n",
      "104 [D loss: 2.039823, acc.: 50.00%] [G loss: 3.623784]\n",
      "105 [D loss: 1.925610, acc.: 50.00%] [G loss: 3.498924]\n",
      "106 [D loss: 1.960315, acc.: 50.00%] [G loss: 3.666538]\n",
      "107 [D loss: 1.931918, acc.: 50.00%] [G loss: 3.726130]\n",
      "108 [D loss: 1.913746, acc.: 50.00%] [G loss: 3.632617]\n",
      "109 [D loss: 2.049191, acc.: 50.00%] [G loss: 4.040678]\n",
      "110 [D loss: 1.938812, acc.: 50.00%] [G loss: 3.332930]\n",
      "111 [D loss: 2.007291, acc.: 50.00%] [G loss: 4.274854]\n",
      "112 [D loss: 1.971911, acc.: 50.00%] [G loss: 5.504148]\n",
      "113 [D loss: 1.955180, acc.: 50.00%] [G loss: 4.734656]\n",
      "114 [D loss: 2.966977, acc.: 30.00%] [G loss: 5.060060]\n",
      "115 [D loss: 2.140282, acc.: 40.00%] [G loss: 7.852025]\n",
      "116 [D loss: 1.940556, acc.: 50.00%] [G loss: 8.006806]\n",
      "117 [D loss: 1.977536, acc.: 42.50%] [G loss: 8.655215]\n",
      "118 [D loss: 1.937669, acc.: 50.00%] [G loss: 7.755916]\n",
      "119 [D loss: 1.956437, acc.: 50.00%] [G loss: 5.757062]\n",
      "120 [D loss: 1.977026, acc.: 50.00%] [G loss: 9.259531]\n",
      "121 [D loss: 1.943136, acc.: 50.00%] [G loss: 8.395706]\n",
      "122 [D loss: 1.918875, acc.: 50.00%] [G loss: 9.186337]\n",
      "123 [D loss: 1.998974, acc.: 50.00%] [G loss: 7.886703]\n",
      "124 [D loss: 1.915325, acc.: 50.00%] [G loss: 8.461490]\n",
      "125 [D loss: 1.969520, acc.: 47.50%] [G loss: 6.743715]\n",
      "126 [D loss: 2.004550, acc.: 50.00%] [G loss: 7.328355]\n",
      "127 [D loss: 1.960540, acc.: 47.50%] [G loss: 8.087403]\n",
      "128 [D loss: 1.950120, acc.: 47.50%] [G loss: 6.748937]\n",
      "129 [D loss: 1.985204, acc.: 47.50%] [G loss: 6.941746]\n",
      "130 [D loss: 2.007375, acc.: 45.00%] [G loss: 7.237875]\n",
      "131 [D loss: 1.899732, acc.: 50.00%] [G loss: 8.056965]\n",
      "132 [D loss: 1.968447, acc.: 50.00%] [G loss: 10.332438]\n",
      "133 [D loss: 1.914044, acc.: 47.50%] [G loss: 5.990977]\n",
      "134 [D loss: 1.903374, acc.: 47.50%] [G loss: 5.918301]\n",
      "135 [D loss: 1.933269, acc.: 47.50%] [G loss: 9.487434]\n",
      "136 [D loss: 1.915299, acc.: 50.00%] [G loss: 9.343701]\n",
      "137 [D loss: 1.924814, acc.: 50.00%] [G loss: 8.839838]\n",
      "138 [D loss: 1.937907, acc.: 50.00%] [G loss: 6.666337]\n",
      "139 [D loss: 1.991505, acc.: 47.50%] [G loss: 7.441657]\n",
      "140 [D loss: 1.867051, acc.: 50.00%] [G loss: 6.715430]\n",
      "141 [D loss: 1.967951, acc.: 50.00%] [G loss: 7.073729]\n",
      "142 [D loss: 1.874114, acc.: 50.00%] [G loss: 8.388646]\n",
      "143 [D loss: 1.915913, acc.: 50.00%] [G loss: 6.634013]\n",
      "144 [D loss: 1.969763, acc.: 47.50%] [G loss: 6.820507]\n",
      "145 [D loss: 1.950664, acc.: 47.50%] [G loss: 8.943671]\n",
      "146 [D loss: 1.869374, acc.: 50.00%] [G loss: 7.424975]\n",
      "147 [D loss: 1.872389, acc.: 50.00%] [G loss: 6.500671]\n",
      "148 [D loss: 1.852914, acc.: 50.00%] [G loss: 6.838440]\n",
      "149 [D loss: 1.875750, acc.: 50.00%] [G loss: 7.798320]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 [D loss: 1.851838, acc.: 50.00%] [G loss: 7.466058]\n",
      "151 [D loss: 1.860549, acc.: 50.00%] [G loss: 7.812449]\n",
      "152 [D loss: 1.834941, acc.: 50.00%] [G loss: 9.071692]\n",
      "153 [D loss: 1.890904, acc.: 50.00%] [G loss: 9.427039]\n",
      "154 [D loss: 1.858492, acc.: 50.00%] [G loss: 7.655781]\n",
      "155 [D loss: 1.875643, acc.: 50.00%] [G loss: 9.317254]\n",
      "156 [D loss: 1.831976, acc.: 50.00%] [G loss: 6.081633]\n",
      "157 [D loss: 1.882924, acc.: 50.00%] [G loss: 5.626334]\n",
      "158 [D loss: 1.827716, acc.: 50.00%] [G loss: 9.254326]\n",
      "159 [D loss: 1.826055, acc.: 50.00%] [G loss: 6.285447]\n",
      "160 [D loss: 1.785657, acc.: 50.00%] [G loss: 8.600945]\n",
      "161 [D loss: 1.832898, acc.: 50.00%] [G loss: 7.930861]\n",
      "162 [D loss: 1.888446, acc.: 47.50%] [G loss: 8.743467]\n",
      "163 [D loss: 1.857211, acc.: 47.50%] [G loss: 8.489744]\n",
      "164 [D loss: 1.851809, acc.: 45.00%] [G loss: 9.277982]\n",
      "165 [D loss: 2.361488, acc.: 42.50%] [G loss: 7.796176]\n",
      "166 [D loss: 1.818909, acc.: 50.00%] [G loss: 10.229945]\n",
      "167 [D loss: 1.844693, acc.: 42.50%] [G loss: 8.370853]\n",
      "168 [D loss: 1.858741, acc.: 42.50%] [G loss: 9.317963]\n",
      "169 [D loss: 2.034328, acc.: 40.00%] [G loss: 9.736900]\n",
      "170 [D loss: 1.957877, acc.: 42.50%] [G loss: 7.915047]\n",
      "171 [D loss: 1.813217, acc.: 47.50%] [G loss: 8.690011]\n",
      "172 [D loss: 2.260553, acc.: 47.50%] [G loss: 8.633217]\n",
      "173 [D loss: 1.862583, acc.: 47.50%] [G loss: 9.412338]\n",
      "174 [D loss: 1.921273, acc.: 42.50%] [G loss: 8.559709]\n",
      "175 [D loss: 1.821126, acc.: 47.50%] [G loss: 8.669996]\n",
      "176 [D loss: 1.901386, acc.: 42.50%] [G loss: 7.704787]\n",
      "177 [D loss: 1.789018, acc.: 47.50%] [G loss: 8.304957]\n",
      "178 [D loss: 3.049486, acc.: 37.50%] [G loss: 10.027575]\n",
      "179 [D loss: 1.810440, acc.: 45.00%] [G loss: 9.314150]\n",
      "180 [D loss: 1.780083, acc.: 50.00%] [G loss: 6.400939]\n",
      "181 [D loss: 2.286325, acc.: 45.00%] [G loss: 7.232342]\n",
      "182 [D loss: 1.902253, acc.: 45.00%] [G loss: 10.734133]\n",
      "183 [D loss: 1.810002, acc.: 47.50%] [G loss: 9.380880]\n",
      "184 [D loss: 1.793072, acc.: 42.50%] [G loss: 7.318330]\n",
      "185 [D loss: 1.768540, acc.: 47.50%] [G loss: 7.653182]\n",
      "186 [D loss: 2.260135, acc.: 45.00%] [G loss: 7.915215]\n",
      "187 [D loss: 1.870901, acc.: 42.50%] [G loss: 9.368956]\n",
      "188 [D loss: 1.766350, acc.: 47.50%] [G loss: 8.413015]\n",
      "189 [D loss: 1.804091, acc.: 45.00%] [G loss: 9.111877]\n",
      "190 [D loss: 2.160014, acc.: 47.50%] [G loss: 8.314717]\n",
      "191 [D loss: 1.700600, acc.: 50.00%] [G loss: 7.988739]\n",
      "192 [D loss: 1.957148, acc.: 40.00%] [G loss: 9.188419]\n",
      "193 [D loss: 1.851528, acc.: 47.50%] [G loss: 7.010336]\n",
      "194 [D loss: 1.917052, acc.: 42.50%] [G loss: 9.467208]\n",
      "195 [D loss: 1.825710, acc.: 47.50%] [G loss: 7.573145]\n",
      "196 [D loss: 1.775078, acc.: 47.50%] [G loss: 8.673903]\n",
      "197 [D loss: 1.744771, acc.: 47.50%] [G loss: 8.869430]\n",
      "198 [D loss: 1.720986, acc.: 50.00%] [G loss: 7.877890]\n",
      "199 [D loss: 1.772787, acc.: 50.00%] [G loss: 10.017204]\n",
      "200 [D loss: 1.770513, acc.: 47.50%] [G loss: 8.547777]\n",
      "201 [D loss: 1.725502, acc.: 50.00%] [G loss: 9.036174]\n",
      "202 [D loss: 2.185881, acc.: 45.00%] [G loss: 6.587013]\n",
      "203 [D loss: 1.818703, acc.: 45.00%] [G loss: 6.869760]\n",
      "204 [D loss: 1.754401, acc.: 47.50%] [G loss: 9.255719]\n",
      "205 [D loss: 1.781105, acc.: 50.00%] [G loss: 13.531042]\n",
      "206 [D loss: 1.800516, acc.: 45.00%] [G loss: 9.837990]\n",
      "207 [D loss: 1.904073, acc.: 47.50%] [G loss: 7.611398]\n",
      "208 [D loss: 1.795103, acc.: 47.50%] [G loss: 10.090944]\n",
      "209 [D loss: 1.738482, acc.: 47.50%] [G loss: 10.467348]\n",
      "210 [D loss: 1.814838, acc.: 45.00%] [G loss: 10.542189]\n",
      "211 [D loss: 1.740063, acc.: 50.00%] [G loss: 9.348028]\n",
      "212 [D loss: 1.710182, acc.: 50.00%] [G loss: 8.830976]\n",
      "213 [D loss: 1.830483, acc.: 47.50%] [G loss: 7.928847]\n",
      "214 [D loss: 1.712128, acc.: 50.00%] [G loss: 10.305644]\n",
      "215 [D loss: 1.856227, acc.: 47.50%] [G loss: 8.725464]\n",
      "216 [D loss: 1.752494, acc.: 47.50%] [G loss: 9.443090]\n",
      "217 [D loss: 1.682437, acc.: 47.50%] [G loss: 8.056462]\n",
      "218 [D loss: 1.774508, acc.: 47.50%] [G loss: 7.094460]\n",
      "219 [D loss: 1.803048, acc.: 47.50%] [G loss: 9.496783]\n",
      "220 [D loss: 1.743508, acc.: 42.50%] [G loss: 8.053136]\n",
      "221 [D loss: 1.817483, acc.: 47.50%] [G loss: 10.367063]\n",
      "222 [D loss: 1.720095, acc.: 50.00%] [G loss: 9.240657]\n",
      "223 [D loss: 1.712207, acc.: 50.00%] [G loss: 11.402407]\n",
      "224 [D loss: 1.719604, acc.: 50.00%] [G loss: 10.644171]\n",
      "225 [D loss: 1.715251, acc.: 50.00%] [G loss: 10.768288]\n",
      "226 [D loss: 1.734807, acc.: 50.00%] [G loss: 7.675637]\n",
      "227 [D loss: 1.735903, acc.: 47.50%] [G loss: 9.910387]\n",
      "228 [D loss: 1.681746, acc.: 50.00%] [G loss: 8.835299]\n",
      "229 [D loss: 1.687789, acc.: 47.50%] [G loss: 7.462650]\n",
      "230 [D loss: 1.700209, acc.: 50.00%] [G loss: 6.981106]\n",
      "231 [D loss: 1.732444, acc.: 47.50%] [G loss: 8.253795]\n",
      "232 [D loss: 1.726831, acc.: 50.00%] [G loss: 8.729048]\n",
      "233 [D loss: 1.678970, acc.: 50.00%] [G loss: 7.933694]\n",
      "234 [D loss: 1.724180, acc.: 47.50%] [G loss: 7.866783]\n",
      "235 [D loss: 2.130713, acc.: 45.00%] [G loss: 6.643512]\n",
      "236 [D loss: 1.666462, acc.: 50.00%] [G loss: 7.499162]\n",
      "237 [D loss: 1.685698, acc.: 47.50%] [G loss: 10.875380]\n",
      "238 [D loss: 1.666957, acc.: 50.00%] [G loss: 9.212774]\n",
      "239 [D loss: 1.720514, acc.: 50.00%] [G loss: 8.259016]\n",
      "240 [D loss: 1.712536, acc.: 47.50%] [G loss: 9.151591]\n",
      "241 [D loss: 1.676971, acc.: 50.00%] [G loss: 8.206427]\n",
      "242 [D loss: 1.627139, acc.: 50.00%] [G loss: 8.882723]\n",
      "243 [D loss: 1.722070, acc.: 50.00%] [G loss: 8.277616]\n",
      "244 [D loss: 1.678716, acc.: 47.50%] [G loss: 8.370049]\n",
      "245 [D loss: 1.645428, acc.: 50.00%] [G loss: 6.907310]\n",
      "246 [D loss: 1.707724, acc.: 50.00%] [G loss: 10.123396]\n",
      "247 [D loss: 1.721798, acc.: 50.00%] [G loss: 8.492981]\n",
      "248 [D loss: 1.605426, acc.: 50.00%] [G loss: 8.208837]\n",
      "249 [D loss: 1.683978, acc.: 50.00%] [G loss: 9.842863]\n",
      "250 [D loss: 1.643213, acc.: 50.00%] [G loss: 8.698111]\n",
      "251 [D loss: 1.615202, acc.: 50.00%] [G loss: 8.294815]\n",
      "252 [D loss: 1.677475, acc.: 50.00%] [G loss: 6.669099]\n",
      "253 [D loss: 1.774001, acc.: 45.00%] [G loss: 7.942621]\n",
      "254 [D loss: 1.642517, acc.: 50.00%] [G loss: 9.858587]\n",
      "255 [D loss: 1.664622, acc.: 50.00%] [G loss: 8.034395]\n",
      "256 [D loss: 1.643717, acc.: 50.00%] [G loss: 7.585026]\n",
      "257 [D loss: 1.675199, acc.: 50.00%] [G loss: 8.768560]\n",
      "258 [D loss: 1.616632, acc.: 50.00%] [G loss: 7.647418]\n",
      "259 [D loss: 1.641600, acc.: 50.00%] [G loss: 9.776118]\n",
      "260 [D loss: 1.657382, acc.: 50.00%] [G loss: 8.393905]\n",
      "261 [D loss: 1.581036, acc.: 50.00%] [G loss: 10.821718]\n",
      "262 [D loss: 1.596264, acc.: 50.00%] [G loss: 9.388903]\n",
      "263 [D loss: 1.602365, acc.: 50.00%] [G loss: 10.125666]\n",
      "264 [D loss: 1.651396, acc.: 50.00%] [G loss: 9.622046]\n",
      "265 [D loss: 1.633903, acc.: 50.00%] [G loss: 7.820827]\n",
      "266 [D loss: 1.654199, acc.: 50.00%] [G loss: 8.288613]\n",
      "267 [D loss: 1.569255, acc.: 50.00%] [G loss: 8.692688]\n",
      "268 [D loss: 1.620245, acc.: 50.00%] [G loss: 8.884895]\n",
      "269 [D loss: 1.626675, acc.: 50.00%] [G loss: 8.942365]\n",
      "270 [D loss: 1.657761, acc.: 47.50%] [G loss: 11.617353]\n",
      "271 [D loss: 1.626180, acc.: 50.00%] [G loss: 8.915443]\n",
      "272 [D loss: 1.574267, acc.: 50.00%] [G loss: 8.012383]\n",
      "273 [D loss: 1.666000, acc.: 50.00%] [G loss: 9.755573]\n",
      "274 [D loss: 1.591799, acc.: 50.00%] [G loss: 8.903461]\n",
      "275 [D loss: 1.613566, acc.: 50.00%] [G loss: 8.191916]\n",
      "276 [D loss: 1.625250, acc.: 50.00%] [G loss: 7.407527]\n",
      "277 [D loss: 1.607579, acc.: 50.00%] [G loss: 8.755804]\n",
      "278 [D loss: 1.593443, acc.: 50.00%] [G loss: 8.100729]\n",
      "279 [D loss: 1.624215, acc.: 50.00%] [G loss: 7.534347]\n",
      "280 [D loss: 1.658752, acc.: 50.00%] [G loss: 9.661989]\n",
      "281 [D loss: 1.582035, acc.: 50.00%] [G loss: 6.809117]\n",
      "282 [D loss: 1.608827, acc.: 50.00%] [G loss: 10.731683]\n",
      "283 [D loss: 1.581762, acc.: 50.00%] [G loss: 6.889212]\n",
      "284 [D loss: 1.603944, acc.: 50.00%] [G loss: 8.888965]\n",
      "285 [D loss: 1.584714, acc.: 50.00%] [G loss: 11.324598]\n",
      "286 [D loss: 1.595374, acc.: 50.00%] [G loss: 7.712564]\n",
      "287 [D loss: 1.611751, acc.: 50.00%] [G loss: 8.113428]\n",
      "288 [D loss: 1.576602, acc.: 50.00%] [G loss: 5.937202]\n",
      "289 [D loss: 1.593150, acc.: 50.00%] [G loss: 9.541025]\n",
      "290 [D loss: 1.648502, acc.: 47.50%] [G loss: 9.024714]\n",
      "291 [D loss: 1.604251, acc.: 50.00%] [G loss: 10.108183]\n",
      "292 [D loss: 1.573633, acc.: 50.00%] [G loss: 6.213674]\n",
      "293 [D loss: 1.552522, acc.: 50.00%] [G loss: 7.243341]\n",
      "294 [D loss: 1.605235, acc.: 50.00%] [G loss: 8.959361]\n",
      "295 [D loss: 1.588561, acc.: 50.00%] [G loss: 9.091536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 [D loss: 1.578538, acc.: 50.00%] [G loss: 9.700925]\n",
      "297 [D loss: 1.583480, acc.: 50.00%] [G loss: 9.326623]\n",
      "298 [D loss: 1.575598, acc.: 50.00%] [G loss: 8.571387]\n",
      "299 [D loss: 1.620642, acc.: 50.00%] [G loss: 9.507285]\n",
      "300 [D loss: 1.589801, acc.: 50.00%] [G loss: 9.990825]\n",
      "301 [D loss: 1.535345, acc.: 50.00%] [G loss: 10.105470]\n",
      "302 [D loss: 1.573591, acc.: 50.00%] [G loss: 6.725083]\n",
      "303 [D loss: 1.550739, acc.: 50.00%] [G loss: 8.174965]\n",
      "304 [D loss: 1.594126, acc.: 50.00%] [G loss: 8.200047]\n",
      "305 [D loss: 1.624520, acc.: 50.00%] [G loss: 7.439374]\n",
      "306 [D loss: 1.601391, acc.: 47.50%] [G loss: 10.750456]\n",
      "307 [D loss: 1.556111, acc.: 50.00%] [G loss: 9.249300]\n",
      "308 [D loss: 1.585419, acc.: 50.00%] [G loss: 7.966725]\n",
      "309 [D loss: 1.571738, acc.: 50.00%] [G loss: 6.343851]\n",
      "310 [D loss: 1.545685, acc.: 50.00%] [G loss: 6.636453]\n",
      "311 [D loss: 1.545375, acc.: 50.00%] [G loss: 12.773995]\n",
      "312 [D loss: 1.524562, acc.: 50.00%] [G loss: 7.926167]\n",
      "313 [D loss: 1.573529, acc.: 50.00%] [G loss: 12.122814]\n",
      "314 [D loss: 1.523929, acc.: 50.00%] [G loss: 9.471975]\n",
      "315 [D loss: 1.525475, acc.: 50.00%] [G loss: 8.100477]\n",
      "316 [D loss: 1.537486, acc.: 50.00%] [G loss: 7.790268]\n",
      "317 [D loss: 1.513250, acc.: 50.00%] [G loss: 9.018619]\n",
      "318 [D loss: 1.513105, acc.: 50.00%] [G loss: 9.777071]\n",
      "319 [D loss: 1.504070, acc.: 50.00%] [G loss: 10.307261]\n",
      "320 [D loss: 1.511713, acc.: 50.00%] [G loss: 12.170809]\n",
      "321 [D loss: 1.564185, acc.: 50.00%] [G loss: 7.687467]\n",
      "322 [D loss: 1.507210, acc.: 50.00%] [G loss: 10.307814]\n",
      "323 [D loss: 1.471029, acc.: 50.00%] [G loss: 8.439499]\n",
      "324 [D loss: 1.494531, acc.: 50.00%] [G loss: 9.780008]\n",
      "325 [D loss: 1.518811, acc.: 50.00%] [G loss: 11.420771]\n",
      "326 [D loss: 1.513210, acc.: 50.00%] [G loss: 8.176671]\n",
      "327 [D loss: 1.517941, acc.: 50.00%] [G loss: 10.202810]\n",
      "328 [D loss: 1.478876, acc.: 50.00%] [G loss: 10.913106]\n",
      "329 [D loss: 1.561096, acc.: 50.00%] [G loss: 8.822231]\n",
      "330 [D loss: 1.490220, acc.: 50.00%] [G loss: 5.043250]\n",
      "331 [D loss: 1.485001, acc.: 50.00%] [G loss: 9.015562]\n",
      "332 [D loss: 1.476702, acc.: 50.00%] [G loss: 11.800738]\n",
      "333 [D loss: 1.487135, acc.: 50.00%] [G loss: 10.899135]\n",
      "334 [D loss: 1.502430, acc.: 50.00%] [G loss: 8.866499]\n",
      "335 [D loss: 1.521371, acc.: 50.00%] [G loss: 6.724370]\n",
      "336 [D loss: 1.545874, acc.: 50.00%] [G loss: 8.262946]\n",
      "337 [D loss: 1.460815, acc.: 50.00%] [G loss: 8.798305]\n",
      "338 [D loss: 1.526900, acc.: 50.00%] [G loss: 11.448668]\n",
      "339 [D loss: 1.566004, acc.: 50.00%] [G loss: 7.734691]\n",
      "340 [D loss: 1.452727, acc.: 50.00%] [G loss: 9.363504]\n",
      "341 [D loss: 1.479980, acc.: 50.00%] [G loss: 8.179042]\n",
      "342 [D loss: 1.486801, acc.: 50.00%] [G loss: 7.564855]\n",
      "343 [D loss: 1.471928, acc.: 50.00%] [G loss: 8.366185]\n",
      "344 [D loss: 1.516495, acc.: 50.00%] [G loss: 6.866033]\n",
      "345 [D loss: 1.456108, acc.: 50.00%] [G loss: 6.508688]\n",
      "346 [D loss: 1.476321, acc.: 50.00%] [G loss: 7.874388]\n",
      "347 [D loss: 1.485982, acc.: 50.00%] [G loss: 8.228495]\n",
      "348 [D loss: 1.500340, acc.: 50.00%] [G loss: 8.820784]\n",
      "349 [D loss: 1.484408, acc.: 50.00%] [G loss: 7.857659]\n",
      "350 [D loss: 1.491647, acc.: 50.00%] [G loss: 6.981715]\n",
      "351 [D loss: 1.488192, acc.: 50.00%] [G loss: 11.493522]\n",
      "352 [D loss: 1.507213, acc.: 50.00%] [G loss: 8.796923]\n",
      "353 [D loss: 1.476149, acc.: 50.00%] [G loss: 7.437361]\n",
      "354 [D loss: 1.513546, acc.: 50.00%] [G loss: 10.504827]\n",
      "355 [D loss: 1.497964, acc.: 50.00%] [G loss: 9.803311]\n",
      "356 [D loss: 1.499888, acc.: 50.00%] [G loss: 6.216230]\n",
      "357 [D loss: 1.502130, acc.: 50.00%] [G loss: 6.591786]\n",
      "358 [D loss: 1.508388, acc.: 50.00%] [G loss: 8.682712]\n",
      "359 [D loss: 1.473009, acc.: 50.00%] [G loss: 8.967233]\n",
      "360 [D loss: 1.493539, acc.: 50.00%] [G loss: 5.475515]\n",
      "361 [D loss: 1.497393, acc.: 50.00%] [G loss: 8.766218]\n",
      "362 [D loss: 1.509314, acc.: 50.00%] [G loss: 8.947210]\n",
      "363 [D loss: 1.485624, acc.: 50.00%] [G loss: 9.519910]\n",
      "364 [D loss: 1.435243, acc.: 50.00%] [G loss: 7.474426]\n",
      "365 [D loss: 1.500905, acc.: 50.00%] [G loss: 10.307076]\n",
      "366 [D loss: 1.420862, acc.: 50.00%] [G loss: 3.926848]\n",
      "367 [D loss: 1.520500, acc.: 50.00%] [G loss: 8.392702]\n",
      "368 [D loss: 1.516958, acc.: 50.00%] [G loss: 8.451106]\n",
      "369 [D loss: 1.497025, acc.: 50.00%] [G loss: 7.695834]\n",
      "370 [D loss: 1.464091, acc.: 50.00%] [G loss: 9.525254]\n",
      "371 [D loss: 1.469788, acc.: 50.00%] [G loss: 8.307961]\n",
      "372 [D loss: 1.477073, acc.: 50.00%] [G loss: 9.069514]\n",
      "373 [D loss: 1.457626, acc.: 50.00%] [G loss: 6.106180]\n",
      "374 [D loss: 1.490573, acc.: 50.00%] [G loss: 9.532434]\n",
      "375 [D loss: 1.476068, acc.: 50.00%] [G loss: 8.026170]\n",
      "376 [D loss: 1.484366, acc.: 50.00%] [G loss: 8.099169]\n",
      "377 [D loss: 1.416739, acc.: 50.00%] [G loss: 7.720758]\n",
      "378 [D loss: 1.511562, acc.: 50.00%] [G loss: 5.629303]\n",
      "379 [D loss: 1.422961, acc.: 50.00%] [G loss: 10.659200]\n",
      "380 [D loss: 1.464564, acc.: 50.00%] [G loss: 8.310240]\n",
      "381 [D loss: 1.442828, acc.: 50.00%] [G loss: 10.337357]\n",
      "382 [D loss: 1.463119, acc.: 50.00%] [G loss: 7.599459]\n",
      "383 [D loss: 1.464544, acc.: 50.00%] [G loss: 9.024671]\n",
      "384 [D loss: 1.409340, acc.: 50.00%] [G loss: 8.640683]\n",
      "385 [D loss: 1.434676, acc.: 50.00%] [G loss: 8.467041]\n",
      "386 [D loss: 1.490967, acc.: 50.00%] [G loss: 6.310935]\n",
      "387 [D loss: 1.457165, acc.: 50.00%] [G loss: 10.079610]\n",
      "388 [D loss: 1.432304, acc.: 50.00%] [G loss: 5.830487]\n",
      "389 [D loss: 1.524828, acc.: 50.00%] [G loss: 8.233315]\n",
      "390 [D loss: 1.394534, acc.: 50.00%] [G loss: 7.657518]\n",
      "391 [D loss: 1.435061, acc.: 50.00%] [G loss: 5.736402]\n",
      "392 [D loss: 1.488573, acc.: 50.00%] [G loss: 7.058151]\n",
      "393 [D loss: 1.468645, acc.: 50.00%] [G loss: 8.158499]\n",
      "394 [D loss: 1.413215, acc.: 50.00%] [G loss: 7.745283]\n",
      "395 [D loss: 1.481493, acc.: 50.00%] [G loss: 6.046548]\n",
      "396 [D loss: 1.399035, acc.: 50.00%] [G loss: 8.256170]\n",
      "397 [D loss: 1.395138, acc.: 50.00%] [G loss: 6.252271]\n",
      "398 [D loss: 1.437034, acc.: 50.00%] [G loss: 6.763644]\n",
      "399 [D loss: 1.462519, acc.: 50.00%] [G loss: 5.800456]\n",
      "400 [D loss: 1.401261, acc.: 50.00%] [G loss: 6.630481]\n",
      "401 [D loss: 1.414738, acc.: 50.00%] [G loss: 9.867954]\n",
      "402 [D loss: 1.434471, acc.: 50.00%] [G loss: 8.836864]\n",
      "403 [D loss: 1.427131, acc.: 50.00%] [G loss: 5.447080]\n",
      "404 [D loss: 1.394891, acc.: 50.00%] [G loss: 8.708875]\n",
      "405 [D loss: 1.402608, acc.: 50.00%] [G loss: 9.377643]\n",
      "406 [D loss: 1.428594, acc.: 50.00%] [G loss: 7.696039]\n",
      "407 [D loss: 1.445745, acc.: 50.00%] [G loss: 7.355481]\n",
      "408 [D loss: 1.457043, acc.: 50.00%] [G loss: 8.379717]\n",
      "409 [D loss: 1.430888, acc.: 50.00%] [G loss: 5.110744]\n",
      "410 [D loss: 1.477006, acc.: 50.00%] [G loss: 5.451377]\n",
      "411 [D loss: 1.433864, acc.: 50.00%] [G loss: 8.418346]\n",
      "412 [D loss: 1.405520, acc.: 50.00%] [G loss: 5.169287]\n",
      "413 [D loss: 1.437871, acc.: 50.00%] [G loss: 5.057544]\n",
      "414 [D loss: 1.426283, acc.: 50.00%] [G loss: 6.482852]\n",
      "415 [D loss: 1.360164, acc.: 50.00%] [G loss: 3.133319]\n",
      "416 [D loss: 1.374684, acc.: 50.00%] [G loss: 3.855409]\n",
      "417 [D loss: 1.427167, acc.: 50.00%] [G loss: 6.423378]\n",
      "418 [D loss: 1.440283, acc.: 50.00%] [G loss: 7.033309]\n",
      "419 [D loss: 1.425863, acc.: 50.00%] [G loss: 5.759868]\n",
      "420 [D loss: 1.425685, acc.: 50.00%] [G loss: 6.637815]\n",
      "421 [D loss: 1.423481, acc.: 50.00%] [G loss: 5.173305]\n",
      "422 [D loss: 1.423685, acc.: 50.00%] [G loss: 5.086030]\n",
      "423 [D loss: 1.438968, acc.: 50.00%] [G loss: 4.865853]\n",
      "424 [D loss: 1.433988, acc.: 50.00%] [G loss: 7.202062]\n",
      "425 [D loss: 1.406126, acc.: 50.00%] [G loss: 4.537515]\n",
      "426 [D loss: 1.411154, acc.: 50.00%] [G loss: 6.436150]\n",
      "427 [D loss: 1.438472, acc.: 50.00%] [G loss: 5.815733]\n",
      "428 [D loss: 1.389966, acc.: 50.00%] [G loss: 6.632803]\n",
      "429 [D loss: 1.400439, acc.: 50.00%] [G loss: 5.109970]\n",
      "430 [D loss: 1.374735, acc.: 50.00%] [G loss: 6.009842]\n",
      "431 [D loss: 1.452081, acc.: 50.00%] [G loss: 5.502217]\n",
      "432 [D loss: 1.391771, acc.: 50.00%] [G loss: 7.437057]\n",
      "433 [D loss: 1.377831, acc.: 50.00%] [G loss: 6.124184]\n",
      "434 [D loss: 1.470437, acc.: 50.00%] [G loss: 4.510423]\n",
      "435 [D loss: 1.450396, acc.: 50.00%] [G loss: 8.471034]\n",
      "436 [D loss: 1.373644, acc.: 50.00%] [G loss: 3.330507]\n",
      "437 [D loss: 1.382911, acc.: 50.00%] [G loss: 6.868463]\n",
      "438 [D loss: 1.405963, acc.: 50.00%] [G loss: 4.336841]\n",
      "439 [D loss: 1.386673, acc.: 50.00%] [G loss: 6.410689]\n",
      "440 [D loss: 1.367312, acc.: 50.00%] [G loss: 4.531652]\n",
      "441 [D loss: 1.417740, acc.: 50.00%] [G loss: 5.979304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442 [D loss: 1.460059, acc.: 50.00%] [G loss: 6.777840]\n",
      "443 [D loss: 1.394670, acc.: 50.00%] [G loss: 5.807047]\n",
      "444 [D loss: 1.335128, acc.: 50.00%] [G loss: 6.548724]\n",
      "445 [D loss: 1.314468, acc.: 50.00%] [G loss: 4.520953]\n",
      "446 [D loss: 1.422111, acc.: 50.00%] [G loss: 4.763595]\n",
      "447 [D loss: 1.379616, acc.: 50.00%] [G loss: 5.691050]\n",
      "448 [D loss: 1.331681, acc.: 50.00%] [G loss: 4.804980]\n",
      "449 [D loss: 1.400488, acc.: 50.00%] [G loss: 4.842690]\n",
      "450 [D loss: 1.398689, acc.: 50.00%] [G loss: 4.015475]\n",
      "451 [D loss: 1.366085, acc.: 50.00%] [G loss: 6.659754]\n",
      "452 [D loss: 1.393770, acc.: 50.00%] [G loss: 5.636536]\n",
      "453 [D loss: 1.325139, acc.: 50.00%] [G loss: 3.278681]\n",
      "454 [D loss: 1.355003, acc.: 50.00%] [G loss: 7.430669]\n",
      "455 [D loss: 1.338534, acc.: 50.00%] [G loss: 6.880937]\n",
      "456 [D loss: 1.350309, acc.: 50.00%] [G loss: 5.459672]\n",
      "457 [D loss: 1.367330, acc.: 50.00%] [G loss: 7.412926]\n",
      "458 [D loss: 1.355929, acc.: 50.00%] [G loss: 6.098155]\n",
      "459 [D loss: 1.364528, acc.: 50.00%] [G loss: 4.081029]\n",
      "460 [D loss: 1.296964, acc.: 50.00%] [G loss: 4.121615]\n",
      "461 [D loss: 1.385359, acc.: 50.00%] [G loss: 5.433827]\n",
      "462 [D loss: 1.359141, acc.: 50.00%] [G loss: 4.779919]\n",
      "463 [D loss: 1.301208, acc.: 50.00%] [G loss: 6.046651]\n",
      "464 [D loss: 1.346743, acc.: 50.00%] [G loss: 5.582568]\n",
      "465 [D loss: 1.340853, acc.: 50.00%] [G loss: 6.577611]\n",
      "466 [D loss: 1.354095, acc.: 50.00%] [G loss: 4.578288]\n",
      "467 [D loss: 1.381018, acc.: 50.00%] [G loss: 3.399471]\n",
      "468 [D loss: 1.368994, acc.: 50.00%] [G loss: 7.346561]\n",
      "469 [D loss: 1.344100, acc.: 50.00%] [G loss: 3.666303]\n",
      "470 [D loss: 1.326653, acc.: 50.00%] [G loss: 4.885763]\n",
      "471 [D loss: 1.318874, acc.: 50.00%] [G loss: 6.148119]\n",
      "472 [D loss: 1.406388, acc.: 50.00%] [G loss: 4.632068]\n",
      "473 [D loss: 1.383384, acc.: 50.00%] [G loss: 5.686285]\n",
      "474 [D loss: 1.344536, acc.: 50.00%] [G loss: 5.803249]\n",
      "475 [D loss: 1.335165, acc.: 50.00%] [G loss: 8.504128]\n",
      "476 [D loss: 1.305963, acc.: 50.00%] [G loss: 6.006753]\n",
      "477 [D loss: 1.331605, acc.: 50.00%] [G loss: 6.591182]\n",
      "478 [D loss: 1.315652, acc.: 50.00%] [G loss: 8.408551]\n",
      "479 [D loss: 1.353784, acc.: 50.00%] [G loss: 7.359281]\n",
      "480 [D loss: 1.310080, acc.: 50.00%] [G loss: 7.951148]\n",
      "481 [D loss: 1.303608, acc.: 50.00%] [G loss: 8.479062]\n",
      "482 [D loss: 1.323843, acc.: 50.00%] [G loss: 8.258946]\n",
      "483 [D loss: 1.380127, acc.: 50.00%] [G loss: 7.322261]\n",
      "484 [D loss: 1.349542, acc.: 50.00%] [G loss: 9.392472]\n",
      "485 [D loss: 1.354881, acc.: 50.00%] [G loss: 5.295806]\n",
      "486 [D loss: 1.400377, acc.: 50.00%] [G loss: 7.176625]\n",
      "487 [D loss: 1.269557, acc.: 50.00%] [G loss: 5.773456]\n",
      "488 [D loss: 1.345691, acc.: 50.00%] [G loss: 8.963807]\n",
      "489 [D loss: 1.294953, acc.: 50.00%] [G loss: 5.605418]\n",
      "490 [D loss: 1.313349, acc.: 50.00%] [G loss: 7.728023]\n",
      "491 [D loss: 1.345775, acc.: 50.00%] [G loss: 7.616423]\n",
      "492 [D loss: 1.275614, acc.: 50.00%] [G loss: 7.807292]\n",
      "493 [D loss: 1.302689, acc.: 50.00%] [G loss: 8.231421]\n",
      "494 [D loss: 1.301737, acc.: 50.00%] [G loss: 9.464162]\n",
      "495 [D loss: 1.313205, acc.: 50.00%] [G loss: 8.553828]\n",
      "496 [D loss: 1.330599, acc.: 50.00%] [G loss: 8.437473]\n",
      "497 [D loss: 1.283786, acc.: 50.00%] [G loss: 8.688239]\n",
      "498 [D loss: 1.392337, acc.: 50.00%] [G loss: 7.919215]\n",
      "499 [D loss: 1.283951, acc.: 50.00%] [G loss: 5.393087]\n",
      "500 [D loss: 1.253269, acc.: 50.00%] [G loss: 6.107624]\n",
      "501 [D loss: 1.304559, acc.: 50.00%] [G loss: 7.177099]\n",
      "502 [D loss: 1.327295, acc.: 50.00%] [G loss: 5.881994]\n",
      "503 [D loss: 1.323409, acc.: 50.00%] [G loss: 7.850550]\n",
      "504 [D loss: 1.333490, acc.: 50.00%] [G loss: 6.304141]\n",
      "505 [D loss: 1.298335, acc.: 50.00%] [G loss: 10.082457]\n",
      "506 [D loss: 1.303308, acc.: 50.00%] [G loss: 6.921880]\n",
      "507 [D loss: 1.317945, acc.: 50.00%] [G loss: 5.390958]\n",
      "508 [D loss: 1.312262, acc.: 50.00%] [G loss: 6.512670]\n",
      "509 [D loss: 1.311481, acc.: 50.00%] [G loss: 7.931439]\n",
      "510 [D loss: 1.290453, acc.: 50.00%] [G loss: 5.815275]\n",
      "511 [D loss: 1.278620, acc.: 50.00%] [G loss: 6.565094]\n",
      "512 [D loss: 1.339022, acc.: 50.00%] [G loss: 5.267140]\n",
      "513 [D loss: 1.338573, acc.: 50.00%] [G loss: 5.554916]\n",
      "514 [D loss: 1.312663, acc.: 50.00%] [G loss: 7.694782]\n",
      "515 [D loss: 1.316630, acc.: 50.00%] [G loss: 6.417270]\n",
      "516 [D loss: 1.297733, acc.: 50.00%] [G loss: 6.790205]\n",
      "517 [D loss: 1.312238, acc.: 50.00%] [G loss: 3.824392]\n",
      "518 [D loss: 1.303925, acc.: 50.00%] [G loss: 6.605606]\n",
      "519 [D loss: 1.282700, acc.: 50.00%] [G loss: 5.930327]\n",
      "520 [D loss: 1.270526, acc.: 50.00%] [G loss: 5.197118]\n",
      "521 [D loss: 1.315100, acc.: 50.00%] [G loss: 6.887118]\n",
      "522 [D loss: 1.312430, acc.: 50.00%] [G loss: 4.635368]\n",
      "523 [D loss: 1.277567, acc.: 50.00%] [G loss: 5.828506]\n",
      "524 [D loss: 1.276775, acc.: 50.00%] [G loss: 5.192036]\n",
      "525 [D loss: 1.318619, acc.: 50.00%] [G loss: 7.669290]\n",
      "526 [D loss: 1.308701, acc.: 50.00%] [G loss: 7.312565]\n",
      "527 [D loss: 1.247455, acc.: 50.00%] [G loss: 5.518408]\n",
      "528 [D loss: 1.298359, acc.: 50.00%] [G loss: 4.258698]\n",
      "529 [D loss: 1.266658, acc.: 50.00%] [G loss: 6.535979]\n",
      "530 [D loss: 1.261173, acc.: 50.00%] [G loss: 6.858238]\n",
      "531 [D loss: 1.324088, acc.: 50.00%] [G loss: 6.498344]\n",
      "532 [D loss: 1.259618, acc.: 50.00%] [G loss: 4.920663]\n",
      "533 [D loss: 1.266310, acc.: 50.00%] [G loss: 6.794869]\n",
      "534 [D loss: 1.367100, acc.: 50.00%] [G loss: 6.288882]\n",
      "535 [D loss: 1.272848, acc.: 50.00%] [G loss: 6.131565]\n",
      "536 [D loss: 1.282047, acc.: 50.00%] [G loss: 4.761307]\n",
      "537 [D loss: 1.310296, acc.: 50.00%] [G loss: 4.875556]\n",
      "538 [D loss: 1.332926, acc.: 50.00%] [G loss: 4.753835]\n",
      "539 [D loss: 1.357008, acc.: 50.00%] [G loss: 5.490088]\n",
      "540 [D loss: 1.322816, acc.: 50.00%] [G loss: 3.880833]\n",
      "541 [D loss: 1.264186, acc.: 50.00%] [G loss: 4.674306]\n",
      "542 [D loss: 1.276189, acc.: 50.00%] [G loss: 4.205487]\n",
      "543 [D loss: 1.246334, acc.: 50.00%] [G loss: 4.163752]\n",
      "544 [D loss: 1.274715, acc.: 50.00%] [G loss: 7.879614]\n",
      "545 [D loss: 1.292922, acc.: 50.00%] [G loss: 5.263519]\n",
      "546 [D loss: 1.260530, acc.: 50.00%] [G loss: 4.334808]\n",
      "547 [D loss: 1.297229, acc.: 50.00%] [G loss: 5.288739]\n",
      "548 [D loss: 1.300618, acc.: 50.00%] [G loss: 5.115140]\n",
      "549 [D loss: 1.302771, acc.: 50.00%] [G loss: 4.721042]\n",
      "550 [D loss: 1.312270, acc.: 50.00%] [G loss: 5.343071]\n",
      "551 [D loss: 1.295141, acc.: 50.00%] [G loss: 3.578184]\n",
      "552 [D loss: 1.292939, acc.: 50.00%] [G loss: 3.529463]\n",
      "553 [D loss: 1.272511, acc.: 50.00%] [G loss: 6.032843]\n",
      "554 [D loss: 1.273070, acc.: 50.00%] [G loss: 3.109954]\n",
      "555 [D loss: 1.239135, acc.: 50.00%] [G loss: 3.467759]\n",
      "556 [D loss: 1.246986, acc.: 50.00%] [G loss: 3.492741]\n",
      "557 [D loss: 1.243172, acc.: 50.00%] [G loss: 3.392286]\n",
      "558 [D loss: 1.306379, acc.: 50.00%] [G loss: 4.096520]\n",
      "559 [D loss: 1.267809, acc.: 50.00%] [G loss: 4.099033]\n",
      "560 [D loss: 1.264247, acc.: 50.00%] [G loss: 5.334927]\n",
      "561 [D loss: 1.258559, acc.: 50.00%] [G loss: 3.697639]\n",
      "562 [D loss: 1.249944, acc.: 50.00%] [G loss: 4.215668]\n",
      "563 [D loss: 1.273162, acc.: 50.00%] [G loss: 3.970769]\n",
      "564 [D loss: 1.284498, acc.: 50.00%] [G loss: 5.090795]\n",
      "565 [D loss: 1.252725, acc.: 50.00%] [G loss: 4.144639]\n",
      "566 [D loss: 1.271005, acc.: 50.00%] [G loss: 3.359428]\n",
      "567 [D loss: 1.325230, acc.: 50.00%] [G loss: 5.018807]\n",
      "568 [D loss: 1.307254, acc.: 50.00%] [G loss: 6.947558]\n",
      "569 [D loss: 1.238878, acc.: 50.00%] [G loss: 3.285084]\n",
      "570 [D loss: 1.289248, acc.: 50.00%] [G loss: 4.645565]\n",
      "571 [D loss: 1.281444, acc.: 50.00%] [G loss: 3.557424]\n",
      "572 [D loss: 1.289453, acc.: 50.00%] [G loss: 4.857459]\n",
      "573 [D loss: 1.277750, acc.: 50.00%] [G loss: 3.926979]\n",
      "574 [D loss: 1.251681, acc.: 50.00%] [G loss: 3.438437]\n",
      "575 [D loss: 1.300122, acc.: 50.00%] [G loss: 4.220423]\n",
      "576 [D loss: 1.288778, acc.: 50.00%] [G loss: 2.870056]\n",
      "577 [D loss: 1.244236, acc.: 50.00%] [G loss: 2.639278]\n",
      "578 [D loss: 1.252419, acc.: 50.00%] [G loss: 3.602925]\n",
      "579 [D loss: 1.221351, acc.: 50.00%] [G loss: 4.332516]\n",
      "580 [D loss: 1.234369, acc.: 50.00%] [G loss: 3.443495]\n",
      "581 [D loss: 1.288043, acc.: 50.00%] [G loss: 3.785186]\n",
      "582 [D loss: 1.292233, acc.: 50.00%] [G loss: 4.703338]\n",
      "583 [D loss: 1.227171, acc.: 50.00%] [G loss: 4.085648]\n",
      "584 [D loss: 1.312265, acc.: 50.00%] [G loss: 2.723356]\n",
      "585 [D loss: 1.249304, acc.: 50.00%] [G loss: 3.451682]\n",
      "586 [D loss: 1.216559, acc.: 50.00%] [G loss: 3.637022]\n",
      "587 [D loss: 1.304312, acc.: 50.00%] [G loss: 4.002848]\n",
      "588 [D loss: 1.279488, acc.: 50.00%] [G loss: 3.354414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589 [D loss: 1.272686, acc.: 50.00%] [G loss: 2.741802]\n",
      "590 [D loss: 1.278945, acc.: 50.00%] [G loss: 4.348274]\n",
      "591 [D loss: 1.254716, acc.: 50.00%] [G loss: 5.250027]\n",
      "592 [D loss: 1.276722, acc.: 50.00%] [G loss: 4.759514]\n",
      "593 [D loss: 1.224299, acc.: 50.00%] [G loss: 3.930524]\n",
      "594 [D loss: 1.233020, acc.: 50.00%] [G loss: 2.903455]\n",
      "595 [D loss: 1.265435, acc.: 50.00%] [G loss: 4.075461]\n",
      "596 [D loss: 1.228385, acc.: 50.00%] [G loss: 3.441188]\n",
      "597 [D loss: 1.254793, acc.: 50.00%] [G loss: 5.475454]\n",
      "598 [D loss: 1.265302, acc.: 50.00%] [G loss: 3.793089]\n",
      "599 [D loss: 1.258045, acc.: 50.00%] [G loss: 3.313280]\n",
      "600 [D loss: 1.244513, acc.: 50.00%] [G loss: 4.784756]\n",
      "601 [D loss: 1.202035, acc.: 50.00%] [G loss: 4.524681]\n",
      "602 [D loss: 1.258195, acc.: 50.00%] [G loss: 2.523983]\n",
      "603 [D loss: 1.322092, acc.: 50.00%] [G loss: 3.076266]\n",
      "604 [D loss: 1.222549, acc.: 50.00%] [G loss: 3.101495]\n",
      "605 [D loss: 1.232145, acc.: 50.00%] [G loss: 3.398639]\n",
      "606 [D loss: 1.216017, acc.: 50.00%] [G loss: 4.701748]\n",
      "607 [D loss: 1.210401, acc.: 50.00%] [G loss: 3.226953]\n",
      "608 [D loss: 1.215177, acc.: 50.00%] [G loss: 2.390324]\n",
      "609 [D loss: 1.209745, acc.: 50.00%] [G loss: 3.712901]\n",
      "610 [D loss: 1.258817, acc.: 50.00%] [G loss: 6.541852]\n",
      "611 [D loss: 1.239031, acc.: 50.00%] [G loss: 5.159512]\n",
      "612 [D loss: 1.233018, acc.: 50.00%] [G loss: 2.830222]\n",
      "613 [D loss: 1.250207, acc.: 50.00%] [G loss: 3.919910]\n",
      "614 [D loss: 1.271637, acc.: 50.00%] [G loss: 3.518216]\n",
      "615 [D loss: 1.308272, acc.: 50.00%] [G loss: 3.122013]\n",
      "616 [D loss: 1.216824, acc.: 50.00%] [G loss: 2.488493]\n",
      "617 [D loss: 1.186491, acc.: 50.00%] [G loss: 3.927330]\n",
      "618 [D loss: 1.199482, acc.: 50.00%] [G loss: 2.688575]\n",
      "619 [D loss: 1.229653, acc.: 50.00%] [G loss: 2.621372]\n",
      "620 [D loss: 1.233827, acc.: 50.00%] [G loss: 3.848547]\n",
      "621 [D loss: 1.189263, acc.: 50.00%] [G loss: 3.274257]\n",
      "622 [D loss: 1.256964, acc.: 50.00%] [G loss: 3.888788]\n",
      "623 [D loss: 1.270308, acc.: 50.00%] [G loss: 3.084243]\n",
      "624 [D loss: 1.233942, acc.: 50.00%] [G loss: 3.890530]\n",
      "625 [D loss: 1.148311, acc.: 50.00%] [G loss: 3.782928]\n",
      "626 [D loss: 1.267716, acc.: 50.00%] [G loss: 3.325285]\n",
      "627 [D loss: 1.239548, acc.: 50.00%] [G loss: 4.518021]\n",
      "628 [D loss: 1.288514, acc.: 50.00%] [G loss: 2.670308]\n",
      "629 [D loss: 1.210925, acc.: 50.00%] [G loss: 2.541856]\n",
      "630 [D loss: 1.218112, acc.: 50.00%] [G loss: 3.039733]\n",
      "631 [D loss: 1.262747, acc.: 50.00%] [G loss: 3.168930]\n",
      "632 [D loss: 1.222477, acc.: 50.00%] [G loss: 2.329239]\n",
      "633 [D loss: 1.229117, acc.: 50.00%] [G loss: 3.426194]\n",
      "634 [D loss: 1.228358, acc.: 50.00%] [G loss: 2.756455]\n",
      "635 [D loss: 1.237996, acc.: 50.00%] [G loss: 3.660532]\n",
      "636 [D loss: 1.213886, acc.: 50.00%] [G loss: 3.483608]\n",
      "637 [D loss: 1.202649, acc.: 50.00%] [G loss: 2.974051]\n",
      "638 [D loss: 1.209719, acc.: 50.00%] [G loss: 4.608841]\n",
      "639 [D loss: 1.250046, acc.: 50.00%] [G loss: 3.489785]\n",
      "640 [D loss: 1.225404, acc.: 50.00%] [G loss: 3.549000]\n",
      "641 [D loss: 1.260091, acc.: 50.00%] [G loss: 2.619078]\n",
      "642 [D loss: 1.213724, acc.: 50.00%] [G loss: 3.160800]\n",
      "643 [D loss: 1.249756, acc.: 50.00%] [G loss: 3.205980]\n",
      "644 [D loss: 1.236439, acc.: 50.00%] [G loss: 3.185401]\n",
      "645 [D loss: 1.216788, acc.: 50.00%] [G loss: 3.845195]\n",
      "646 [D loss: 1.232926, acc.: 50.00%] [G loss: 2.441653]\n",
      "647 [D loss: 1.185161, acc.: 50.00%] [G loss: 2.529388]\n",
      "648 [D loss: 1.190183, acc.: 50.00%] [G loss: 2.915933]\n",
      "649 [D loss: 1.266938, acc.: 50.00%] [G loss: 3.687129]\n",
      "650 [D loss: 1.213636, acc.: 50.00%] [G loss: 3.100551]\n",
      "651 [D loss: 1.188933, acc.: 50.00%] [G loss: 3.618174]\n",
      "652 [D loss: 1.259660, acc.: 50.00%] [G loss: 2.329821]\n",
      "653 [D loss: 1.228242, acc.: 50.00%] [G loss: 3.883174]\n",
      "654 [D loss: 1.197091, acc.: 50.00%] [G loss: 2.453765]\n",
      "655 [D loss: 1.245090, acc.: 50.00%] [G loss: 3.103410]\n",
      "656 [D loss: 1.226817, acc.: 50.00%] [G loss: 3.579865]\n",
      "657 [D loss: 1.155312, acc.: 50.00%] [G loss: 2.556312]\n",
      "658 [D loss: 1.193454, acc.: 50.00%] [G loss: 4.314477]\n",
      "659 [D loss: 1.257821, acc.: 50.00%] [G loss: 2.507134]\n",
      "660 [D loss: 1.217470, acc.: 50.00%] [G loss: 3.094256]\n",
      "661 [D loss: 1.264502, acc.: 50.00%] [G loss: 3.698711]\n",
      "662 [D loss: 1.166909, acc.: 50.00%] [G loss: 3.038388]\n",
      "663 [D loss: 1.178682, acc.: 50.00%] [G loss: 2.973577]\n",
      "664 [D loss: 1.146788, acc.: 50.00%] [G loss: 3.702054]\n",
      "665 [D loss: 1.206314, acc.: 50.00%] [G loss: 3.096334]\n",
      "666 [D loss: 1.171994, acc.: 50.00%] [G loss: 3.009267]\n",
      "667 [D loss: 1.203192, acc.: 50.00%] [G loss: 2.893382]\n",
      "668 [D loss: 1.198495, acc.: 50.00%] [G loss: 3.879035]\n",
      "669 [D loss: 1.185749, acc.: 50.00%] [G loss: 2.452618]\n",
      "670 [D loss: 1.172919, acc.: 50.00%] [G loss: 3.451592]\n",
      "671 [D loss: 1.216913, acc.: 50.00%] [G loss: 2.592680]\n",
      "672 [D loss: 1.199182, acc.: 50.00%] [G loss: 2.678237]\n",
      "673 [D loss: 1.172403, acc.: 50.00%] [G loss: 3.171112]\n",
      "674 [D loss: 1.272425, acc.: 50.00%] [G loss: 3.832360]\n",
      "675 [D loss: 1.191575, acc.: 50.00%] [G loss: 3.523454]\n",
      "676 [D loss: 1.151929, acc.: 50.00%] [G loss: 3.067108]\n",
      "677 [D loss: 1.126295, acc.: 50.00%] [G loss: 2.537881]\n",
      "678 [D loss: 1.219250, acc.: 50.00%] [G loss: 2.837929]\n",
      "679 [D loss: 1.174775, acc.: 50.00%] [G loss: 3.190319]\n",
      "680 [D loss: 1.210816, acc.: 50.00%] [G loss: 2.574299]\n",
      "681 [D loss: 1.196908, acc.: 50.00%] [G loss: 2.907043]\n",
      "682 [D loss: 1.200276, acc.: 50.00%] [G loss: 3.776473]\n",
      "683 [D loss: 1.178990, acc.: 50.00%] [G loss: 3.742659]\n",
      "684 [D loss: 1.211025, acc.: 50.00%] [G loss: 3.351435]\n",
      "685 [D loss: 1.142771, acc.: 50.00%] [G loss: 2.982020]\n",
      "686 [D loss: 1.174667, acc.: 50.00%] [G loss: 3.968163]\n",
      "687 [D loss: 1.202385, acc.: 50.00%] [G loss: 2.409013]\n",
      "688 [D loss: 1.166601, acc.: 50.00%] [G loss: 3.910735]\n",
      "689 [D loss: 1.142536, acc.: 50.00%] [G loss: 3.901098]\n",
      "690 [D loss: 1.243031, acc.: 50.00%] [G loss: 3.291930]\n",
      "691 [D loss: 1.166457, acc.: 50.00%] [G loss: 2.498166]\n",
      "692 [D loss: 1.200903, acc.: 50.00%] [G loss: 3.248309]\n",
      "693 [D loss: 1.179489, acc.: 50.00%] [G loss: 3.318459]\n",
      "694 [D loss: 1.104434, acc.: 50.00%] [G loss: 3.723881]\n",
      "695 [D loss: 1.195418, acc.: 50.00%] [G loss: 3.804131]\n",
      "696 [D loss: 1.143411, acc.: 50.00%] [G loss: 2.608101]\n",
      "697 [D loss: 1.162217, acc.: 50.00%] [G loss: 3.765418]\n",
      "698 [D loss: 1.183629, acc.: 50.00%] [G loss: 3.192574]\n",
      "699 [D loss: 1.173254, acc.: 50.00%] [G loss: 3.248594]\n",
      "700 [D loss: 1.168112, acc.: 50.00%] [G loss: 3.438558]\n",
      "701 [D loss: 1.168952, acc.: 50.00%] [G loss: 2.651622]\n",
      "702 [D loss: 1.191487, acc.: 50.00%] [G loss: 3.576912]\n",
      "703 [D loss: 1.191799, acc.: 50.00%] [G loss: 3.717746]\n",
      "704 [D loss: 1.193432, acc.: 50.00%] [G loss: 2.554754]\n",
      "705 [D loss: 1.182711, acc.: 50.00%] [G loss: 3.225938]\n",
      "706 [D loss: 1.155999, acc.: 50.00%] [G loss: 4.454865]\n",
      "707 [D loss: 1.181031, acc.: 50.00%] [G loss: 3.290934]\n",
      "708 [D loss: 1.175387, acc.: 50.00%] [G loss: 2.368417]\n",
      "709 [D loss: 1.169539, acc.: 50.00%] [G loss: 3.491433]\n",
      "710 [D loss: 1.186388, acc.: 50.00%] [G loss: 2.531713]\n",
      "711 [D loss: 1.173322, acc.: 50.00%] [G loss: 4.378615]\n",
      "712 [D loss: 1.147226, acc.: 50.00%] [G loss: 2.499949]\n",
      "713 [D loss: 1.135289, acc.: 50.00%] [G loss: 2.452199]\n",
      "714 [D loss: 1.165961, acc.: 50.00%] [G loss: 2.654749]\n",
      "715 [D loss: 1.198924, acc.: 50.00%] [G loss: 2.560910]\n",
      "716 [D loss: 1.183919, acc.: 50.00%] [G loss: 2.323026]\n",
      "717 [D loss: 1.155134, acc.: 50.00%] [G loss: 3.102181]\n",
      "718 [D loss: 1.190160, acc.: 50.00%] [G loss: 3.615771]\n",
      "719 [D loss: 1.199712, acc.: 50.00%] [G loss: 3.083741]\n",
      "720 [D loss: 1.135796, acc.: 50.00%] [G loss: 2.523420]\n",
      "721 [D loss: 1.153716, acc.: 50.00%] [G loss: 2.957892]\n",
      "722 [D loss: 1.167125, acc.: 50.00%] [G loss: 3.073876]\n",
      "723 [D loss: 1.181550, acc.: 50.00%] [G loss: 2.433577]\n",
      "724 [D loss: 1.144926, acc.: 50.00%] [G loss: 3.065805]\n",
      "725 [D loss: 1.147827, acc.: 50.00%] [G loss: 2.417962]\n",
      "726 [D loss: 1.168401, acc.: 50.00%] [G loss: 2.252852]\n",
      "727 [D loss: 1.177814, acc.: 50.00%] [G loss: 2.343845]\n",
      "728 [D loss: 1.131110, acc.: 50.00%] [G loss: 2.488331]\n",
      "729 [D loss: 1.140972, acc.: 50.00%] [G loss: 3.192213]\n",
      "730 [D loss: 1.153950, acc.: 50.00%] [G loss: 2.668999]\n",
      "731 [D loss: 1.113555, acc.: 50.00%] [G loss: 3.061761]\n",
      "732 [D loss: 1.132756, acc.: 50.00%] [G loss: 2.913760]\n",
      "733 [D loss: 1.146330, acc.: 50.00%] [G loss: 2.231054]\n",
      "734 [D loss: 1.116380, acc.: 50.00%] [G loss: 2.998511]\n",
      "735 [D loss: 1.201226, acc.: 50.00%] [G loss: 3.596253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736 [D loss: 1.183407, acc.: 50.00%] [G loss: 2.829086]\n",
      "737 [D loss: 1.222493, acc.: 50.00%] [G loss: 2.364318]\n",
      "738 [D loss: 1.166381, acc.: 50.00%] [G loss: 2.852952]\n",
      "739 [D loss: 1.125589, acc.: 50.00%] [G loss: 3.187335]\n",
      "740 [D loss: 1.107753, acc.: 50.00%] [G loss: 2.935491]\n",
      "741 [D loss: 1.142646, acc.: 50.00%] [G loss: 2.281870]\n",
      "742 [D loss: 1.166515, acc.: 50.00%] [G loss: 3.021385]\n",
      "743 [D loss: 1.165322, acc.: 50.00%] [G loss: 2.267668]\n",
      "744 [D loss: 1.183825, acc.: 50.00%] [G loss: 2.643602]\n",
      "745 [D loss: 1.147832, acc.: 50.00%] [G loss: 2.788322]\n",
      "746 [D loss: 1.173910, acc.: 50.00%] [G loss: 2.341171]\n",
      "747 [D loss: 1.164811, acc.: 50.00%] [G loss: 2.567042]\n",
      "748 [D loss: 1.161829, acc.: 50.00%] [G loss: 2.546349]\n",
      "749 [D loss: 1.193778, acc.: 50.00%] [G loss: 2.050416]\n",
      "750 [D loss: 1.122177, acc.: 50.00%] [G loss: 4.388975]\n",
      "751 [D loss: 1.170019, acc.: 50.00%] [G loss: 2.982201]\n",
      "752 [D loss: 1.116402, acc.: 50.00%] [G loss: 2.662439]\n",
      "753 [D loss: 1.167819, acc.: 50.00%] [G loss: 3.129569]\n",
      "754 [D loss: 1.171576, acc.: 50.00%] [G loss: 2.992710]\n",
      "755 [D loss: 1.129757, acc.: 50.00%] [G loss: 4.288953]\n",
      "756 [D loss: 1.156440, acc.: 50.00%] [G loss: 2.994896]\n",
      "757 [D loss: 1.109070, acc.: 50.00%] [G loss: 2.614617]\n",
      "758 [D loss: 1.177709, acc.: 50.00%] [G loss: 2.432326]\n",
      "759 [D loss: 1.136283, acc.: 50.00%] [G loss: 2.073521]\n",
      "760 [D loss: 1.113959, acc.: 50.00%] [G loss: 2.131167]\n",
      "761 [D loss: 1.134806, acc.: 50.00%] [G loss: 3.024785]\n",
      "762 [D loss: 1.132830, acc.: 50.00%] [G loss: 2.369134]\n",
      "763 [D loss: 1.076686, acc.: 50.00%] [G loss: 3.070959]\n",
      "764 [D loss: 1.131733, acc.: 50.00%] [G loss: 3.157490]\n",
      "765 [D loss: 1.111262, acc.: 50.00%] [G loss: 3.266434]\n",
      "766 [D loss: 1.150051, acc.: 50.00%] [G loss: 2.461179]\n",
      "767 [D loss: 1.154643, acc.: 50.00%] [G loss: 2.994044]\n",
      "768 [D loss: 1.079051, acc.: 50.00%] [G loss: 3.239562]\n",
      "769 [D loss: 1.127375, acc.: 50.00%] [G loss: 2.541749]\n",
      "770 [D loss: 1.119951, acc.: 50.00%] [G loss: 4.098022]\n",
      "771 [D loss: 1.182092, acc.: 50.00%] [G loss: 3.536760]\n",
      "772 [D loss: 1.151113, acc.: 50.00%] [G loss: 3.662318]\n",
      "773 [D loss: 1.171140, acc.: 50.00%] [G loss: 2.222706]\n",
      "774 [D loss: 1.092121, acc.: 50.00%] [G loss: 2.448246]\n",
      "775 [D loss: 1.154403, acc.: 50.00%] [G loss: 3.130743]\n",
      "776 [D loss: 1.154951, acc.: 50.00%] [G loss: 3.510063]\n",
      "777 [D loss: 1.169985, acc.: 50.00%] [G loss: 2.883791]\n",
      "778 [D loss: 1.146443, acc.: 50.00%] [G loss: 2.220271]\n",
      "779 [D loss: 1.153239, acc.: 50.00%] [G loss: 3.105808]\n",
      "780 [D loss: 1.131887, acc.: 50.00%] [G loss: 2.462440]\n",
      "781 [D loss: 1.119662, acc.: 50.00%] [G loss: 2.433856]\n",
      "782 [D loss: 1.143524, acc.: 50.00%] [G loss: 2.356213]\n",
      "783 [D loss: 1.113761, acc.: 50.00%] [G loss: 3.037851]\n",
      "784 [D loss: 1.095693, acc.: 50.00%] [G loss: 2.891737]\n",
      "785 [D loss: 1.150095, acc.: 50.00%] [G loss: 1.948489]\n",
      "786 [D loss: 1.146814, acc.: 50.00%] [G loss: 2.666921]\n",
      "787 [D loss: 1.141683, acc.: 50.00%] [G loss: 2.922864]\n",
      "788 [D loss: 1.115641, acc.: 50.00%] [G loss: 2.827566]\n",
      "789 [D loss: 1.156020, acc.: 50.00%] [G loss: 2.947653]\n",
      "790 [D loss: 1.084863, acc.: 50.00%] [G loss: 2.072765]\n",
      "791 [D loss: 1.084535, acc.: 50.00%] [G loss: 2.105295]\n",
      "792 [D loss: 1.149100, acc.: 50.00%] [G loss: 2.557554]\n",
      "793 [D loss: 1.136300, acc.: 50.00%] [G loss: 3.100473]\n",
      "794 [D loss: 1.121958, acc.: 50.00%] [G loss: 2.168145]\n",
      "795 [D loss: 1.104631, acc.: 50.00%] [G loss: 2.649830]\n",
      "796 [D loss: 1.133290, acc.: 50.00%] [G loss: 2.373950]\n",
      "797 [D loss: 1.158740, acc.: 50.00%] [G loss: 2.213855]\n",
      "798 [D loss: 1.129346, acc.: 50.00%] [G loss: 2.581393]\n",
      "799 [D loss: 1.166639, acc.: 50.00%] [G loss: 2.936722]\n",
      "800 [D loss: 1.136271, acc.: 50.00%] [G loss: 2.247103]\n",
      "801 [D loss: 1.108035, acc.: 50.00%] [G loss: 2.504358]\n",
      "802 [D loss: 1.125050, acc.: 50.00%] [G loss: 2.560694]\n",
      "803 [D loss: 1.089507, acc.: 50.00%] [G loss: 2.153363]\n",
      "804 [D loss: 1.104513, acc.: 50.00%] [G loss: 1.977323]\n",
      "805 [D loss: 1.113158, acc.: 50.00%] [G loss: 2.612837]\n",
      "806 [D loss: 1.118802, acc.: 50.00%] [G loss: 2.190373]\n",
      "807 [D loss: 1.063188, acc.: 50.00%] [G loss: 2.298533]\n",
      "808 [D loss: 1.089377, acc.: 50.00%] [G loss: 2.735953]\n",
      "809 [D loss: 1.137377, acc.: 50.00%] [G loss: 2.662730]\n",
      "810 [D loss: 1.130140, acc.: 50.00%] [G loss: 2.138381]\n",
      "811 [D loss: 1.115323, acc.: 50.00%] [G loss: 2.913702]\n",
      "812 [D loss: 1.047371, acc.: 50.00%] [G loss: 2.423820]\n",
      "813 [D loss: 1.126616, acc.: 50.00%] [G loss: 2.096634]\n",
      "814 [D loss: 1.108920, acc.: 50.00%] [G loss: 3.779227]\n",
      "815 [D loss: 1.100847, acc.: 50.00%] [G loss: 2.332123]\n",
      "816 [D loss: 1.109298, acc.: 50.00%] [G loss: 2.258890]\n",
      "817 [D loss: 1.107779, acc.: 50.00%] [G loss: 2.229471]\n",
      "818 [D loss: 1.130764, acc.: 50.00%] [G loss: 2.272374]\n",
      "819 [D loss: 1.103530, acc.: 50.00%] [G loss: 2.819650]\n",
      "820 [D loss: 1.060208, acc.: 50.00%] [G loss: 1.981327]\n",
      "821 [D loss: 1.089303, acc.: 50.00%] [G loss: 2.470335]\n",
      "822 [D loss: 1.090228, acc.: 50.00%] [G loss: 2.140257]\n",
      "823 [D loss: 1.127301, acc.: 50.00%] [G loss: 2.972051]\n",
      "824 [D loss: 1.097889, acc.: 50.00%] [G loss: 2.128974]\n",
      "825 [D loss: 1.120136, acc.: 50.00%] [G loss: 3.040186]\n",
      "826 [D loss: 1.114433, acc.: 50.00%] [G loss: 3.085517]\n",
      "827 [D loss: 1.089285, acc.: 50.00%] [G loss: 2.374145]\n",
      "828 [D loss: 1.087644, acc.: 50.00%] [G loss: 2.908395]\n",
      "829 [D loss: 1.064163, acc.: 50.00%] [G loss: 2.312951]\n",
      "830 [D loss: 1.107553, acc.: 50.00%] [G loss: 2.257401]\n",
      "831 [D loss: 1.108252, acc.: 50.00%] [G loss: 2.122115]\n",
      "832 [D loss: 1.103785, acc.: 50.00%] [G loss: 2.188560]\n",
      "833 [D loss: 1.097645, acc.: 50.00%] [G loss: 2.642866]\n",
      "834 [D loss: 1.119014, acc.: 50.00%] [G loss: 2.032006]\n",
      "835 [D loss: 1.129962, acc.: 50.00%] [G loss: 2.081876]\n",
      "836 [D loss: 1.063241, acc.: 50.00%] [G loss: 2.106486]\n",
      "837 [D loss: 1.146981, acc.: 50.00%] [G loss: 3.018213]\n",
      "838 [D loss: 1.099511, acc.: 50.00%] [G loss: 2.778115]\n",
      "839 [D loss: 1.100367, acc.: 50.00%] [G loss: 2.125159]\n",
      "840 [D loss: 1.052544, acc.: 50.00%] [G loss: 2.171616]\n",
      "841 [D loss: 1.110356, acc.: 50.00%] [G loss: 2.683276]\n",
      "842 [D loss: 1.084463, acc.: 50.00%] [G loss: 3.679595]\n",
      "843 [D loss: 1.065630, acc.: 50.00%] [G loss: 2.369010]\n",
      "844 [D loss: 1.057697, acc.: 50.00%] [G loss: 2.007648]\n",
      "845 [D loss: 1.112953, acc.: 50.00%] [G loss: 2.119037]\n",
      "846 [D loss: 1.085360, acc.: 50.00%] [G loss: 2.833102]\n",
      "847 [D loss: 1.129934, acc.: 50.00%] [G loss: 2.374428]\n",
      "848 [D loss: 1.102974, acc.: 50.00%] [G loss: 2.210637]\n",
      "849 [D loss: 1.066867, acc.: 50.00%] [G loss: 2.862939]\n",
      "850 [D loss: 1.063170, acc.: 50.00%] [G loss: 3.565356]\n",
      "851 [D loss: 1.056936, acc.: 50.00%] [G loss: 2.213938]\n",
      "852 [D loss: 1.133682, acc.: 50.00%] [G loss: 2.250571]\n",
      "853 [D loss: 1.086174, acc.: 50.00%] [G loss: 2.327216]\n",
      "854 [D loss: 1.081624, acc.: 50.00%] [G loss: 2.345792]\n",
      "855 [D loss: 1.082460, acc.: 50.00%] [G loss: 3.769459]\n",
      "856 [D loss: 1.095594, acc.: 50.00%] [G loss: 2.643348]\n",
      "857 [D loss: 1.054858, acc.: 50.00%] [G loss: 2.515340]\n",
      "858 [D loss: 1.037285, acc.: 50.00%] [G loss: 2.134725]\n",
      "859 [D loss: 1.110692, acc.: 50.00%] [G loss: 2.958175]\n",
      "860 [D loss: 1.069288, acc.: 50.00%] [G loss: 2.064953]\n",
      "861 [D loss: 1.071483, acc.: 50.00%] [G loss: 2.289425]\n",
      "862 [D loss: 1.055140, acc.: 50.00%] [G loss: 2.158103]\n",
      "863 [D loss: 1.094352, acc.: 50.00%] [G loss: 2.380519]\n",
      "864 [D loss: 1.103298, acc.: 50.00%] [G loss: 2.293196]\n",
      "865 [D loss: 1.038833, acc.: 50.00%] [G loss: 2.506529]\n",
      "866 [D loss: 1.052732, acc.: 50.00%] [G loss: 4.208610]\n",
      "867 [D loss: 1.088143, acc.: 50.00%] [G loss: 2.139205]\n",
      "868 [D loss: 1.092836, acc.: 50.00%] [G loss: 2.069408]\n",
      "869 [D loss: 1.029204, acc.: 50.00%] [G loss: 2.127356]\n",
      "870 [D loss: 1.141824, acc.: 50.00%] [G loss: 2.456532]\n",
      "871 [D loss: 1.049741, acc.: 50.00%] [G loss: 2.291221]\n",
      "872 [D loss: 1.044621, acc.: 50.00%] [G loss: 2.838800]\n",
      "873 [D loss: 1.062398, acc.: 50.00%] [G loss: 2.122007]\n",
      "874 [D loss: 1.056186, acc.: 50.00%] [G loss: 2.505270]\n",
      "875 [D loss: 1.126928, acc.: 50.00%] [G loss: 2.143328]\n",
      "876 [D loss: 0.999896, acc.: 50.00%] [G loss: 2.153206]\n",
      "877 [D loss: 1.076235, acc.: 50.00%] [G loss: 2.092083]\n",
      "878 [D loss: 1.056533, acc.: 50.00%] [G loss: 2.310628]\n",
      "879 [D loss: 1.097088, acc.: 50.00%] [G loss: 2.105268]\n",
      "880 [D loss: 1.056207, acc.: 50.00%] [G loss: 2.965781]\n",
      "881 [D loss: 1.057476, acc.: 50.00%] [G loss: 2.033465]\n",
      "882 [D loss: 1.122784, acc.: 50.00%] [G loss: 2.026890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "883 [D loss: 0.977281, acc.: 50.00%] [G loss: 2.169137]\n",
      "884 [D loss: 1.025962, acc.: 50.00%] [G loss: 1.936904]\n",
      "885 [D loss: 1.063762, acc.: 50.00%] [G loss: 2.125591]\n",
      "886 [D loss: 1.095403, acc.: 50.00%] [G loss: 2.795900]\n",
      "887 [D loss: 1.065552, acc.: 50.00%] [G loss: 2.138342]\n",
      "888 [D loss: 1.075401, acc.: 50.00%] [G loss: 2.146857]\n",
      "889 [D loss: 1.044415, acc.: 50.00%] [G loss: 2.188331]\n",
      "890 [D loss: 1.089026, acc.: 50.00%] [G loss: 2.172479]\n",
      "891 [D loss: 1.105425, acc.: 50.00%] [G loss: 2.096358]\n",
      "892 [D loss: 1.074037, acc.: 50.00%] [G loss: 2.238744]\n",
      "893 [D loss: 1.021742, acc.: 50.00%] [G loss: 1.925767]\n",
      "894 [D loss: 1.069873, acc.: 50.00%] [G loss: 2.646398]\n",
      "895 [D loss: 1.045286, acc.: 50.00%] [G loss: 2.431042]\n",
      "896 [D loss: 1.063193, acc.: 50.00%] [G loss: 2.946819]\n",
      "897 [D loss: 1.024373, acc.: 50.00%] [G loss: 2.920117]\n",
      "898 [D loss: 1.062854, acc.: 50.00%] [G loss: 2.238012]\n",
      "899 [D loss: 1.076182, acc.: 50.00%] [G loss: 2.281085]\n",
      "900 [D loss: 1.043080, acc.: 50.00%] [G loss: 1.989383]\n",
      "901 [D loss: 0.984223, acc.: 50.00%] [G loss: 2.191069]\n",
      "902 [D loss: 1.044950, acc.: 50.00%] [G loss: 2.646138]\n",
      "903 [D loss: 1.032275, acc.: 50.00%] [G loss: 1.984492]\n",
      "904 [D loss: 0.994795, acc.: 50.00%] [G loss: 3.001188]\n",
      "905 [D loss: 1.077353, acc.: 50.00%] [G loss: 2.201772]\n",
      "906 [D loss: 1.085967, acc.: 50.00%] [G loss: 2.052317]\n",
      "907 [D loss: 1.011558, acc.: 50.00%] [G loss: 2.324960]\n",
      "908 [D loss: 1.042245, acc.: 50.00%] [G loss: 2.818538]\n",
      "909 [D loss: 1.011457, acc.: 50.00%] [G loss: 2.097116]\n",
      "910 [D loss: 1.071639, acc.: 50.00%] [G loss: 2.002598]\n",
      "911 [D loss: 1.045810, acc.: 50.00%] [G loss: 2.619475]\n",
      "912 [D loss: 1.036875, acc.: 50.00%] [G loss: 2.268492]\n",
      "913 [D loss: 1.054690, acc.: 50.00%] [G loss: 2.117062]\n",
      "914 [D loss: 1.086591, acc.: 50.00%] [G loss: 2.903188]\n",
      "915 [D loss: 1.031364, acc.: 50.00%] [G loss: 2.364745]\n",
      "916 [D loss: 1.040864, acc.: 50.00%] [G loss: 2.128413]\n",
      "917 [D loss: 1.046393, acc.: 50.00%] [G loss: 2.284041]\n",
      "918 [D loss: 1.109885, acc.: 50.00%] [G loss: 2.155140]\n",
      "919 [D loss: 1.085760, acc.: 50.00%] [G loss: 2.150393]\n",
      "920 [D loss: 1.050565, acc.: 50.00%] [G loss: 2.665886]\n",
      "921 [D loss: 1.020539, acc.: 50.00%] [G loss: 2.242321]\n",
      "922 [D loss: 1.119010, acc.: 50.00%] [G loss: 2.098828]\n",
      "923 [D loss: 1.030593, acc.: 50.00%] [G loss: 3.092118]\n",
      "924 [D loss: 1.070883, acc.: 50.00%] [G loss: 2.090458]\n",
      "925 [D loss: 1.055775, acc.: 50.00%] [G loss: 2.799127]\n",
      "926 [D loss: 1.004709, acc.: 50.00%] [G loss: 2.056637]\n",
      "927 [D loss: 1.056290, acc.: 50.00%] [G loss: 2.083543]\n",
      "928 [D loss: 1.052512, acc.: 50.00%] [G loss: 3.004454]\n",
      "929 [D loss: 1.001929, acc.: 50.00%] [G loss: 1.994840]\n",
      "930 [D loss: 1.089829, acc.: 50.00%] [G loss: 2.174371]\n",
      "931 [D loss: 1.051295, acc.: 50.00%] [G loss: 2.139369]\n",
      "932 [D loss: 1.035853, acc.: 50.00%] [G loss: 2.157658]\n",
      "933 [D loss: 1.028517, acc.: 50.00%] [G loss: 2.064493]\n",
      "934 [D loss: 1.017040, acc.: 50.00%] [G loss: 1.978883]\n",
      "935 [D loss: 1.068585, acc.: 50.00%] [G loss: 1.980445]\n",
      "936 [D loss: 1.058711, acc.: 50.00%] [G loss: 2.147478]\n",
      "937 [D loss: 1.002396, acc.: 50.00%] [G loss: 2.181312]\n",
      "938 [D loss: 1.085221, acc.: 50.00%] [G loss: 2.751460]\n",
      "939 [D loss: 1.049633, acc.: 50.00%] [G loss: 2.127590]\n",
      "940 [D loss: 1.050553, acc.: 50.00%] [G loss: 2.136688]\n",
      "941 [D loss: 1.037030, acc.: 50.00%] [G loss: 2.889678]\n",
      "942 [D loss: 1.035427, acc.: 50.00%] [G loss: 2.205924]\n",
      "943 [D loss: 1.013691, acc.: 50.00%] [G loss: 2.131812]\n",
      "944 [D loss: 1.040427, acc.: 50.00%] [G loss: 2.289223]\n",
      "945 [D loss: 0.989549, acc.: 50.00%] [G loss: 2.120557]\n",
      "946 [D loss: 1.070863, acc.: 50.00%] [G loss: 2.137333]\n",
      "947 [D loss: 1.032431, acc.: 50.00%] [G loss: 2.017141]\n",
      "948 [D loss: 0.994557, acc.: 50.00%] [G loss: 2.704664]\n",
      "949 [D loss: 1.003858, acc.: 50.00%] [G loss: 1.912195]\n",
      "950 [D loss: 1.017597, acc.: 50.00%] [G loss: 2.268586]\n",
      "951 [D loss: 1.079786, acc.: 50.00%] [G loss: 3.325780]\n",
      "952 [D loss: 1.031048, acc.: 50.00%] [G loss: 2.175602]\n",
      "953 [D loss: 1.034620, acc.: 50.00%] [G loss: 3.082021]\n",
      "954 [D loss: 1.006250, acc.: 50.00%] [G loss: 2.234742]\n",
      "955 [D loss: 1.008983, acc.: 50.00%] [G loss: 2.261299]\n",
      "956 [D loss: 1.046968, acc.: 50.00%] [G loss: 2.783494]\n",
      "957 [D loss: 1.010096, acc.: 50.00%] [G loss: 2.119924]\n",
      "958 [D loss: 1.062807, acc.: 50.00%] [G loss: 2.087223]\n",
      "959 [D loss: 1.042401, acc.: 50.00%] [G loss: 2.202877]\n",
      "960 [D loss: 1.015792, acc.: 50.00%] [G loss: 2.206183]\n",
      "961 [D loss: 0.988167, acc.: 50.00%] [G loss: 2.063990]\n",
      "962 [D loss: 0.995154, acc.: 50.00%] [G loss: 2.051416]\n",
      "963 [D loss: 1.090528, acc.: 50.00%] [G loss: 1.962806]\n",
      "964 [D loss: 1.004312, acc.: 50.00%] [G loss: 2.145549]\n",
      "965 [D loss: 1.036216, acc.: 50.00%] [G loss: 2.052587]\n",
      "966 [D loss: 1.006185, acc.: 50.00%] [G loss: 2.190684]\n",
      "967 [D loss: 1.004338, acc.: 50.00%] [G loss: 3.015062]\n",
      "968 [D loss: 1.018189, acc.: 50.00%] [G loss: 2.227565]\n",
      "969 [D loss: 1.000815, acc.: 50.00%] [G loss: 2.046168]\n",
      "970 [D loss: 1.010827, acc.: 50.00%] [G loss: 2.560058]\n",
      "971 [D loss: 1.070073, acc.: 50.00%] [G loss: 2.913136]\n",
      "972 [D loss: 1.086191, acc.: 50.00%] [G loss: 2.113199]\n",
      "973 [D loss: 1.028357, acc.: 50.00%] [G loss: 2.068299]\n",
      "974 [D loss: 1.064464, acc.: 50.00%] [G loss: 2.185893]\n",
      "975 [D loss: 1.044866, acc.: 50.00%] [G loss: 2.229239]\n",
      "976 [D loss: 0.981545, acc.: 50.00%] [G loss: 2.149157]\n",
      "977 [D loss: 1.100082, acc.: 50.00%] [G loss: 1.940783]\n",
      "978 [D loss: 0.987439, acc.: 50.00%] [G loss: 2.037513]\n",
      "979 [D loss: 0.988436, acc.: 50.00%] [G loss: 2.099567]\n",
      "980 [D loss: 1.048544, acc.: 50.00%] [G loss: 2.112371]\n",
      "981 [D loss: 0.955660, acc.: 50.00%] [G loss: 2.079085]\n",
      "982 [D loss: 1.048785, acc.: 50.00%] [G loss: 2.089644]\n",
      "983 [D loss: 1.069919, acc.: 50.00%] [G loss: 2.031606]\n",
      "984 [D loss: 0.987559, acc.: 50.00%] [G loss: 2.179798]\n",
      "985 [D loss: 1.027694, acc.: 50.00%] [G loss: 3.035391]\n",
      "986 [D loss: 0.992613, acc.: 50.00%] [G loss: 2.064053]\n",
      "987 [D loss: 1.007128, acc.: 50.00%] [G loss: 1.938179]\n",
      "988 [D loss: 1.048256, acc.: 50.00%] [G loss: 2.130280]\n",
      "989 [D loss: 1.006309, acc.: 50.00%] [G loss: 2.007670]\n",
      "990 [D loss: 1.041922, acc.: 50.00%] [G loss: 1.896266]\n",
      "991 [D loss: 1.065846, acc.: 50.00%] [G loss: 2.015621]\n",
      "992 [D loss: 0.982373, acc.: 50.00%] [G loss: 2.216510]\n",
      "993 [D loss: 0.963253, acc.: 50.00%] [G loss: 2.052907]\n",
      "994 [D loss: 1.044610, acc.: 50.00%] [G loss: 2.131771]\n",
      "995 [D loss: 1.010425, acc.: 50.00%] [G loss: 2.097717]\n",
      "996 [D loss: 1.031363, acc.: 50.00%] [G loss: 1.981796]\n",
      "997 [D loss: 1.045143, acc.: 50.00%] [G loss: 2.120874]\n",
      "998 [D loss: 1.092320, acc.: 50.00%] [G loss: 2.192614]\n",
      "999 [D loss: 1.023707, acc.: 50.00%] [G loss: 2.035998]\n"
     ]
    }
   ],
   "source": [
    "lstmgan.train(epochs=1000, batch_size=20, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0h 9m 33s\n"
     ]
    }
   ],
   "source": [
    "end = datetime.datetime.now()\n",
    "\n",
    "diff = (end - start)\n",
    "\n",
    "datetime.timedelta(seconds=10, microseconds=885206)\n",
    "\n",
    "diff_seconds = int(diff.total_seconds())\n",
    "\n",
    "minute_seconds, seconds = divmod(diff_seconds, 60)\n",
    "hours, minutes = divmod(minute_seconds, 60)\n",
    "hms = f\"{hours}h {minutes}m {seconds}s\"\n",
    "\n",
    "'0h 0m 10s'\n",
    "print(hms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.28932735,  0.28090334, -0.09896404,  0.16789201,  0.16083069,\n",
      "         0.07909998, -0.19358641, -0.04603494, -0.07185977,  0.01948999,\n",
      "        -0.29909247,  0.07616757, -0.28480074,  0.08794321,  0.2755247 ,\n",
      "        -0.17734568,  0.22789575, -0.19770132,  0.262634  ,  0.25595284,\n",
      "        -0.13701852,  0.06268808,  0.07455632, -0.08462346, -0.02683613,\n",
      "        -0.00334612,  0.01868126, -0.2605231 , -0.15917125, -0.17929563,\n",
      "        -0.21470726,  0.03023739, -0.30187008,  0.05839606,  0.11221778,\n",
      "        -0.27679223, -0.31245184, -0.06610083, -0.11990946, -0.20296225,\n",
      "        -0.2729845 ,  0.27078605,  0.18337841, -0.2591905 ,  0.10125928,\n",
      "         0.04193975,  0.03252538,  0.13200958, -0.2139449 , -0.2826586 ,\n",
      "        -0.14157157,  0.08539122,  0.15724719, -0.02581102, -0.08951039,\n",
      "         0.17226134, -0.08288154, -0.16760391, -0.24035272, -0.17347221,\n",
      "        -0.25561577,  0.02474762, -0.28838316, -0.12878221]],\n",
      "      dtype=float32), array([[[-0.1330465 , -0.08079099, -0.00032843, ...,  0.04362429,\n",
      "         -0.10778368,  0.12914824],\n",
      "        [ 0.11018768, -0.08164287, -0.01652144, ..., -0.11551268,\n",
      "         -0.08559137, -0.06221715],\n",
      "        [-0.0610567 , -0.12162681, -0.06450877, ..., -0.03308174,\n",
      "         -0.01658802,  0.05887324],\n",
      "        ...,\n",
      "        [-0.04900126, -0.00693681,  0.12545186, ...,  0.05869414,\n",
      "         -0.04210733,  0.0753905 ],\n",
      "        [ 0.04099267,  0.14761557,  0.12592037, ...,  0.04489933,\n",
      "         -0.02801718,  0.02790884],\n",
      "        [ 0.05257222,  0.05059908,  0.08666064, ..., -0.12096694,\n",
      "          0.09014151, -0.11792184]],\n",
      "\n",
      "       [[ 0.05929812,  0.1198459 ,  0.09445257, ...,  0.00626025,\n",
      "          0.07631534, -0.13373892],\n",
      "        [-0.01459699,  0.03364163,  0.01330962, ...,  0.07555395,\n",
      "          0.09101643, -0.07785919],\n",
      "        [-0.01416772, -0.00414911, -0.09397264, ...,  0.08667968,\n",
      "          0.05091357, -0.10969039],\n",
      "        ...,\n",
      "        [-0.12802525, -0.09206774,  0.02320483, ..., -0.08385234,\n",
      "         -0.15649122, -0.01177629],\n",
      "        [-0.05839926,  0.09714136,  0.14677921, ..., -0.05065503,\n",
      "         -0.07141458, -0.04384602],\n",
      "        [ 0.03570078, -0.02409237,  0.11332435, ..., -0.00067279,\n",
      "          0.08510236, -0.138048  ]],\n",
      "\n",
      "       [[ 0.14685322, -0.14309716, -0.04791403, ...,  0.03522711,\n",
      "          0.00237468, -0.07156619],\n",
      "        [ 0.09446183, -0.063513  ,  0.1506765 , ...,  0.04029454,\n",
      "          0.00176355, -0.11768882],\n",
      "        [ 0.03305161,  0.04060406,  0.02944739, ..., -0.06363401,\n",
      "         -0.13128692, -0.10339496],\n",
      "        ...,\n",
      "        [-0.04029569, -0.01745413,  0.0524337 , ..., -0.07907122,\n",
      "         -0.14364742, -0.020737  ],\n",
      "        [ 0.07415076,  0.07138083,  0.09946745, ..., -0.07091524,\n",
      "         -0.06530815, -0.09944765],\n",
      "        [-0.09289624, -0.00463439,  0.12214943, ...,  0.13140626,\n",
      "          0.02727338, -0.1165742 ]]], dtype=float32), array([[[-0.21088955],\n",
      "        [ 0.02880092],\n",
      "        [-0.19686675],\n",
      "        [-0.16817404],\n",
      "        [ 0.01376955],\n",
      "        [ 0.09364121],\n",
      "        [-0.14796814],\n",
      "        [ 0.02810215],\n",
      "        [-0.03435069],\n",
      "        [-0.03795574],\n",
      "        [-0.03500964],\n",
      "        [-0.11741695],\n",
      "        [-0.06514349],\n",
      "        [-0.12805264],\n",
      "        [ 0.23432098],\n",
      "        [ 0.14210652],\n",
      "        [ 0.06464321],\n",
      "        [ 0.18186918],\n",
      "        [ 0.28684312],\n",
      "        [-0.16913456],\n",
      "        [ 0.17872873],\n",
      "        [ 0.21049775],\n",
      "        [ 0.19129798],\n",
      "        [ 0.13079305],\n",
      "        [-0.13401437],\n",
      "        [ 0.2015885 ],\n",
      "        [-0.19055013],\n",
      "        [ 0.11040591],\n",
      "        [ 0.0768139 ],\n",
      "        [-0.16806738],\n",
      "        [ 0.17481928],\n",
      "        [-0.08046494]],\n",
      "\n",
      "       [[-0.00716189],\n",
      "        [ 0.11531032],\n",
      "        [-0.11169854],\n",
      "        [-0.23828292],\n",
      "        [ 0.06700575],\n",
      "        [ 0.05005173],\n",
      "        [-0.06973555],\n",
      "        [-0.13593726],\n",
      "        [ 0.03991525],\n",
      "        [ 0.15806147],\n",
      "        [-0.04813862],\n",
      "        [ 0.2274036 ],\n",
      "        [ 0.2005087 ],\n",
      "        [ 0.05767255],\n",
      "        [-0.24334455],\n",
      "        [ 0.23991753],\n",
      "        [-0.15443207],\n",
      "        [ 0.03142916],\n",
      "        [ 0.17826554],\n",
      "        [-0.15539528],\n",
      "        [-0.14565597],\n",
      "        [-0.17882475],\n",
      "        [-0.20166065],\n",
      "        [ 0.06662583],\n",
      "        [-0.1810362 ],\n",
      "        [-0.13972688],\n",
      "        [ 0.19017045],\n",
      "        [ 0.09428185],\n",
      "        [-0.21085274],\n",
      "        [ 0.06882668],\n",
      "        [ 0.04365404],\n",
      "        [-0.17274103]],\n",
      "\n",
      "       [[ 0.06919218],\n",
      "        [ 0.02072838],\n",
      "        [ 0.08563159],\n",
      "        [-0.03515689],\n",
      "        [-0.00113652],\n",
      "        [ 0.10012671],\n",
      "        [ 0.20898959],\n",
      "        [-0.02775055],\n",
      "        [-0.23153946],\n",
      "        [-0.05664826],\n",
      "        [-0.1142848 ],\n",
      "        [ 0.10255289],\n",
      "        [-0.10469992],\n",
      "        [ 0.12699893],\n",
      "        [-0.12201745],\n",
      "        [ 0.02375696],\n",
      "        [ 0.2586507 ],\n",
      "        [ 0.22653845],\n",
      "        [ 0.09595982],\n",
      "        [-0.2329175 ],\n",
      "        [-0.2317259 ],\n",
      "        [ 0.06488381],\n",
      "        [-0.17662823],\n",
      "        [-0.06431955],\n",
      "        [-0.13417242],\n",
      "        [ 0.15122466],\n",
      "        [ 0.20663182],\n",
      "        [-0.10882977],\n",
      "        [ 0.24621804],\n",
      "        [ 0.20119986],\n",
      "        [ 0.2281196 ],\n",
      "        [-0.16396135]]], dtype=float32), array([[-0.18397932,  0.24878696,  0.3110953 , -0.05374182,  0.268722  ,\n",
      "        -0.0759892 , -0.20804857, -0.0177834 , -0.09414089, -0.04763618,\n",
      "         0.1677055 ,  0.12941597,  0.01654832,  0.34238064,  0.30945626,\n",
      "        -0.12900308, -0.25598437,  0.21996382,  0.05594296, -0.0820078 ,\n",
      "         0.18318659, -0.1113575 , -0.31342658, -0.15674017, -0.26363763,\n",
      "        -0.1764438 , -0.08009389, -0.14360063, -0.07933606,  0.18188082,\n",
      "        -0.06818994, -0.3725321 ],\n",
      "       [ 0.14718215, -0.15463813,  0.37516287,  0.34770748,  0.17391309,\n",
      "         0.10119957,  0.33346787, -0.03966746,  0.05175234,  0.17935996,\n",
      "        -0.33548185,  0.32427907, -0.2086878 ,  0.12348789, -0.10174271,\n",
      "         0.25137633, -0.17404798,  0.37496108, -0.15229732,  0.2520487 ,\n",
      "        -0.1873486 ,  0.09394758, -0.08706278,  0.28725755,  0.3149665 ,\n",
      "         0.19965066, -0.21448825,  0.1737608 ,  0.25790223, -0.20785245,\n",
      "        -0.00557927, -0.11803526],\n",
      "       [ 0.32540786, -0.06570974,  0.36695802,  0.03582711,  0.04231821,\n",
      "        -0.04064906,  0.27310455,  0.09429727,  0.18513529, -0.21548401,\n",
      "        -0.17735435, -0.17560828,  0.09172232, -0.18224834,  0.05130807,\n",
      "         0.2533013 ,  0.37018764, -0.09391726, -0.20660949, -0.3511796 ,\n",
      "        -0.21969019, -0.12350206,  0.12167993, -0.2022965 ,  0.02362065,\n",
      "        -0.00105689, -0.16812748,  0.24573545,  0.18349323, -0.2731704 ,\n",
      "        -0.17758358, -0.1043085 ],\n",
      "       [-0.3167813 ,  0.33234203,  0.32118273, -0.02972404,  0.0788456 ,\n",
      "         0.2354303 ,  0.2958094 ,  0.02748619, -0.2410598 , -0.1471749 ,\n",
      "         0.2135029 , -0.2187691 ,  0.39415503, -0.3434695 , -0.24012579,\n",
      "        -0.1921694 ,  0.15059018,  0.22546236,  0.07156937, -0.04933836,\n",
      "         0.04953796,  0.3479157 ,  0.11473385,  0.17779222,  0.25541827,\n",
      "         0.39336616,  0.23531024, -0.19856052, -0.21326637,  0.2615776 ,\n",
      "         0.04230289, -0.3710736 ],\n",
      "       [-0.28150675,  0.34700492,  0.1256527 ,  0.19155003, -0.2764714 ,\n",
      "         0.14354134,  0.22668688,  0.29385394,  0.02506493,  0.36778235,\n",
      "        -0.15639204,  0.2854891 ,  0.06250064,  0.25910816,  0.29083902,\n",
      "         0.18526432,  0.21299855,  0.29957163,  0.16617388,  0.05900055,\n",
      "        -0.19664834,  0.02667067,  0.2903063 , -0.3982824 , -0.33902633,\n",
      "        -0.08461905,  0.06729427,  0.0728276 ,  0.23497985, -0.3035015 ,\n",
      "        -0.28298405,  0.3428656 ],\n",
      "       [ 0.16761248,  0.1336504 , -0.19138744,  0.17826235,  0.24421445,\n",
      "        -0.03923221, -0.09498989,  0.3681953 ,  0.06661162, -0.02543592,\n",
      "        -0.02462907, -0.20564051,  0.04164732, -0.1588097 ,  0.21340641,\n",
      "         0.0583133 ,  0.1507074 ,  0.20790237, -0.35850903, -0.093016  ,\n",
      "        -0.13086443, -0.32831365,  0.19054063, -0.10813486, -0.15863082,\n",
      "        -0.00242675,  0.06820141,  0.12207149, -0.28478783,  0.04528788,\n",
      "        -0.11758931,  0.25958508],\n",
      "       [-0.39097887, -0.20347661,  0.09294032, -0.04849842,  0.12855247,\n",
      "         0.42167896,  0.23354818, -0.05779221, -0.13824362, -0.38261747,\n",
      "        -0.00802893,  0.27861637,  0.02068211, -0.00955025,  0.17182472,\n",
      "         0.25050992, -0.10046301, -0.20290592,  0.21846706,  0.22309768,\n",
      "        -0.29509383, -0.11735265,  0.18785875, -0.04170369, -0.03929986,\n",
      "         0.19649091,  0.2744213 ,  0.17920132,  0.02151409, -0.12771323,\n",
      "         0.20500569, -0.18265913],\n",
      "       [-0.15067227,  0.20613162,  0.22936736, -0.12358534, -0.20675136,\n",
      "         0.2558223 ,  0.12829046, -0.3169297 ,  0.16514158,  0.07241577,\n",
      "         0.30991325, -0.01007331,  0.32439196, -0.24635091,  0.36713737,\n",
      "        -0.3769194 ,  0.13238533, -0.09094172,  0.31954637, -0.31837767,\n",
      "        -0.20042504,  0.02801684, -0.03938809,  0.1152317 , -0.29520017,\n",
      "        -0.1474149 ,  0.25078702,  0.02196916,  0.14680763,  0.12785581,\n",
      "         0.06700134,  0.01140322]], dtype=float32), array([-0.00315025,  0.04991024,  0.04428628,  0.0078661 ,  0.00930536,\n",
      "        0.07190496,  0.01582403,  0.02470231,  0.02672389, -0.00176056,\n",
      "        0.02089109,  0.00844338, -0.00489984, -0.0020003 , -0.00361508,\n",
      "       -0.00249681,  0.03048262,  0.06702337, -0.00256735, -0.00141336,\n",
      "        0.00691138,  0.0573753 , -0.00192136, -0.00225794, -0.00250164,\n",
      "        0.00767919, -0.00417044, -0.00260658, -0.00348007,  0.06327933,\n",
      "       -0.00161443, -0.00150765], dtype=float32), array([[-0.27840155],\n",
      "       [ 0.47595733],\n",
      "       [ 0.28287193],\n",
      "       [ 0.4232638 ],\n",
      "       [ 0.04187113],\n",
      "       [ 0.20493616],\n",
      "       [ 0.4135216 ],\n",
      "       [ 0.4554017 ],\n",
      "       [ 0.50709224],\n",
      "       [-0.19216356],\n",
      "       [ 0.23502341],\n",
      "       [ 0.14125842],\n",
      "       [-0.17984751],\n",
      "       [-0.35673973],\n",
      "       [-0.16240542],\n",
      "       [-0.14829002],\n",
      "       [ 0.21009046],\n",
      "       [ 0.18415347],\n",
      "       [-0.06354276],\n",
      "       [-0.24012941],\n",
      "       [ 0.35615242],\n",
      "       [ 0.3023209 ],\n",
      "       [-0.16157655],\n",
      "       [-0.08751324],\n",
      "       [-0.38923246],\n",
      "       [ 0.25236532],\n",
      "       [-0.26886594],\n",
      "       [-0.29195824],\n",
      "       [-0.3334846 ],\n",
      "       [ 0.1914807 ],\n",
      "       [-0.43012184],\n",
      "       [-0.26182595]], dtype=float32), array([0.01029321], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(lstmgan.discriminator.layers[1].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[0.        , 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.0003    ]],\n",
      "\n",
      "       [[0.        , 0.        , 0.05892772, ..., 0.        ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.10332245, 0.        , ..., 0.        ,\n",
      "         0.        , 0.07849346]],\n",
      "\n",
      "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.04008254]]], dtype=float32)]\n",
      "(1, 68424, 1, 30)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "hidden_layers = keras.backend.function(\n",
    "[lstmgan.discriminator.layers[0].input],  # we will feed the function with the input of the first layer  \n",
    "[lstmgan.discriminator.layers[0].output,] # we want to get the output of the first layer\n",
    ")\n",
    "h=hidden_layers([df_train])\n",
    "print(h)\n",
    "print(np.shape(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=np.array(h)  \n",
    "\n",
    "arr_reshaped = np.array(h).reshape(68424, 30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68424, 30)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(arr_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('normal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.         0.         0.05892772 0.         0.         0.\n",
      "   0.01444091 0.         0.         0.         0.         0.07184511\n",
      "   0.         0.         0.         0.         0.         0.06395902\n",
      "   0.         0.         0.         0.         0.09989215 0.\n",
      "   0.         0.         0.         0.         0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "#To check 1st row\n",
    "print(h[:,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_1 = pd.read_csv('normal.csv')\n",
    "csv_2 = pd.read_csv('normallabel.csv')\n",
    "\n",
    "result = pd.concat([csv_1, csv_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"lgcnormalresult.csv\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x28a4cd67648>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['gen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x28a4ce47fc8>,\n",
       " <matplotlib.lines.Line2D at 0x28a4ce501c8>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['dis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature22</th>\n",
       "      <th>feature23</th>\n",
       "      <th>feature24</th>\n",
       "      <th>feature25</th>\n",
       "      <th>feature26</th>\n",
       "      <th>feature27</th>\n",
       "      <th>feature28</th>\n",
       "      <th>feature29</th>\n",
       "      <th>feature30</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006962</td>\n",
       "      <td>0.055625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048416</td>\n",
       "      <td>0.014631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013936</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051723</td>\n",
       "      <td>0.013089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0       0.0  0.006962  0.055625       0.0       0.0       0.0  0.036116   \n",
       "1       0.0  0.052188  0.000000       0.0       0.0       0.0  0.047845   \n",
       "2       0.0  0.048416  0.014631       0.0       0.0       0.0  0.045899   \n",
       "3       0.0  0.000000  0.038806       0.0       0.0       0.0  0.000000   \n",
       "4       0.0  0.051723  0.013089       0.0       0.0       0.0  0.047847   \n",
       "\n",
       "   feature8  feature9  feature10  ...  feature22  feature23  feature24  \\\n",
       "0       0.0       0.0        0.0  ...        0.0   0.000000   0.000318   \n",
       "1       0.0       0.0        0.0  ...        0.0   0.000000   0.000000   \n",
       "2       0.0       0.0        0.0  ...        0.0   0.000000   0.000000   \n",
       "3       0.0       0.0        0.0  ...        0.0   0.027111   0.000000   \n",
       "4       0.0       0.0        0.0  ...        0.0   0.000000   0.000000   \n",
       "\n",
       "   feature25  feature26  feature27  feature28  feature29  feature30  label  \n",
       "0        0.0        0.0        0.0   0.021632        0.0   0.000000      0  \n",
       "1        0.0        0.0        0.0   0.057461        0.0   0.000000      0  \n",
       "2        0.0        0.0        0.0   0.059599        0.0   0.000000      0  \n",
       "3        0.0        0.0        0.0   0.044531        0.0   0.013936      0  \n",
       "4        0.0        0.0        0.0   0.060423        0.0   0.000000      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test=df1.label\n",
    "\n",
    "x_test=df1.drop('label',axis=1).values.reshape(-1,30,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.15730402]]\n",
      "\n",
      " [[0.16039658]]\n",
      "\n",
      " [[0.16107297]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.15706038]]\n",
      "\n",
      " [[0.15703748]]\n",
      "\n",
      " [[0.15700035]]]\n"
     ]
    }
   ],
   "source": [
    "#y_pred=lstmgan.generator.predict(x_test)\n",
    "y_pred=lstmgan.discriminator.predict(x_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "yhat_probs = lstmgan.discriminator.predict(x_test, verbose=0)\n",
    "yhat_classes=np.argmax(yhat_probs,axis=1)\n",
    "# predict crisp classes for test set\n",
    "#yhat_classes = model.predict_classes(x_val, verbose=0)\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "#yhat_classes = yhat_classes[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-0ee39934b722>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# We set the threshold equal to the training loss of the autoencoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtesting_set_predictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlstmgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "# Helper function that calculates the reconstruction loss of each data sample\n",
    "def calculate_losses(x,preds):\n",
    "    losses=np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        losses[i]=((preds[i] - x[i]) ** 2).mean(axis=None)\n",
    "        \n",
    "    return losses\n",
    "\n",
    "# We set the threshold equal to the training loss of the autoencoder\n",
    "threshold=history.history[\"loss\"][-1]\n",
    "\n",
    "testing_set_predictions=lstmgan.discriminator.predict(x_test)\n",
    "test_losses=calculate_losses(x_test,testing_set_predictions)\n",
    "testing_set_predictions=np.zeros(len(test_losses))\n",
    "testing_set_predictions[np.where(test_losses>threshold)]=1\n",
    "\n",
    "\n",
    "error_df = pd.DataFrame({'Reconstruction_error': testing_set_predictions, 'True_class': y_test})\n",
    "error_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,precision_score,f1_score\n",
    "accuracy=accuracy_score(y_val,testing_set_predictions)\n",
    "recall=recall_score(y_val,testing_set_predictions)\n",
    "precision=precision_score(y_val,testing_set_predictions)\n",
    "f1=f1_score(y_val,testing_set_predictions)\n",
    "print(\"Performance over the testing data set \\n\")\n",
    "print(\"Accuracy : {} , Recall : {} , Precision : {} , F1 : {}\\n\".format(accuracy,recall,precision,f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/#:~:text=Optimal%20Threshold%20for%20Precision%2DRecall%20Curve,-Unlike%20the%20ROC&text=Recall%20is%20calculated%20as%20the,positives%20and%20the%20false%20negatives.\n",
    "#Optimal Threshold Tuning\n",
    "# search thresholds for imbalanced classification\n",
    "from numpy import arange\n",
    "from numpy import argmax\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')\n",
    " \n",
    "\n",
    "# predict probabilities\n",
    "#yhat = model.predict_proba(x_val)\n",
    "# keep probabilities for the positive outcome only\n",
    "#probs = yhat[:, 1]\n",
    "# define thresholds\n",
    "thresholds = arange(0, 1, 0.001)\n",
    "# evaluate each threshold\n",
    "scores = [f1_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each threshold\n",
    "scores = [accuracy_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, Accuracy-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each threshold\n",
    "scores = [precision_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, Precision-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each threshold\n",
    "scores = [recall_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, recall-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# ROC AUC\n",
    "auc = roc_auc_score(y_test, yhat_probs)\n",
    "print('ROC AUC: %f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=8\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(df_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "#x_test = keras.preprocessing.sequence.pad_sequences(x_test , maxlen=maxlen)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "#https://www.kaggle.com/suicaokhoailang/10-fold-lstm-with-attention-0-991-lb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import sklearn.metrics as sk\n",
    "from sklearn.metrics import roc_curve,f1_score\n",
    "from sklearn.metrics import auc\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=42069)\n",
    "preds = []\n",
    "fold = 0\n",
    "aucs = 0\n",
    "for train_idx, val_idx in kf.split(x_train):\n",
    "    x_train_f = x_train[train_idx]\n",
    "    y_train_f = y_test[train_idx]\n",
    "    x_val_f = x_train[val_idx]\n",
    "    y_val_f = y_test[val_idx]\n",
    "    lstmgan.discriminator.compile(optimizer, \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    #model = get_model()\n",
    "    lstmgan.discriminator.fit(x_train_f, y_train_f,\n",
    "              batch_size=256,\n",
    "              epochs=10,\n",
    "              verbose = 1,\n",
    "              validation_data=(x_val_f, y_val_f))\n",
    "\n",
    "    # Get accuracy of model on validation data. It's not AUC but it's something at least!\n",
    "    preds_val = lstmgan.discriminator.predict([x_val_f], batch_size=512)\n",
    "    preds.append(lstmgan.discriminator.predict(x_test))\n",
    "    fold+=1\n",
    "    fpr, tpr, thresholds = roc_curve(y_val_f, preds_val, pos_label=1)\n",
    "    # calculate scores\n",
    "    #lr_f1, lr_auc = f1_score(testy, yhat), auc(lr_recall, lr_precision)\n",
    "    aucs += auc(fpr,tpr)\n",
    "    print('Fold {}, AUC = {}'.format(fold,auc(fpr, tpr)))\n",
    "print(\"Cross Validation AUC = {}\".format(aucs/10))\n",
    "#print(sk.confusion_matrix(y_val_f,preds_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, accuracy_score ,recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#range of output value\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(range(len(y_pred)),y_pred.reshape(-1,1))\n",
    "plt.ylabel('y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for the optimal probability threshold\n",
    "#prob=[0.5,0.55,0.6,0.65,0.75,0.8,0.85,0.9,0.95,1,2,2.5,5]\n",
    "prob=[-0.2,-0.1,-0.05,0,0.005,0.1,0.157,0.2,0.3,0.5,1,1.5,2]\n",
    "for p in prob:\n",
    "    pred_value =[1 if i<p   else 0 for i in y_pred]\n",
    "    f1=f1_score(y_test,pred_value)\n",
    "    acc=accuracy_score(y_test,pred_value)\n",
    "    precision=average_precision_score(y_test,pred_value)\n",
    "    recall= recall_score(y_test,pred_value)\n",
    "    print(f'prob ={p} and f1 score ={f1} : accuracy={acc} : precision={precision} : recall={recall}')\n",
    "    print(pred_value.count(0),pred_value.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=dict()\n",
    "history['gen']=[]\n",
    "history['dis']=[]\n",
    "def train1(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for batch in dataset:\n",
    "\n",
    "           gen_loss,dis_loss= lstmgan.train(batch)\n",
    "        history['gen'].append(gen_loss)\n",
    "        history['dis'].append(dis_loss)\n",
    "        print ('Time for epoch {} is {} sec '.format(epoch + 1, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = lstmgan.discriminator.evaluate(x_test, y_test)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = lstmgan.generator.predict_classes(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average=\"binary\")\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"binary\")\n",
    "#np.savetxt('results/expected1.txt', y_test, fmt='%01d')\n",
    "#np.savetxt('results/predicted1.txt', y_pred, fmt='%01d')\n",
    "\n",
    "print(\"confusion matrix\")\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"accuracy\")\n",
    "print(\"%.6f\" %accuracy)\n",
    "print(\"racall\")\n",
    "print(\"%.6f\" %recall)\n",
    "print(\"precision\")\n",
    "print(\"%.6f\" %precision)\n",
    "print(\"f1score\")\n",
    "print(\"%.6f\" %f1)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"==============================================\")\n",
    "print (cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
