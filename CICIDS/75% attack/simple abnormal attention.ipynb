{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a707dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0fc698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(\"file3.csv\") \n",
    "csv_1 = pd.read_csv(r\"C:\\Users\\Admin\\CICIDS - 30 neurons\\with attention and dimensionality reduction\\lgcabnormalresult.csv\") \n",
    "  \n",
    "# changing cols with rename() \n",
    "new_data = csv_1.rename(columns = {0: \"Feature1\",\n",
    "                                  1: \"Feature2\", \n",
    "                                  2: \"Feature3\", \n",
    "                                  3: \"Feature4\",\n",
    "                                  4: \"Feature5\",\n",
    "                                  5: \"Feature6\",\n",
    "                                  6: \"Feature7\",\n",
    "                                  7: \"Feature8\",\n",
    "                                  8: \"Feature9\",\n",
    "                                  9: \"Feature10\",\n",
    "                                  10: \"Feature11\",\n",
    "                                  11: \"Feature12\",\n",
    "                                  12: \"Feature13\",\n",
    "                                  13: \"Feature14\",\n",
    "                                  14: \"Feature15\",\n",
    "                                  15: \"Feature16\",\n",
    "                                  16: \"Feature17\",\n",
    "                                  17: \"Feature18\",\n",
    "                                  18: \"Feature19\", \n",
    "                                  19: \"Feature20\",\n",
    "                                  20: \"Feature21\",\n",
    "                                  21: \"Feature22\",\n",
    "                                  22: \"Feature23\",\n",
    "                                  23: \"Feature24\",                                   \n",
    "                                  24: \"Feature25\",\n",
    "                                  25: \"Feature26\",  \n",
    "                                  26: \"Feature27\",\n",
    "                                  27: \"Feature28\",\n",
    "                                  28: \"Feature29\",\n",
    "                                  29: \"Feature30\",                                   \n",
    "                                  30: \"Label\"}) \n",
    "  \n",
    "# changing columns using .columns() \n",
    "csv_1.columns = ['Feature1', 'Feature2', 'Feature3', 'Feature4', \n",
    "                'Feature5', 'Feature6', 'Feature7', 'Feature8',\n",
    "                 'Feature9', 'Feature10', 'Feature11', 'Feature12',\n",
    "                 'Feature13', 'Feature14', 'Feature15', 'Feature16',\n",
    "                 'Feature17', 'Feature18', 'Feature19', 'Feature20','Feature21','Feature22','Feature23','Feature24','Feature25','Feature26','Feature27','Feature28','Feature29','Feature30',\"Label\"] \n",
    "  \n",
    "\n",
    "csv_1.to_csv(\"lgcabnormalresult.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ab5da6",
   "metadata": {},
   "source": [
    "#https://datacarpentry.org/python-ecology-lesson/03-index-slice-subset/index.html\n",
    "csv_1=csv_1[0:100]\n",
    "csv_2=csv_2[0:40000]\n",
    "merged = csv_1.merge(csv_2,on=\"Label\")\n",
    "\n",
    "merged.to_csv(\"attentiongan.csv\", index=False)\n",
    "#result = pd.concat([csv_1, csv_2], axis=1)\n",
    "out = csv_1.append(csv_2)\n",
    "print(out)\n",
    "out.to_csv(\"attentiongan.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a62ea226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "322b1f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"lgcabnormalresult.csv\",sep=\",\", index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9db41934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature1     float64\n",
      "Feature2     float64\n",
      "Feature3     float64\n",
      "Feature4     float64\n",
      "Feature5     float64\n",
      "Feature6     float64\n",
      "Feature7     float64\n",
      "Feature8     float64\n",
      "Feature9     float64\n",
      "Feature10    float64\n",
      "Feature11    float64\n",
      "Feature12    float64\n",
      "Feature13    float64\n",
      "Feature14    float64\n",
      "Feature15    float64\n",
      "Feature16    float64\n",
      "Feature17    float64\n",
      "Feature18    float64\n",
      "Feature19    float64\n",
      "Feature20    float64\n",
      "Feature21    float64\n",
      "Feature22    float64\n",
      "Feature23    float64\n",
      "Feature24    float64\n",
      "Feature25    float64\n",
      "Feature26    float64\n",
      "Feature27    float64\n",
      "Feature28    float64\n",
      "Feature29    float64\n",
      "Feature30    float64\n",
      "Label          int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0331e445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         Feature1  Feature2  Feature3  Feature4  Feature5  Feature6  Feature7  \\\n",
       "0       0.162374  0.266411       0.0  0.000000  0.269083  0.000000  0.149491   \n",
       "1       0.181202  0.278652       0.0  0.000000  0.238064  0.000000  0.139341   \n",
       "2       0.179038  0.256140       0.0  0.000000  0.218407  0.000000  0.180448   \n",
       "3       0.000000  0.000000       0.0  0.000000  0.333200  0.047211  0.057498   \n",
       "4       0.000000  0.163501       0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "556550  0.177695  0.276158       0.0  0.000000  0.237661  0.000000  0.141832   \n",
       "556551  0.000000  0.078991       0.0  0.000000  0.000000  0.000000  0.071014   \n",
       "556552  0.071085  0.155015       0.0  0.078527  0.000000  0.000000  0.000000   \n",
       "556553  0.058132  0.131831       0.0  0.073028  0.000000  0.000000  0.000000   \n",
       "556554  0.071085  0.155015       0.0  0.078527  0.000000  0.000000  0.000000   \n",
       "\n",
       "        Feature8  Feature9  Feature10  ...  Feature22  Feature23  Feature24  \\\n",
       "0       0.079647  0.000000   0.115767  ...   0.000000   0.000000        0.0   \n",
       "1       0.000000  0.000000   0.094577  ...   0.086715   0.000000        0.0   \n",
       "2       0.000000  0.000000   0.095815  ...   0.105288   0.000000        0.0   \n",
       "3       0.079608  0.207704   0.000000  ...   0.000000   0.000000        0.0   \n",
       "4       0.000000  0.077643   0.169931  ...   0.000000   0.000000        0.0   \n",
       "...          ...       ...        ...  ...        ...        ...        ...   \n",
       "556550  0.000000  0.000000   0.097309  ...   0.085627   0.000000        0.0   \n",
       "556551  0.000000  0.000092   0.000000  ...   0.000000   0.133075        0.0   \n",
       "556552  0.000000  0.000000   0.173622  ...   0.000000   0.108337        0.0   \n",
       "556553  0.000000  0.000000   0.174541  ...   0.000000   0.108792        0.0   \n",
       "556554  0.000000  0.000000   0.173622  ...   0.000000   0.108337        0.0   \n",
       "\n",
       "        Feature25  Feature26  Feature27  Feature28  Feature29  Feature30  \\\n",
       "0        0.090651   0.000000   0.113022   0.061046   0.000000   0.000000   \n",
       "1        0.111254   0.000000   0.086987   0.074906   0.000000   0.000000   \n",
       "2        0.106155   0.000000   0.111396   0.057318   0.000000   0.000000   \n",
       "3        0.000000   0.278125   0.000000   0.140124   0.000000   0.000000   \n",
       "4        0.048168   0.006734   0.127435   0.000000   0.128147   0.143139   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "556550   0.110582   0.000000   0.086608   0.106597   0.000000   0.000000   \n",
       "556551   0.000000   0.088550   0.000000   0.258052   0.000000   0.000000   \n",
       "556552   0.059559   0.000000   0.123869   0.215068   0.124593   0.102316   \n",
       "556553   0.000000   0.000000   0.102138   0.191882   0.000000   0.078428   \n",
       "556554   0.059559   0.000000   0.123869   0.215068   0.124593   0.102316   \n",
       "\n",
       "        Label  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  \n",
       "...       ...  \n",
       "556550      1  \n",
       "556551      1  \n",
       "556552      1  \n",
       "556553      1  \n",
       "556554      1  \n",
       "\n",
       "[556555 rows x 31 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f92de787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly sample 500 data point for training\n",
    "df_train=df.sample(417417)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45a1f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = mydataframe.drop(['acol','bcol'], axis=1).values \n",
    "#y = mydataframe['targetvalue'].values\n",
    "df_label=df_train.Label\n",
    "X=df_train.drop(['Label'], axis=1).values \n",
    "y=df_train['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3faee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label.to_csv('abnormallabel1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3048d9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292191 Training sequences (292191, 30)\n",
      "125226 Validation sequences (125226, 30)\n",
      "292191 Training sequences (292191,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(X,y,train_size=0.70, random_state=2)\n",
    "print(len(x_train), \"Training sequences\",x_train.shape)\n",
    "print(len(x_val), \"Validation sequences\",x_val.shape)\n",
    "print(len(y_train), \"Training sequences\",y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b6ec4c",
   "metadata": {},
   "source": [
    "# Simple Attention\n",
    "#https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38c6b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(self,input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(attention, self).build(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b304465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self,x):\n",
    "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
    "        at=K.softmax(et)\n",
    "        at=K.expand_dims(at,axis=-1)\n",
    "        output=x*at\n",
    "        return K.sum(output,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba83277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18d8f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
    "        at=K.softmax(et)\n",
    "        at=K.expand_dims(at,axis=-1)\n",
    "        output=x*at\n",
    "        return K.sum(output,axis=1)\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(attention,self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84e9ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation,RepeatVector,Permute \n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "maxlen=30\n",
    "vocab_size = x_train.shape[0]\n",
    "visible = layers.Input(shape=(maxlen,))\n",
    "\n",
    "embed=Embedding(vocab_size,30)(visible)\n",
    "\n",
    "activations= keras.layers.LSTM(30, return_sequences=True)(embed)\n",
    "\n",
    "attention = TimeDistributed(Dense(1, activation='tanh'))(activations) \n",
    "attention = Flatten()(attention)\n",
    "attention = Activation('softmax', name='attention_vec')(attention)\n",
    "#attention = RepeatVector(1)(attention)\n",
    "#attention = Permute([2, 1])(attention) \n",
    "\n",
    "#sent_representation = keras.layers.multiply([activations, attention])\n",
    "#sent_representation = Lambda(lambda xin: K.sum(xin, axis=1))(sent_representation)\n",
    "predictions=Dense(1, activation='sigmoid')(attention)\n",
    "\n",
    "model = keras.Model(inputs=visible, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8f556ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 30, 30)            8765730   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 30, 30)            7320      \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 30, 1)             31        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "attention_vec (Activation)   (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 8,773,112\n",
      "Trainable params: 8,773,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dbb0169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85336451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 292191 samples, validate on 125226 samples\n",
      "Epoch 1/5\n",
      "292191/292191 [==============================] - 103s 351us/sample - loss: 0.5113 - accuracy: 1.0000 - val_loss: 0.4119 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "292191/292191 [==============================] - 99s 339us/sample - loss: 0.3352 - accuracy: 1.0000 - val_loss: 0.2719 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "292191/292191 [==============================] - 97s 332us/sample - loss: 0.2284 - accuracy: 1.0000 - val_loss: 0.1910 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "292191/292191 [==============================] - 94s 320us/sample - loss: 0.1638 - accuracy: 1.0000 - val_loss: 0.1400 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "292191/292191 [==============================] - 97s 332us/sample - loss: 0.1220 - accuracy: 1.0000 - val_loss: 0.1060 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train, y_train, batch_size=1024, epochs=5, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "739db6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0h 8m 19s\n"
     ]
    }
   ],
   "source": [
    "end = datetime.datetime.now()\n",
    "\n",
    "diff = (end - start)\n",
    "\n",
    "datetime.timedelta(seconds=10, microseconds=885206)\n",
    "\n",
    "diff_seconds = int(diff.total_seconds())\n",
    "\n",
    "minute_seconds, seconds = divmod(diff_seconds, 60)\n",
    "hours, minutes = divmod(minute_seconds, 60)\n",
    "hms = f\"{hours}h {minutes}m {seconds}s\"\n",
    "\n",
    "'0h 0m 10s'\n",
    "print(hms) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "22284ecc",
   "metadata": {},
   "source": [
    "new_model = Model(inputs=model.input, outputs=model.layers[2].output)\n",
    "#output_before_att = new_model.predict(x_test_sample) #extract layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31dc6073",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=model.input,\n",
    "              outputs=[model.output, model.get_layer('attention_vec').output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed435634",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.get_layer('attention_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94604c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([[0.096499  , 0.09907921, 0.09916287, ..., 0.01342091, 0.01342091,\n",
      "        0.01342091],\n",
      "       [0.096499  , 0.09907921, 0.09916287, ..., 0.01342091, 0.01342091,\n",
      "        0.01342091],\n",
      "       [0.096499  , 0.09907921, 0.09916287, ..., 0.01342091, 0.01342091,\n",
      "        0.01342091],\n",
      "       ...,\n",
      "       [0.096499  , 0.09907917, 0.09916287, ..., 0.01342091, 0.01342091,\n",
      "        0.01342091],\n",
      "       [0.096499  , 0.09907917, 0.09916287, ..., 0.01342091, 0.01342091,\n",
      "        0.01342091],\n",
      "       [0.096499  , 0.09907917, 0.09916287, ..., 0.01342091, 0.01342091,\n",
      "        0.01342091]], dtype=float32)]]\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "keras_function = K.function([model.input], [layer.output])\n",
    "outputs.append(keras_function([x_train, 0]))\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01f92a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.03278775, 0.0356584 , 0.03053794, ..., 0.03786541, 0.03458995,\n",
      "        0.03382791],\n",
      "       [0.04147852, 0.03682404, 0.03139435, ..., 0.04010637, 0.03139435,\n",
      "        0.03139435],\n",
      "       [0.03317318, 0.0357272 , 0.03130845, ..., 0.03793639, 0.03130845,\n",
      "        0.0335738 ],\n",
      "       ...,\n",
      "       [0.03165696, 0.03279825, 0.03165696, ..., 0.04294105, 0.03165696,\n",
      "        0.03530839],\n",
      "       [0.03137515, 0.03137515, 0.03137515, ..., 0.03493673, 0.03137515,\n",
      "        0.03137515],\n",
      "       [0.03279227, 0.03566266, 0.03054188, ..., 0.03787083, 0.03459419,\n",
      "        0.03383232]], dtype=float32)]\n",
      "(1, 417417, 30)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "hidden_layers = keras.backend.function(\n",
    "[layer.input],  # we will feed the function with the input of the first layer  \n",
    "[layer.output,] # we want to get the output of the first layer\n",
    ")\n",
    "h=hidden_layers([X])\n",
    "print(h)\n",
    "print(np.shape(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b4f31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.concatenate(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbb39a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('abnormalattentionvector.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2eecd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_1 = pd.read_csv('abnormalattentionvector.csv')\n",
    "csv_2 = pd.read_csv('abnormallabel1.csv')\n",
    "\n",
    "result = pd.concat([csv_1, csv_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5563f0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER=/tmp\n"
     ]
    }
   ],
   "source": [
    "%env JOBLIB_TEMP_FOLDER=/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5cbd6935",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"attentionabnormalresult.csv\", index=None, header=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
