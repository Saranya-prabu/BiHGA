{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/function9/bidirectional-lstm-gan-music-generation\n",
    "#https://www.kaggle.com/abhisheksinha28/bidirectional-lstm/data\n",
    "#https://www.kaggle.com/sekfook97/gan-for-anomaly-detection\n",
    "# importing libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Bidirectional, LSTM, Reshape, RepeatVector, TimeDistributed\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "\"feature1\": np.float16,\n",
    "\"feature2\": np.float16,\n",
    "\"feature3\": np.float16,\n",
    "\"feature4\": np.float16,\n",
    "\"feature5\": np.float16,\n",
    "\"feature6\": np.float16,\n",
    "\"feature7\": np.float16,\n",
    "\"feature8\": np.float16,\n",
    "\"feature9\": np.float16,\n",
    "\"feature10\": np.float16,\n",
    "\"feature11\": np.float16,\n",
    "\"feature12\": np.float16,\n",
    "\"feature13\": np.float16,\n",
    "\"feature14\": np.float16,\n",
    "\"feature15\": np.float16,\n",
    "\"feature16\": np.float16,\n",
    "\"feature17\": np.float16,\n",
    "\"feature18\": np.float16,\n",
    "\"feature19\": np.float16,\n",
    "\"feature20\": np.float16,\n",
    "\"feature21\": np.float16,\n",
    "\"feature22\": np.float16,\n",
    "\"feature23\": np.float16,\n",
    "\"feature24\": np.float16,\n",
    "\"feature25\": np.float16,    \n",
    "\"feature26\": np.float16,\n",
    "\"feature27\": np.float16,\n",
    "\"feature28\": np.float16,\n",
    "\"feature29\": np.float16,\n",
    "\"feature30\": np.float16,    \n",
    "\"label\": np.object}    \n",
    "columns = [\"feature1\",\"feature2\",\"feature3\",\"feature4\",\"feature5\",\"feature6\",\"feature7\",\"feature8\",\"feature9\",\"feature10\",\"feature11\",\"feature12\",\"feature13\",\"feature14\",\"feature15\",\"feature16\",\"feature17\",\"feature18\",\"feature19\",\"feature20\",\"feature21\",\"feature22\",\"feature23\",\"feature24\",\"feature25\",\"feature26\",\"feature27\",\"feature28\",\"feature29\",\"feature30\",\"label\"]\n",
    "#df = pd.read_csv(\"/kaggle/input/kdd-cup-1999-data/kddcup.data_10_percent_corrected\", sep=\",\", names=columns, dtype=dtypes, index_col=None)\n",
    "df = pd.read_csv(r\"C:\\Users\\admin\\CICIDS - 30 neurons\\afterdimensionalityreduction\",sep=\",\", names=columns, dtype=str, index_col=None)\n",
    "#df_read = pd.read_csv(savefile, dtype=str, index_col=0)\n",
    "#print(type(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal      2271320\n",
       "Abnormal     556556\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             feature1     feature2 feature3     feature4    feature5  \\\n",
       "0        0.065463625  0.027736843      0.0          0.0         0.0   \n",
       "1          0.0848677          0.0      0.0          0.0         0.0   \n",
       "2         0.08617264          0.0      0.0          0.0         0.0   \n",
       "3         0.08651951          0.0      0.0          0.0         0.0   \n",
       "4        0.065462984   0.02774103      0.0          0.0         0.0   \n",
       "...              ...          ...      ...          ...         ...   \n",
       "2827871          0.0  0.030126706      0.0     0.045611  0.17092325   \n",
       "2827872          0.0          0.0      0.0          0.0  0.13751188   \n",
       "2827873          0.0  0.051236615      0.0          0.0         0.0   \n",
       "2827874          0.0          0.0      0.0   0.02014964  0.15557319   \n",
       "2827875          0.0          0.0      0.0  0.016282216  0.15501131   \n",
       "\n",
       "           feature6    feature7    feature8    feature9   feature10  ...  \\\n",
       "0               0.0  0.18545482         0.0  0.06830312  0.16591072  ...   \n",
       "1               0.0  0.18601944         0.0  0.07059492  0.24759579  ...   \n",
       "2               0.0  0.18536246         0.0  0.07189356  0.24657574  ...   \n",
       "3               0.0   0.1523949         0.0  0.05285023  0.24662375  ...   \n",
       "4               0.0  0.18544693         0.0  0.06833266  0.16591166  ...   \n",
       "...             ...         ...         ...         ...         ...  ...   \n",
       "2827871  0.18404113         0.0  0.07704185         0.0  0.07995535  ...   \n",
       "2827872  0.20500034         0.0         0.0         0.0         0.0  ...   \n",
       "2827873         0.0  0.10252521         0.0  0.18571489  0.15420453  ...   \n",
       "2827874  0.19371656         0.0         0.0         0.0   0.0209108  ...   \n",
       "2827875  0.19090527         0.0         0.0         0.0  0.03676054  ...   \n",
       "\n",
       "           feature22    feature23 feature24     feature25 feature26  \\\n",
       "0        0.060978614  0.008421805       0.0    0.03520457       0.0   \n",
       "1         0.10572878          0.0       0.0   0.014328428       0.0   \n",
       "2         0.10626503          0.0       0.0   0.014573529       0.0   \n",
       "3         0.09038926          0.0       0.0   0.018660925       0.0   \n",
       "4          0.0609689   0.00842534       0.0    0.03520482       0.0   \n",
       "...              ...          ...       ...           ...       ...   \n",
       "2827871          0.0   0.06389466       0.0     0.0486761       0.0   \n",
       "2827872          0.0  0.014810607       0.0    0.05983961       0.0   \n",
       "2827873   0.08684944          0.0       0.0   0.024336088       0.0   \n",
       "2827874          0.0   0.06913914       0.0           0.0       0.0   \n",
       "2827875          0.0   0.06263712       0.0  0.0008633882       0.0   \n",
       "\n",
       "          feature27 feature28 feature29    feature30   label  \n",
       "0        0.25070333       0.0       0.0   0.09787195  Normal  \n",
       "1         0.2499516       0.0       0.0          0.0  Normal  \n",
       "2         0.2502615       0.0       0.0          0.0  Normal  \n",
       "3         0.2299474       0.0       0.0          0.0  Normal  \n",
       "4        0.25069907       0.0       0.0  0.097872004  Normal  \n",
       "...             ...       ...       ...          ...     ...  \n",
       "2827871  0.09643841       0.0       0.0          0.0  Normal  \n",
       "2827872  0.08832435       0.0       0.0          0.0  Normal  \n",
       "2827873  0.25827682       0.0       0.0   0.09253141  Normal  \n",
       "2827874  0.08868496       0.0       0.0          0.0  Normal  \n",
       "2827875  0.09066569       0.0       0.0          0.0  Normal  \n",
       "\n",
       "[2827876 rows x 31 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 for normal data and 1 for abnormalities\n",
    "df.label=df.label.apply(lambda x: 0 if x == 'Normal' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2827876, 31)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             feature1     feature2 feature3     feature4    feature5  \\\n",
       "0        0.065463625  0.027736843      0.0          0.0         0.0   \n",
       "1          0.0848677          0.0      0.0          0.0         0.0   \n",
       "2         0.08617264          0.0      0.0          0.0         0.0   \n",
       "3         0.08651951          0.0      0.0          0.0         0.0   \n",
       "4        0.065462984   0.02774103      0.0          0.0         0.0   \n",
       "...              ...          ...      ...          ...         ...   \n",
       "2827871          0.0  0.030126706      0.0     0.045611  0.17092325   \n",
       "2827872          0.0          0.0      0.0          0.0  0.13751188   \n",
       "2827873          0.0  0.051236615      0.0          0.0         0.0   \n",
       "2827874          0.0          0.0      0.0   0.02014964  0.15557319   \n",
       "2827875          0.0          0.0      0.0  0.016282216  0.15501131   \n",
       "\n",
       "           feature6    feature7    feature8    feature9   feature10  ...  \\\n",
       "0               0.0  0.18545482         0.0  0.06830312  0.16591072  ...   \n",
       "1               0.0  0.18601944         0.0  0.07059492  0.24759579  ...   \n",
       "2               0.0  0.18536246         0.0  0.07189356  0.24657574  ...   \n",
       "3               0.0   0.1523949         0.0  0.05285023  0.24662375  ...   \n",
       "4               0.0  0.18544693         0.0  0.06833266  0.16591166  ...   \n",
       "...             ...         ...         ...         ...         ...  ...   \n",
       "2827871  0.18404113         0.0  0.07704185         0.0  0.07995535  ...   \n",
       "2827872  0.20500034         0.0         0.0         0.0         0.0  ...   \n",
       "2827873         0.0  0.10252521         0.0  0.18571489  0.15420453  ...   \n",
       "2827874  0.19371656         0.0         0.0         0.0   0.0209108  ...   \n",
       "2827875  0.19090527         0.0         0.0         0.0  0.03676054  ...   \n",
       "\n",
       "           feature22    feature23 feature24     feature25 feature26  \\\n",
       "0        0.060978614  0.008421805       0.0    0.03520457       0.0   \n",
       "1         0.10572878          0.0       0.0   0.014328428       0.0   \n",
       "2         0.10626503          0.0       0.0   0.014573529       0.0   \n",
       "3         0.09038926          0.0       0.0   0.018660925       0.0   \n",
       "4          0.0609689   0.00842534       0.0    0.03520482       0.0   \n",
       "...              ...          ...       ...           ...       ...   \n",
       "2827871          0.0   0.06389466       0.0     0.0486761       0.0   \n",
       "2827872          0.0  0.014810607       0.0    0.05983961       0.0   \n",
       "2827873   0.08684944          0.0       0.0   0.024336088       0.0   \n",
       "2827874          0.0   0.06913914       0.0           0.0       0.0   \n",
       "2827875          0.0   0.06263712       0.0  0.0008633882       0.0   \n",
       "\n",
       "          feature27 feature28 feature29    feature30 label  \n",
       "0        0.25070333       0.0       0.0   0.09787195     0  \n",
       "1         0.2499516       0.0       0.0          0.0     0  \n",
       "2         0.2502615       0.0       0.0          0.0     0  \n",
       "3         0.2299474       0.0       0.0          0.0     0  \n",
       "4        0.25069907       0.0       0.0  0.097872004     0  \n",
       "...             ...       ...       ...          ...   ...  \n",
       "2827871  0.09643841       0.0       0.0          0.0     0  \n",
       "2827872  0.08832435       0.0       0.0          0.0     0  \n",
       "2827873  0.25827682       0.0       0.0   0.09253141     0  \n",
       "2827874  0.08868496       0.0       0.0          0.0     0  \n",
       "2827875  0.09066569       0.0       0.0          0.0     0  \n",
       "\n",
       "[2827876 rows x 31 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     object\n",
       "feature2     object\n",
       "feature3     object\n",
       "feature4     object\n",
       "feature5     object\n",
       "feature6     object\n",
       "feature7     object\n",
       "feature8     object\n",
       "feature9     object\n",
       "feature10    object\n",
       "feature11    object\n",
       "feature12    object\n",
       "feature13    object\n",
       "feature14    object\n",
       "feature15    object\n",
       "feature16    object\n",
       "feature17    object\n",
       "feature18    object\n",
       "feature19    object\n",
       "feature20    object\n",
       "feature21    object\n",
       "feature22    object\n",
       "feature23    object\n",
       "feature24    object\n",
       "feature25    object\n",
       "feature26    object\n",
       "feature27    object\n",
       "feature28    object\n",
       "feature29    object\n",
       "feature30    object\n",
       "label         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df)   \n",
    " \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['feature1'] = df['feature1'].astype(float) \n",
    "df['feature2'] = df['feature2'].astype(float) \n",
    "df['feature3'] = df['feature3'].astype(float) \n",
    "df['feature4'] = df['feature4'].astype(float) \n",
    "df['feature5'] = df['feature5'].astype(float) \n",
    "df['feature6'] = df['feature6'].astype(float) \n",
    "df['feature7'] = df['feature7'].astype(float) \n",
    "df['feature8'] = df['feature8'].astype(float) \n",
    "df['feature9'] = df['feature9'].astype(float) \n",
    "df['feature10'] = df['feature10'].astype(float) \n",
    "df['feature11'] = df['feature11'].astype(float) \n",
    "df['feature12'] = df['feature12'].astype(float) \n",
    "df['feature13'] = df['feature13'].astype(float) \n",
    "df['feature14'] = df['feature14'].astype(float) \n",
    "df['feature15'] = df['feature15'].astype(float) \n",
    "df['feature16'] = df['feature16'].astype(float) \n",
    "df['feature17'] = df['feature17'].astype(float) \n",
    "df['feature18'] = df['feature18'].astype(float) \n",
    "df['feature19'] = df['feature19'].astype(float) \n",
    "df['feature20'] = df['feature20'].astype(float) \n",
    "df['feature21'] = df['feature21'].astype(float) \n",
    "df['feature22'] = df['feature22'].astype(float) \n",
    "df['feature23'] = df['feature23'].astype(float) \n",
    "df['feature24'] = df['feature24'].astype(float) \n",
    "df['feature25'] = df['feature25'].astype(float) \n",
    "df['feature26'] = df['feature26'].astype(float) \n",
    "df['feature27'] = df['feature27'].astype(float) \n",
    "df['feature28'] = df['feature28'].astype(float)\n",
    "df['feature29'] = df['feature29'].astype(float) \n",
    "df['feature30'] = df['feature30'].astype(float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly sample 500 data point for training\n",
    "df_train=df[df.label==0].sample(2271320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2271320, 31)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     float64\n",
       "feature2     float64\n",
       "feature3     float64\n",
       "feature4     float64\n",
       "feature5     float64\n",
       "feature6     float64\n",
       "feature7     float64\n",
       "feature8     float64\n",
       "feature9     float64\n",
       "feature10    float64\n",
       "feature11    float64\n",
       "feature12    float64\n",
       "feature13    float64\n",
       "feature14    float64\n",
       "feature15    float64\n",
       "feature16    float64\n",
       "feature17    float64\n",
       "feature18    float64\n",
       "feature19    float64\n",
       "feature20    float64\n",
       "feature21    float64\n",
       "feature22    float64\n",
       "feature23    float64\n",
       "feature24    float64\n",
       "feature25    float64\n",
       "feature26    float64\n",
       "feature27    float64\n",
       "feature28    float64\n",
       "feature29    float64\n",
       "feature30    float64\n",
       "label          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the item sampled from our dataset\n",
    "index_list=df_train.index\n",
    "df=df.drop(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     0.000000\n",
       "feature2     0.252052\n",
       "feature3     0.000000\n",
       "feature4     0.000000\n",
       "feature5     0.232086\n",
       "feature6     0.000000\n",
       "feature7     0.121329\n",
       "feature8     0.000000\n",
       "feature9     0.000000\n",
       "feature10    0.000000\n",
       "feature11    0.000000\n",
       "feature12    0.000000\n",
       "feature13    0.000000\n",
       "feature14    0.110067\n",
       "feature15    0.000000\n",
       "feature16    0.165029\n",
       "feature17    0.100319\n",
       "feature18    0.089029\n",
       "feature19    0.249916\n",
       "feature20    0.000000\n",
       "feature21    0.027309\n",
       "feature22    0.000000\n",
       "feature23    0.057609\n",
       "feature24    0.000000\n",
       "feature25    0.072715\n",
       "feature26    0.000000\n",
       "feature27    0.000000\n",
       "feature28    0.169824\n",
       "feature29    0.000000\n",
       "feature30    0.000000\n",
       "label        0.000000\n",
       "Name: 1393636, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature22</th>\n",
       "      <th>feature23</th>\n",
       "      <th>feature24</th>\n",
       "      <th>feature25</th>\n",
       "      <th>feature26</th>\n",
       "      <th>feature27</th>\n",
       "      <th>feature28</th>\n",
       "      <th>feature29</th>\n",
       "      <th>feature30</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18871</th>\n",
       "      <td>0.153178</td>\n",
       "      <td>0.195167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009780</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18872</th>\n",
       "      <td>0.058067</td>\n",
       "      <td>0.131876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100335</td>\n",
       "      <td>0.191909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076363</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18873</th>\n",
       "      <td>0.169810</td>\n",
       "      <td>0.210422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18874</th>\n",
       "      <td>0.103257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18875</th>\n",
       "      <td>0.198886</td>\n",
       "      <td>0.179771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.215698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.215064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "18871  0.153178  0.195167       0.0  0.000000  0.221847       0.0  0.083204   \n",
       "18872  0.058067  0.131876       0.0  0.072852  0.000000       0.0  0.000000   \n",
       "18873  0.169810  0.210422       0.0  0.000000  0.234195       0.0  0.090171   \n",
       "18874  0.103257  0.000000       0.0  0.000000  0.000000       0.0  0.034368   \n",
       "18875  0.198886  0.179771       0.0  0.000000  0.215698       0.0  0.075397   \n",
       "\n",
       "       feature8  feature9  feature10  ...  feature22  feature23  feature24  \\\n",
       "18871       0.0       0.0   0.000000  ...        0.0   0.009780        0.0   \n",
       "18872       0.0       0.0   0.174663  ...        0.0   0.108989        0.0   \n",
       "18873       0.0       0.0   0.000000  ...        0.0   0.015881        0.0   \n",
       "18874       0.0       0.0   0.000000  ...        0.0   0.114448        0.0   \n",
       "18875       0.0       0.0   0.000000  ...        0.0   0.024363        0.0   \n",
       "\n",
       "       feature25  feature26  feature27  feature28  feature29  feature30  label  \n",
       "18871        0.0   0.000000   0.000000   0.206045        0.0   0.000000      1  \n",
       "18872        0.0   0.000000   0.100335   0.191909        0.0   0.076363      1  \n",
       "18873        0.0   0.000000   0.000000   0.194010        0.0   0.000000      1  \n",
       "18874        0.0   0.092519   0.000000   0.324249        0.0   0.069000      1  \n",
       "18875        0.0   0.000000   0.000000   0.215064        0.0   0.000000      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the label columns\n",
    "df_label=df_train.label\n",
    "df_train=df_train.drop('label',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label\n",
    "df_label.to_csv('normallabel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2271320, 30)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     0.000000\n",
       "feature2     0.252052\n",
       "feature3     0.000000\n",
       "feature4     0.000000\n",
       "feature5     0.232086\n",
       "feature6     0.000000\n",
       "feature7     0.121329\n",
       "feature8     0.000000\n",
       "feature9     0.000000\n",
       "feature10    0.000000\n",
       "feature11    0.000000\n",
       "feature12    0.000000\n",
       "feature13    0.000000\n",
       "feature14    0.110067\n",
       "feature15    0.000000\n",
       "feature16    0.165029\n",
       "feature17    0.100319\n",
       "feature18    0.089029\n",
       "feature19    0.249916\n",
       "feature20    0.000000\n",
       "feature21    0.027309\n",
       "feature22    0.000000\n",
       "feature23    0.057609\n",
       "feature24    0.000000\n",
       "feature25    0.072715\n",
       "feature26    0.000000\n",
       "feature27    0.000000\n",
       "feature28    0.169824\n",
       "feature29    0.000000\n",
       "feature30    0.000000\n",
       "Name: 1393636, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2271320, 30)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.values.reshape((df_train.shape[0], 1, df_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2271320, 1, 30)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature22</th>\n",
       "      <th>feature23</th>\n",
       "      <th>feature24</th>\n",
       "      <th>feature25</th>\n",
       "      <th>feature26</th>\n",
       "      <th>feature27</th>\n",
       "      <th>feature28</th>\n",
       "      <th>feature29</th>\n",
       "      <th>feature30</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18871</th>\n",
       "      <td>0.153178</td>\n",
       "      <td>0.195167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009780</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18872</th>\n",
       "      <td>0.058067</td>\n",
       "      <td>0.131876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100335</td>\n",
       "      <td>0.191909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076363</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18873</th>\n",
       "      <td>0.169810</td>\n",
       "      <td>0.210422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18874</th>\n",
       "      <td>0.103257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18875</th>\n",
       "      <td>0.198886</td>\n",
       "      <td>0.179771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.215698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.215064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "18871  0.153178  0.195167       0.0  0.000000  0.221847       0.0  0.083204   \n",
       "18872  0.058067  0.131876       0.0  0.072852  0.000000       0.0  0.000000   \n",
       "18873  0.169810  0.210422       0.0  0.000000  0.234195       0.0  0.090171   \n",
       "18874  0.103257  0.000000       0.0  0.000000  0.000000       0.0  0.034368   \n",
       "18875  0.198886  0.179771       0.0  0.000000  0.215698       0.0  0.075397   \n",
       "\n",
       "       feature8  feature9  feature10  ...  feature22  feature23  feature24  \\\n",
       "18871       0.0       0.0   0.000000  ...        0.0   0.009780        0.0   \n",
       "18872       0.0       0.0   0.174663  ...        0.0   0.108989        0.0   \n",
       "18873       0.0       0.0   0.000000  ...        0.0   0.015881        0.0   \n",
       "18874       0.0       0.0   0.000000  ...        0.0   0.114448        0.0   \n",
       "18875       0.0       0.0   0.000000  ...        0.0   0.024363        0.0   \n",
       "\n",
       "       feature25  feature26  feature27  feature28  feature29  feature30  label  \n",
       "18871        0.0   0.000000   0.000000   0.206045        0.0   0.000000      1  \n",
       "18872        0.0   0.000000   0.100335   0.191909        0.0   0.076363      1  \n",
       "18873        0.0   0.000000   0.000000   0.194010        0.0   0.000000      1  \n",
       "18874        0.0   0.092519   0.000000   0.324249        0.0   0.069000      1  \n",
       "18875        0.0   0.000000   0.000000   0.215064        0.0   0.000000      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "timesteps =  df_train.shape[1] # equal to the lookback\n",
    "n_features =  df_train.shape[2] # 59\n",
    "\n",
    "epochs = 15\n",
    "batch = 64\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=dict()\n",
    "history['gen']=[]\n",
    "history['dis']=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error\n",
    "                             ,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 1\n",
    "        self.img_cols = 30\n",
    "        self.img_shape = (self.img_rows, self.img_cols)\n",
    "        self.latent_dim = 30\n",
    "\n",
    "        optimizer = Adam(0.0001, 0.4)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates song\n",
    "        z = Input(shape=(1,30))\n",
    "    \n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(1, 30)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #specifying output to have 40 timesteps\n",
    "        model.add(RepeatVector(30))\n",
    "        #specifying 1 feature as the output\n",
    "        \n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.3))   \n",
    "        model.add(TimeDistributed(Dense(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(1,30))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "        #return Model(img)\n",
    "    \n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(64, use_bias=False, input_shape=(30, 1)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Convolution1D(32, 3, strides=2, padding='same', use_bias=False))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Convolution1D(1, 3, strides=2, padding='same', use_bias=False))\n",
    "        model.add(Flatten())        \n",
    "        model.add(RepeatVector(1))        \n",
    "        model.add(TimeDistributed(Dense(32, activation = 'relu')))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(1, activation = 'relu')))\n",
    "        #model.add(TimeDistributed(Dense(1)))\n",
    "        #model.add(Dense(1, activation='tanh'))\n",
    "        #model.add(Dense(1))\n",
    "        #model.add(Activation('sigmoid'))\n",
    "        model.summary()\n",
    "        #model.add(layers.BatchNormalization())\n",
    "        #model.add(sigmoid())\n",
    "       \n",
    "        img = Input(shape=(30,1))\n",
    "        validity = model(img)\n",
    "        print(\"exit\")\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs=15, batch_size=64, save_interval=50):\n",
    "        print(\"enter\")  \n",
    "        # Load the dataset\n",
    "        X_train = df_train\n",
    "       \n",
    "        # Rescale 0 to 1\n",
    "      #  X_train = X_train / 128\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size,1,1))\n",
    "        fake = np.zeros((batch_size,1,1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of songs\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            imgs = np.array(imgs)\n",
    "            imgs = imgs.reshape(len(imgs),30,1)\n",
    "\n",
    "            # Sample noise and generate a batch of new songs\n",
    "            noise = np.random.normal(0, 1, (batch_size,1,30))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake songs as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "            history['gen'].append(g_loss)\n",
    "            \n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            history['dis'].append(d_loss) \n",
    "            #return (g_loss,d_loss)\n",
    "            # If at save interval => save model\n",
    "#            if epoch % save_interval == 0:\n",
    " #               self.generator.save(\"LSTM_generator.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30, 64)            64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 15, 32)            6144      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 15, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 8, 1)              96        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 1, 8)              0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 1, 32)             288       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 1)              33        \n",
      "=================================================================\n",
      "Total params: 6,625\n",
      "Trainable params: 6,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "exit\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 1, 256)            162816    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1, 256)            394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 30, 256)           394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 30, 256)           394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 30, 256)           394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 30, 128)           32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 30, 128)           16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 30, 1)             129       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 30, 1)             0         \n",
      "=================================================================\n",
      "Total params: 2,183,553\n",
      "Trainable params: 2,183,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstmgan = LSTMGAN()\n",
    "#lstmgan.train(epochs=1000, batch_size=20, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter\n",
      "0 [D loss: 4.229382, acc.: 50.00%] [G loss: 7.435492]\n",
      "1 [D loss: 3.022723, acc.: 50.00%] [G loss: 6.613857]\n",
      "2 [D loss: 3.177461, acc.: 50.00%] [G loss: 6.103940]\n",
      "3 [D loss: 3.623389, acc.: 50.00%] [G loss: 5.910861]\n",
      "4 [D loss: 3.801928, acc.: 50.00%] [G loss: 5.731080]\n",
      "5 [D loss: 3.226596, acc.: 50.00%] [G loss: 5.430469]\n",
      "6 [D loss: 2.837979, acc.: 50.00%] [G loss: 5.170566]\n",
      "7 [D loss: 2.819552, acc.: 50.00%] [G loss: 5.023452]\n",
      "8 [D loss: 2.703064, acc.: 50.00%] [G loss: 4.914137]\n",
      "9 [D loss: 2.613641, acc.: 50.00%] [G loss: 5.015864]\n",
      "10 [D loss: 3.239717, acc.: 50.00%] [G loss: 4.981689]\n",
      "11 [D loss: 2.366534, acc.: 50.00%] [G loss: 4.834348]\n",
      "12 [D loss: 2.963384, acc.: 50.00%] [G loss: 4.742686]\n",
      "13 [D loss: 2.816781, acc.: 50.00%] [G loss: 4.627281]\n",
      "14 [D loss: 2.742011, acc.: 50.00%] [G loss: 4.574924]\n",
      "15 [D loss: 2.376592, acc.: 50.00%] [G loss: 4.586765]\n",
      "16 [D loss: 2.959889, acc.: 50.00%] [G loss: 4.499024]\n",
      "17 [D loss: 2.031479, acc.: 50.00%] [G loss: 4.523494]\n",
      "18 [D loss: 2.476005, acc.: 50.00%] [G loss: 4.566472]\n",
      "19 [D loss: 2.906918, acc.: 50.00%] [G loss: 4.400426]\n",
      "20 [D loss: 2.306024, acc.: 50.00%] [G loss: 4.290843]\n",
      "21 [D loss: 2.097229, acc.: 50.00%] [G loss: 4.158759]\n",
      "22 [D loss: 2.407945, acc.: 50.00%] [G loss: 4.149817]\n",
      "23 [D loss: 3.269538, acc.: 50.00%] [G loss: 4.251400]\n",
      "24 [D loss: 2.472698, acc.: 50.00%] [G loss: 5.321030]\n",
      "25 [D loss: 2.312268, acc.: 50.00%] [G loss: 3.841167]\n",
      "26 [D loss: 2.357434, acc.: 50.00%] [G loss: 5.163502]\n",
      "27 [D loss: 2.077720, acc.: 50.00%] [G loss: 3.594032]\n",
      "28 [D loss: 1.946801, acc.: 50.00%] [G loss: 4.441910]\n",
      "29 [D loss: 2.019805, acc.: 50.00%] [G loss: 3.223001]\n",
      "30 [D loss: 2.056470, acc.: 50.00%] [G loss: 4.421208]\n",
      "31 [D loss: 2.126548, acc.: 50.00%] [G loss: 3.991421]\n",
      "32 [D loss: 1.944358, acc.: 50.00%] [G loss: 4.999838]\n",
      "33 [D loss: 2.237714, acc.: 50.00%] [G loss: 3.477142]\n",
      "34 [D loss: 1.931799, acc.: 50.00%] [G loss: 2.789169]\n",
      "35 [D loss: 2.173631, acc.: 50.00%] [G loss: 2.940547]\n",
      "36 [D loss: 1.946924, acc.: 50.00%] [G loss: 1.679699]\n",
      "37 [D loss: 1.981258, acc.: 42.50%] [G loss: 4.118584]\n",
      "38 [D loss: 2.073502, acc.: 32.50%] [G loss: 1.864026]\n",
      "39 [D loss: 2.439232, acc.: 25.00%] [G loss: 4.304769]\n",
      "40 [D loss: 2.358568, acc.: 40.00%] [G loss: 3.647941]\n",
      "41 [D loss: 2.327362, acc.: 35.00%] [G loss: 3.437433]\n",
      "42 [D loss: 2.421438, acc.: 35.00%] [G loss: 3.157344]\n",
      "43 [D loss: 2.515227, acc.: 42.50%] [G loss: 0.854746]\n",
      "44 [D loss: 3.083406, acc.: 32.50%] [G loss: 1.762118]\n",
      "45 [D loss: 2.764600, acc.: 32.50%] [G loss: 2.584929]\n",
      "46 [D loss: 2.860841, acc.: 32.50%] [G loss: 1.886255]\n",
      "47 [D loss: 2.797308, acc.: 30.00%] [G loss: 3.299739]\n",
      "48 [D loss: 3.330940, acc.: 27.50%] [G loss: 4.068086]\n",
      "49 [D loss: 2.899568, acc.: 35.00%] [G loss: 3.103558]\n",
      "50 [D loss: 2.118291, acc.: 37.50%] [G loss: 3.992919]\n",
      "51 [D loss: 3.215094, acc.: 30.00%] [G loss: 5.575630]\n",
      "52 [D loss: 2.425640, acc.: 40.00%] [G loss: 6.249756]\n",
      "53 [D loss: 2.406240, acc.: 35.00%] [G loss: 2.140807]\n",
      "54 [D loss: 2.370224, acc.: 32.50%] [G loss: 4.803628]\n",
      "55 [D loss: 3.009012, acc.: 25.00%] [G loss: 2.621845]\n",
      "56 [D loss: 2.803998, acc.: 30.00%] [G loss: 6.304714]\n",
      "57 [D loss: 3.607334, acc.: 27.50%] [G loss: 2.547574]\n",
      "58 [D loss: 2.776603, acc.: 37.50%] [G loss: 3.807587]\n",
      "59 [D loss: 2.595896, acc.: 32.50%] [G loss: 5.673796]\n",
      "60 [D loss: 2.831410, acc.: 35.00%] [G loss: 4.127270]\n",
      "61 [D loss: 2.390164, acc.: 42.50%] [G loss: 7.529292]\n",
      "62 [D loss: 2.326744, acc.: 42.50%] [G loss: 8.328272]\n",
      "63 [D loss: 1.921802, acc.: 42.50%] [G loss: 6.013080]\n",
      "64 [D loss: 2.032538, acc.: 42.50%] [G loss: 6.828920]\n",
      "65 [D loss: 1.894429, acc.: 45.00%] [G loss: 9.633296]\n",
      "66 [D loss: 2.671324, acc.: 40.00%] [G loss: 5.706352]\n",
      "67 [D loss: 1.828797, acc.: 47.50%] [G loss: 4.113262]\n",
      "68 [D loss: 2.312147, acc.: 37.50%] [G loss: 7.034833]\n",
      "69 [D loss: 2.081125, acc.: 42.50%] [G loss: 7.921369]\n",
      "70 [D loss: 1.877763, acc.: 42.50%] [G loss: 6.596631]\n",
      "71 [D loss: 2.014898, acc.: 42.50%] [G loss: 5.359090]\n",
      "72 [D loss: 1.958208, acc.: 42.50%] [G loss: 7.734383]\n",
      "73 [D loss: 2.336648, acc.: 42.50%] [G loss: 5.109648]\n",
      "74 [D loss: 2.342996, acc.: 40.00%] [G loss: 6.423563]\n",
      "75 [D loss: 2.270075, acc.: 42.50%] [G loss: 7.100224]\n",
      "76 [D loss: 2.094861, acc.: 47.50%] [G loss: 9.018782]\n",
      "77 [D loss: 1.836275, acc.: 47.50%] [G loss: 7.046196]\n",
      "78 [D loss: 2.231569, acc.: 40.00%] [G loss: 3.193268]\n",
      "79 [D loss: 2.480125, acc.: 47.50%] [G loss: 8.308538]\n",
      "80 [D loss: 1.900977, acc.: 42.50%] [G loss: 8.840246]\n",
      "81 [D loss: 2.205750, acc.: 45.00%] [G loss: 8.494232]\n",
      "82 [D loss: 1.771308, acc.: 47.50%] [G loss: 10.541587]\n",
      "83 [D loss: 3.033288, acc.: 37.50%] [G loss: 6.568594]\n",
      "84 [D loss: 2.203851, acc.: 45.00%] [G loss: 8.579554]\n",
      "85 [D loss: 1.790810, acc.: 47.50%] [G loss: 9.798137]\n",
      "86 [D loss: 1.845259, acc.: 42.50%] [G loss: 7.789756]\n",
      "87 [D loss: 1.895446, acc.: 42.50%] [G loss: 7.689475]\n",
      "88 [D loss: 2.291109, acc.: 37.50%] [G loss: 7.858879]\n",
      "89 [D loss: 2.007840, acc.: 42.50%] [G loss: 7.611674]\n",
      "90 [D loss: 1.837739, acc.: 45.00%] [G loss: 7.063967]\n",
      "91 [D loss: 1.761375, acc.: 45.00%] [G loss: 6.268977]\n",
      "92 [D loss: 2.568942, acc.: 40.00%] [G loss: 9.821718]\n",
      "93 [D loss: 2.499245, acc.: 42.50%] [G loss: 4.702323]\n",
      "94 [D loss: 1.835597, acc.: 40.00%] [G loss: 9.118509]\n",
      "95 [D loss: 2.087427, acc.: 45.00%] [G loss: 8.332461]\n",
      "96 [D loss: 1.817929, acc.: 47.50%] [G loss: 9.670978]\n",
      "97 [D loss: 2.107208, acc.: 42.50%] [G loss: 10.745560]\n",
      "98 [D loss: 1.785949, acc.: 47.50%] [G loss: 8.407549]\n",
      "99 [D loss: 1.686570, acc.: 50.00%] [G loss: 10.157448]\n",
      "100 [D loss: 1.829955, acc.: 45.00%] [G loss: 8.982124]\n",
      "101 [D loss: 1.693359, acc.: 47.50%] [G loss: 6.274711]\n",
      "102 [D loss: 1.761896, acc.: 45.00%] [G loss: 9.870373]\n",
      "103 [D loss: 1.749853, acc.: 45.00%] [G loss: 10.064790]\n",
      "104 [D loss: 1.748611, acc.: 45.00%] [G loss: 8.258405]\n",
      "105 [D loss: 2.075301, acc.: 45.00%] [G loss: 10.691725]\n",
      "106 [D loss: 1.708530, acc.: 50.00%] [G loss: 8.517969]\n",
      "107 [D loss: 1.679527, acc.: 47.50%] [G loss: 7.130895]\n",
      "108 [D loss: 1.707194, acc.: 45.00%] [G loss: 9.420212]\n",
      "109 [D loss: 2.149799, acc.: 37.50%] [G loss: 10.627350]\n",
      "110 [D loss: 1.681226, acc.: 50.00%] [G loss: 7.703482]\n",
      "111 [D loss: 2.048218, acc.: 45.00%] [G loss: 11.676755]\n",
      "112 [D loss: 1.727045, acc.: 45.00%] [G loss: 6.946578]\n",
      "113 [D loss: 1.631680, acc.: 47.50%] [G loss: 7.923015]\n",
      "114 [D loss: 1.683700, acc.: 40.00%] [G loss: 11.167338]\n",
      "115 [D loss: 1.729125, acc.: 47.50%] [G loss: 9.114964]\n",
      "116 [D loss: 1.654909, acc.: 50.00%] [G loss: 7.405319]\n",
      "117 [D loss: 2.099646, acc.: 40.00%] [G loss: 8.660418]\n",
      "118 [D loss: 1.696666, acc.: 47.50%] [G loss: 6.962139]\n",
      "119 [D loss: 2.508590, acc.: 37.50%] [G loss: 11.848543]\n",
      "120 [D loss: 2.010337, acc.: 47.50%] [G loss: 8.797567]\n",
      "121 [D loss: 1.626662, acc.: 47.50%] [G loss: 6.513290]\n",
      "122 [D loss: 1.746266, acc.: 45.00%] [G loss: 8.845381]\n",
      "123 [D loss: 2.085815, acc.: 45.00%] [G loss: 5.606285]\n",
      "124 [D loss: 1.581457, acc.: 47.50%] [G loss: 10.597887]\n",
      "125 [D loss: 1.655237, acc.: 42.50%] [G loss: 9.810438]\n",
      "126 [D loss: 1.595596, acc.: 42.50%] [G loss: 7.515730]\n",
      "127 [D loss: 1.732106, acc.: 45.00%] [G loss: 7.864786]\n",
      "128 [D loss: 1.523247, acc.: 50.00%] [G loss: 7.846010]\n",
      "129 [D loss: 1.721902, acc.: 45.00%] [G loss: 9.385888]\n",
      "130 [D loss: 1.708753, acc.: 40.00%] [G loss: 9.289447]\n",
      "131 [D loss: 1.990676, acc.: 42.50%] [G loss: 8.801834]\n",
      "132 [D loss: 1.651657, acc.: 45.00%] [G loss: 9.169659]\n",
      "133 [D loss: 1.960053, acc.: 45.00%] [G loss: 8.290961]\n",
      "134 [D loss: 1.656201, acc.: 45.00%] [G loss: 11.338758]\n",
      "135 [D loss: 1.652246, acc.: 45.00%] [G loss: 8.260714]\n",
      "136 [D loss: 1.622009, acc.: 45.00%] [G loss: 7.068377]\n",
      "137 [D loss: 2.074234, acc.: 40.00%] [G loss: 8.563193]\n",
      "138 [D loss: 1.593397, acc.: 50.00%] [G loss: 8.645798]\n",
      "139 [D loss: 1.592958, acc.: 50.00%] [G loss: 8.462316]\n",
      "140 [D loss: 1.935393, acc.: 42.50%] [G loss: 10.559044]\n",
      "141 [D loss: 2.084366, acc.: 40.00%] [G loss: 7.602155]\n",
      "142 [D loss: 2.406729, acc.: 42.50%] [G loss: 9.275325]\n",
      "143 [D loss: 1.982574, acc.: 42.50%] [G loss: 9.699718]\n",
      "144 [D loss: 2.246917, acc.: 45.00%] [G loss: 11.313075]\n",
      "145 [D loss: 1.534058, acc.: 47.50%] [G loss: 5.870327]\n",
      "146 [D loss: 1.661012, acc.: 47.50%] [G loss: 12.699118]\n",
      "147 [D loss: 1.543358, acc.: 47.50%] [G loss: 8.390219]\n",
      "148 [D loss: 1.571263, acc.: 47.50%] [G loss: 10.458140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 [D loss: 1.901767, acc.: 47.50%] [G loss: 10.860750]\n",
      "150 [D loss: 1.935331, acc.: 45.00%] [G loss: 9.677599]\n",
      "151 [D loss: 1.562776, acc.: 47.50%] [G loss: 8.427628]\n",
      "152 [D loss: 1.570220, acc.: 45.00%] [G loss: 8.941604]\n",
      "153 [D loss: 1.548822, acc.: 50.00%] [G loss: 9.477774]\n",
      "154 [D loss: 1.594282, acc.: 47.50%] [G loss: 11.294312]\n",
      "155 [D loss: 1.887206, acc.: 47.50%] [G loss: 7.524220]\n",
      "156 [D loss: 1.859654, acc.: 47.50%] [G loss: 10.476431]\n",
      "157 [D loss: 1.929291, acc.: 47.50%] [G loss: 9.807926]\n",
      "158 [D loss: 1.554977, acc.: 47.50%] [G loss: 5.688222]\n",
      "159 [D loss: 1.644143, acc.: 50.00%] [G loss: 13.297432]\n",
      "160 [D loss: 1.509240, acc.: 45.00%] [G loss: 14.045749]\n",
      "161 [D loss: 1.869604, acc.: 47.50%] [G loss: 11.054953]\n",
      "162 [D loss: 1.594570, acc.: 47.50%] [G loss: 7.210312]\n",
      "163 [D loss: 1.503408, acc.: 47.50%] [G loss: 6.247193]\n",
      "164 [D loss: 1.559116, acc.: 45.00%] [G loss: 12.025699]\n",
      "165 [D loss: 1.558027, acc.: 50.00%] [G loss: 8.589411]\n",
      "166 [D loss: 1.608910, acc.: 50.00%] [G loss: 9.408036]\n",
      "167 [D loss: 1.496628, acc.: 45.00%] [G loss: 12.641047]\n",
      "168 [D loss: 1.448350, acc.: 47.50%] [G loss: 10.510013]\n",
      "169 [D loss: 1.471320, acc.: 50.00%] [G loss: 11.333140]\n",
      "170 [D loss: 1.495898, acc.: 50.00%] [G loss: 10.785280]\n",
      "171 [D loss: 1.656154, acc.: 42.50%] [G loss: 9.903490]\n",
      "172 [D loss: 1.916779, acc.: 47.50%] [G loss: 9.775541]\n",
      "173 [D loss: 1.544522, acc.: 42.50%] [G loss: 10.478625]\n",
      "174 [D loss: 1.558426, acc.: 42.50%] [G loss: 11.929848]\n",
      "175 [D loss: 1.573892, acc.: 42.50%] [G loss: 8.669863]\n",
      "176 [D loss: 1.938126, acc.: 45.00%] [G loss: 12.032977]\n",
      "177 [D loss: 1.524621, acc.: 42.50%] [G loss: 11.113347]\n",
      "178 [D loss: 1.899860, acc.: 42.50%] [G loss: 9.008300]\n",
      "179 [D loss: 2.276482, acc.: 42.50%] [G loss: 9.121397]\n",
      "180 [D loss: 1.559625, acc.: 47.50%] [G loss: 10.421791]\n",
      "181 [D loss: 1.491122, acc.: 45.00%] [G loss: 10.334780]\n",
      "182 [D loss: 1.494557, acc.: 47.50%] [G loss: 12.912404]\n",
      "183 [D loss: 1.499436, acc.: 45.00%] [G loss: 8.465360]\n",
      "184 [D loss: 1.529943, acc.: 45.00%] [G loss: 10.614908]\n",
      "185 [D loss: 1.459395, acc.: 47.50%] [G loss: 10.345157]\n",
      "186 [D loss: 1.518861, acc.: 47.50%] [G loss: 10.595953]\n",
      "187 [D loss: 1.565937, acc.: 42.50%] [G loss: 9.578428]\n",
      "188 [D loss: 1.503265, acc.: 50.00%] [G loss: 9.032674]\n",
      "189 [D loss: 1.513946, acc.: 42.50%] [G loss: 10.606470]\n",
      "190 [D loss: 1.437174, acc.: 47.50%] [G loss: 7.829082]\n",
      "191 [D loss: 1.521161, acc.: 47.50%] [G loss: 9.057352]\n",
      "192 [D loss: 1.544403, acc.: 45.00%] [G loss: 10.379721]\n",
      "193 [D loss: 1.429551, acc.: 47.50%] [G loss: 9.789536]\n",
      "194 [D loss: 2.668123, acc.: 37.50%] [G loss: 9.829842]\n",
      "195 [D loss: 1.823722, acc.: 47.50%] [G loss: 6.379515]\n",
      "196 [D loss: 1.481828, acc.: 50.00%] [G loss: 11.279826]\n",
      "197 [D loss: 1.934546, acc.: 37.50%] [G loss: 9.130053]\n",
      "198 [D loss: 1.521062, acc.: 42.50%] [G loss: 7.511680]\n",
      "199 [D loss: 1.511916, acc.: 45.00%] [G loss: 6.224345]\n",
      "200 [D loss: 1.469699, acc.: 47.50%] [G loss: 9.884247]\n",
      "201 [D loss: 1.928265, acc.: 40.00%] [G loss: 10.417472]\n",
      "202 [D loss: 1.875068, acc.: 42.50%] [G loss: 9.733691]\n",
      "203 [D loss: 1.420585, acc.: 50.00%] [G loss: 9.939710]\n",
      "204 [D loss: 1.531644, acc.: 40.00%] [G loss: 9.037629]\n",
      "205 [D loss: 1.486410, acc.: 42.50%] [G loss: 9.023293]\n",
      "206 [D loss: 1.915738, acc.: 42.50%] [G loss: 9.302248]\n",
      "207 [D loss: 2.231870, acc.: 42.50%] [G loss: 9.118100]\n",
      "208 [D loss: 1.517789, acc.: 45.00%] [G loss: 9.167581]\n",
      "209 [D loss: 1.422990, acc.: 47.50%] [G loss: 8.617139]\n",
      "210 [D loss: 1.493250, acc.: 45.00%] [G loss: 12.432865]\n",
      "211 [D loss: 1.462834, acc.: 47.50%] [G loss: 9.697788]\n",
      "212 [D loss: 1.457611, acc.: 47.50%] [G loss: 11.760732]\n",
      "213 [D loss: 1.430602, acc.: 50.00%] [G loss: 10.763430]\n",
      "214 [D loss: 1.501764, acc.: 47.50%] [G loss: 11.504404]\n",
      "215 [D loss: 1.462612, acc.: 45.00%] [G loss: 11.112524]\n",
      "216 [D loss: 1.421238, acc.: 47.50%] [G loss: 8.721451]\n",
      "217 [D loss: 1.364020, acc.: 50.00%] [G loss: 9.018019]\n",
      "218 [D loss: 1.397440, acc.: 50.00%] [G loss: 11.397814]\n",
      "219 [D loss: 1.923249, acc.: 37.50%] [G loss: 9.699010]\n",
      "220 [D loss: 1.361201, acc.: 45.00%] [G loss: 11.313062]\n",
      "221 [D loss: 1.870909, acc.: 42.50%] [G loss: 12.654510]\n",
      "222 [D loss: 1.424747, acc.: 45.00%] [G loss: 10.583305]\n",
      "223 [D loss: 1.429566, acc.: 47.50%] [G loss: 7.789762]\n",
      "224 [D loss: 1.469832, acc.: 45.00%] [G loss: 8.651093]\n",
      "225 [D loss: 1.463925, acc.: 50.00%] [G loss: 10.616395]\n",
      "226 [D loss: 1.799871, acc.: 45.00%] [G loss: 11.232765]\n",
      "227 [D loss: 1.433794, acc.: 47.50%] [G loss: 11.914013]\n",
      "228 [D loss: 1.429074, acc.: 47.50%] [G loss: 9.830813]\n",
      "229 [D loss: 1.793566, acc.: 47.50%] [G loss: 11.307436]\n",
      "230 [D loss: 1.377629, acc.: 47.50%] [G loss: 9.813686]\n",
      "231 [D loss: 1.379056, acc.: 47.50%] [G loss: 9.753893]\n",
      "232 [D loss: 1.375690, acc.: 50.00%] [G loss: 10.600256]\n",
      "233 [D loss: 1.512626, acc.: 42.50%] [G loss: 7.683734]\n",
      "234 [D loss: 1.387376, acc.: 47.50%] [G loss: 9.124301]\n",
      "235 [D loss: 1.377780, acc.: 47.50%] [G loss: 9.174440]\n",
      "236 [D loss: 1.315002, acc.: 45.00%] [G loss: 9.816587]\n",
      "237 [D loss: 1.321702, acc.: 50.00%] [G loss: 9.820506]\n",
      "238 [D loss: 1.360695, acc.: 47.50%] [G loss: 11.972090]\n",
      "239 [D loss: 1.311814, acc.: 50.00%] [G loss: 10.720912]\n",
      "240 [D loss: 1.368014, acc.: 47.50%] [G loss: 7.482594]\n",
      "241 [D loss: 1.415178, acc.: 45.00%] [G loss: 8.375724]\n",
      "242 [D loss: 1.386198, acc.: 45.00%] [G loss: 8.553312]\n",
      "243 [D loss: 1.353945, acc.: 50.00%] [G loss: 9.119354]\n",
      "244 [D loss: 1.597004, acc.: 42.50%] [G loss: 10.691866]\n",
      "245 [D loss: 1.411373, acc.: 47.50%] [G loss: 7.851649]\n",
      "246 [D loss: 1.395774, acc.: 45.00%] [G loss: 9.838788]\n",
      "247 [D loss: 1.311273, acc.: 50.00%] [G loss: 7.502066]\n",
      "248 [D loss: 1.395314, acc.: 50.00%] [G loss: 8.566107]\n",
      "249 [D loss: 1.362274, acc.: 50.00%] [G loss: 7.164698]\n",
      "250 [D loss: 1.400315, acc.: 45.00%] [G loss: 7.831358]\n",
      "251 [D loss: 1.805813, acc.: 45.00%] [G loss: 6.940294]\n",
      "252 [D loss: 1.744320, acc.: 45.00%] [G loss: 9.181810]\n",
      "253 [D loss: 1.408979, acc.: 47.50%] [G loss: 9.837662]\n",
      "254 [D loss: 1.390026, acc.: 47.50%] [G loss: 9.868396]\n",
      "255 [D loss: 1.423044, acc.: 45.00%] [G loss: 12.019075]\n",
      "256 [D loss: 1.425242, acc.: 47.50%] [G loss: 9.687116]\n",
      "257 [D loss: 1.299393, acc.: 50.00%] [G loss: 11.829565]\n",
      "258 [D loss: 1.377946, acc.: 47.50%] [G loss: 11.962705]\n",
      "259 [D loss: 1.687132, acc.: 47.50%] [G loss: 10.643995]\n",
      "260 [D loss: 1.305847, acc.: 50.00%] [G loss: 10.442602]\n",
      "261 [D loss: 1.710605, acc.: 45.00%] [G loss: 9.264250]\n",
      "262 [D loss: 1.345998, acc.: 45.00%] [G loss: 11.960135]\n",
      "263 [D loss: 1.345755, acc.: 47.50%] [G loss: 9.089729]\n",
      "264 [D loss: 1.364241, acc.: 50.00%] [G loss: 8.521322]\n",
      "265 [D loss: 1.333144, acc.: 47.50%] [G loss: 8.192312]\n",
      "266 [D loss: 1.330218, acc.: 47.50%] [G loss: 13.163420]\n",
      "267 [D loss: 1.304427, acc.: 50.00%] [G loss: 8.260803]\n",
      "268 [D loss: 1.317730, acc.: 50.00%] [G loss: 9.213689]\n",
      "269 [D loss: 1.319271, acc.: 50.00%] [G loss: 14.188293]\n",
      "270 [D loss: 1.495945, acc.: 47.50%] [G loss: 10.790967]\n",
      "271 [D loss: 1.338600, acc.: 47.50%] [G loss: 12.120390]\n",
      "272 [D loss: 1.302273, acc.: 50.00%] [G loss: 9.884595]\n",
      "273 [D loss: 1.692043, acc.: 42.50%] [G loss: 9.745272]\n",
      "274 [D loss: 1.330908, acc.: 45.00%] [G loss: 9.278171]\n",
      "275 [D loss: 1.301590, acc.: 50.00%] [G loss: 9.056523]\n",
      "276 [D loss: 1.334546, acc.: 50.00%] [G loss: 10.434153]\n",
      "277 [D loss: 1.288204, acc.: 50.00%] [G loss: 10.488193]\n",
      "278 [D loss: 1.307243, acc.: 47.50%] [G loss: 8.902055]\n",
      "279 [D loss: 1.653460, acc.: 47.50%] [G loss: 9.908616]\n",
      "280 [D loss: 1.384421, acc.: 47.50%] [G loss: 9.017447]\n",
      "281 [D loss: 1.336501, acc.: 50.00%] [G loss: 9.815142]\n",
      "282 [D loss: 1.380408, acc.: 42.50%] [G loss: 11.369207]\n",
      "283 [D loss: 1.259179, acc.: 50.00%] [G loss: 8.378018]\n",
      "284 [D loss: 1.289155, acc.: 45.00%] [G loss: 10.162335]\n",
      "285 [D loss: 1.326302, acc.: 47.50%] [G loss: 11.220126]\n",
      "286 [D loss: 1.332019, acc.: 50.00%] [G loss: 9.620260]\n",
      "287 [D loss: 1.267677, acc.: 50.00%] [G loss: 11.891119]\n",
      "288 [D loss: 1.371060, acc.: 42.50%] [G loss: 9.053656]\n",
      "289 [D loss: 1.348337, acc.: 45.00%] [G loss: 9.886826]\n",
      "290 [D loss: 1.363872, acc.: 42.50%] [G loss: 10.124863]\n",
      "291 [D loss: 1.323297, acc.: 47.50%] [G loss: 10.504918]\n",
      "292 [D loss: 1.330537, acc.: 47.50%] [G loss: 12.708536]\n",
      "293 [D loss: 1.267911, acc.: 50.00%] [G loss: 11.046431]\n",
      "294 [D loss: 1.364609, acc.: 45.00%] [G loss: 10.039083]\n",
      "295 [D loss: 1.239172, acc.: 50.00%] [G loss: 12.714867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 [D loss: 1.253087, acc.: 50.00%] [G loss: 8.975397]\n",
      "297 [D loss: 1.650115, acc.: 45.00%] [G loss: 10.571669]\n",
      "298 [D loss: 1.326794, acc.: 47.50%] [G loss: 9.988840]\n",
      "299 [D loss: 1.289349, acc.: 47.50%] [G loss: 10.782104]\n",
      "300 [D loss: 1.306353, acc.: 47.50%] [G loss: 8.518938]\n",
      "301 [D loss: 1.252215, acc.: 47.50%] [G loss: 10.510282]\n",
      "302 [D loss: 1.243052, acc.: 47.50%] [G loss: 9.982672]\n",
      "303 [D loss: 1.251691, acc.: 50.00%] [G loss: 9.655444]\n",
      "304 [D loss: 1.642708, acc.: 45.00%] [G loss: 8.209069]\n",
      "305 [D loss: 1.256802, acc.: 47.50%] [G loss: 9.071614]\n",
      "306 [D loss: 1.259689, acc.: 47.50%] [G loss: 9.197041]\n",
      "307 [D loss: 1.274942, acc.: 50.00%] [G loss: 12.020796]\n",
      "308 [D loss: 1.254959, acc.: 50.00%] [G loss: 9.011586]\n",
      "309 [D loss: 1.334053, acc.: 50.00%] [G loss: 11.662203]\n",
      "310 [D loss: 1.339373, acc.: 45.00%] [G loss: 12.016088]\n",
      "311 [D loss: 1.267341, acc.: 50.00%] [G loss: 11.210583]\n",
      "312 [D loss: 1.312096, acc.: 42.50%] [G loss: 10.623324]\n",
      "313 [D loss: 1.197575, acc.: 50.00%] [G loss: 10.593254]\n",
      "314 [D loss: 1.276205, acc.: 50.00%] [G loss: 8.485391]\n",
      "315 [D loss: 1.276897, acc.: 45.00%] [G loss: 9.303015]\n",
      "316 [D loss: 1.240192, acc.: 50.00%] [G loss: 11.868532]\n",
      "317 [D loss: 1.244889, acc.: 50.00%] [G loss: 11.189195]\n",
      "318 [D loss: 1.257238, acc.: 47.50%] [G loss: 10.521549]\n",
      "319 [D loss: 1.389364, acc.: 40.00%] [G loss: 9.121561]\n",
      "320 [D loss: 1.236428, acc.: 50.00%] [G loss: 10.526510]\n",
      "321 [D loss: 1.259802, acc.: 50.00%] [G loss: 10.640448]\n",
      "322 [D loss: 1.316327, acc.: 47.50%] [G loss: 11.784967]\n",
      "323 [D loss: 1.288049, acc.: 50.00%] [G loss: 9.905267]\n",
      "324 [D loss: 1.293642, acc.: 45.00%] [G loss: 13.289850]\n",
      "325 [D loss: 1.271382, acc.: 50.00%] [G loss: 9.097158]\n",
      "326 [D loss: 1.262499, acc.: 50.00%] [G loss: 9.309277]\n",
      "327 [D loss: 1.237849, acc.: 47.50%] [G loss: 11.979559]\n",
      "328 [D loss: 1.241900, acc.: 47.50%] [G loss: 9.902097]\n",
      "329 [D loss: 1.244408, acc.: 45.00%] [G loss: 11.123701]\n",
      "330 [D loss: 1.231407, acc.: 50.00%] [G loss: 9.577164]\n",
      "331 [D loss: 1.224222, acc.: 47.50%] [G loss: 9.955253]\n",
      "332 [D loss: 1.170563, acc.: 50.00%] [G loss: 10.425878]\n",
      "333 [D loss: 1.233720, acc.: 50.00%] [G loss: 9.917175]\n",
      "334 [D loss: 1.188693, acc.: 50.00%] [G loss: 8.537237]\n",
      "335 [D loss: 1.259332, acc.: 50.00%] [G loss: 12.390423]\n",
      "336 [D loss: 1.273903, acc.: 47.50%] [G loss: 10.791295]\n",
      "337 [D loss: 1.227544, acc.: 50.00%] [G loss: 7.783809]\n",
      "338 [D loss: 1.218870, acc.: 50.00%] [G loss: 7.895358]\n",
      "339 [D loss: 1.193925, acc.: 47.50%] [G loss: 10.786122]\n",
      "340 [D loss: 1.251150, acc.: 47.50%] [G loss: 9.251376]\n",
      "341 [D loss: 1.262480, acc.: 47.50%] [G loss: 9.290621]\n",
      "342 [D loss: 1.147555, acc.: 47.50%] [G loss: 11.479482]\n",
      "343 [D loss: 1.202922, acc.: 50.00%] [G loss: 9.957544]\n",
      "344 [D loss: 1.269591, acc.: 50.00%] [G loss: 11.262308]\n",
      "345 [D loss: 1.252491, acc.: 50.00%] [G loss: 9.656163]\n",
      "346 [D loss: 1.261707, acc.: 47.50%] [G loss: 9.407534]\n",
      "347 [D loss: 1.205740, acc.: 50.00%] [G loss: 9.729849]\n",
      "348 [D loss: 1.220356, acc.: 50.00%] [G loss: 11.361166]\n",
      "349 [D loss: 1.190081, acc.: 50.00%] [G loss: 12.135875]\n",
      "350 [D loss: 1.286126, acc.: 50.00%] [G loss: 12.731078]\n",
      "351 [D loss: 1.160297, acc.: 50.00%] [G loss: 9.621282]\n",
      "352 [D loss: 1.190715, acc.: 45.00%] [G loss: 11.971260]\n",
      "353 [D loss: 1.579595, acc.: 45.00%] [G loss: 10.177277]\n",
      "354 [D loss: 1.559682, acc.: 42.50%] [G loss: 8.904095]\n",
      "355 [D loss: 1.226383, acc.: 50.00%] [G loss: 8.462851]\n",
      "356 [D loss: 1.180166, acc.: 47.50%] [G loss: 11.879389]\n",
      "357 [D loss: 1.163190, acc.: 50.00%] [G loss: 11.954016]\n",
      "358 [D loss: 1.146719, acc.: 50.00%] [G loss: 12.010633]\n",
      "359 [D loss: 1.199741, acc.: 47.50%] [G loss: 10.831596]\n",
      "360 [D loss: 1.203932, acc.: 50.00%] [G loss: 12.821722]\n",
      "361 [D loss: 1.208347, acc.: 50.00%] [G loss: 12.463945]\n",
      "362 [D loss: 1.572231, acc.: 47.50%] [G loss: 11.195379]\n",
      "363 [D loss: 1.182137, acc.: 47.50%] [G loss: 10.461224]\n",
      "364 [D loss: 1.127207, acc.: 50.00%] [G loss: 12.193463]\n",
      "365 [D loss: 1.202473, acc.: 50.00%] [G loss: 9.764822]\n",
      "366 [D loss: 1.248488, acc.: 50.00%] [G loss: 11.877520]\n",
      "367 [D loss: 1.145352, acc.: 50.00%] [G loss: 10.476267]\n",
      "368 [D loss: 1.317110, acc.: 45.00%] [G loss: 13.306112]\n",
      "369 [D loss: 1.182159, acc.: 47.50%] [G loss: 9.340693]\n",
      "370 [D loss: 1.179364, acc.: 50.00%] [G loss: 11.775385]\n",
      "371 [D loss: 1.099648, acc.: 50.00%] [G loss: 11.440173]\n",
      "372 [D loss: 1.151876, acc.: 50.00%] [G loss: 10.610336]\n",
      "373 [D loss: 1.152417, acc.: 45.00%] [G loss: 11.129023]\n",
      "374 [D loss: 1.143022, acc.: 50.00%] [G loss: 7.881944]\n",
      "375 [D loss: 1.158698, acc.: 47.50%] [G loss: 11.789154]\n",
      "376 [D loss: 1.160094, acc.: 50.00%] [G loss: 11.885269]\n",
      "377 [D loss: 1.144084, acc.: 50.00%] [G loss: 11.989116]\n",
      "378 [D loss: 1.152465, acc.: 50.00%] [G loss: 9.491858]\n",
      "379 [D loss: 1.191709, acc.: 47.50%] [G loss: 10.020365]\n",
      "380 [D loss: 1.148728, acc.: 50.00%] [G loss: 12.547507]\n",
      "381 [D loss: 1.155229, acc.: 50.00%] [G loss: 11.374640]\n",
      "382 [D loss: 1.182495, acc.: 50.00%] [G loss: 12.122881]\n",
      "383 [D loss: 1.139238, acc.: 50.00%] [G loss: 11.903725]\n",
      "384 [D loss: 1.139113, acc.: 47.50%] [G loss: 11.426727]\n",
      "385 [D loss: 1.102820, acc.: 50.00%] [G loss: 8.986517]\n",
      "386 [D loss: 1.180842, acc.: 47.50%] [G loss: 9.286062]\n",
      "387 [D loss: 1.125029, acc.: 50.00%] [G loss: 9.277712]\n",
      "388 [D loss: 1.127269, acc.: 50.00%] [G loss: 10.441632]\n",
      "389 [D loss: 1.125062, acc.: 50.00%] [G loss: 7.702237]\n",
      "390 [D loss: 1.086298, acc.: 50.00%] [G loss: 10.305830]\n",
      "391 [D loss: 1.136837, acc.: 50.00%] [G loss: 11.436411]\n",
      "392 [D loss: 1.148483, acc.: 50.00%] [G loss: 11.537860]\n",
      "393 [D loss: 1.088286, acc.: 50.00%] [G loss: 9.210480]\n",
      "394 [D loss: 1.141801, acc.: 50.00%] [G loss: 11.416944]\n",
      "395 [D loss: 1.151054, acc.: 47.50%] [G loss: 13.410913]\n",
      "396 [D loss: 1.209839, acc.: 45.00%] [G loss: 10.470133]\n",
      "397 [D loss: 1.119973, acc.: 50.00%] [G loss: 9.987713]\n",
      "398 [D loss: 1.201570, acc.: 50.00%] [G loss: 11.326427]\n",
      "399 [D loss: 1.130622, acc.: 50.00%] [G loss: 8.408949]\n",
      "400 [D loss: 1.147978, acc.: 50.00%] [G loss: 9.613243]\n",
      "401 [D loss: 1.097790, acc.: 50.00%] [G loss: 9.390927]\n",
      "402 [D loss: 1.509581, acc.: 45.00%] [G loss: 11.263835]\n",
      "403 [D loss: 1.147169, acc.: 47.50%] [G loss: 11.331845]\n",
      "404 [D loss: 1.154414, acc.: 47.50%] [G loss: 9.219272]\n",
      "405 [D loss: 1.173710, acc.: 47.50%] [G loss: 12.582883]\n",
      "406 [D loss: 1.144161, acc.: 45.00%] [G loss: 12.184052]\n",
      "407 [D loss: 1.114159, acc.: 50.00%] [G loss: 12.003487]\n",
      "408 [D loss: 1.081682, acc.: 50.00%] [G loss: 10.468117]\n",
      "409 [D loss: 1.183808, acc.: 47.50%] [G loss: 8.405617]\n",
      "410 [D loss: 1.153826, acc.: 47.50%] [G loss: 12.026127]\n",
      "411 [D loss: 1.118307, acc.: 50.00%] [G loss: 10.454292]\n",
      "412 [D loss: 1.146390, acc.: 50.00%] [G loss: 11.330727]\n",
      "413 [D loss: 1.148488, acc.: 50.00%] [G loss: 11.992238]\n",
      "414 [D loss: 1.079853, acc.: 50.00%] [G loss: 9.070711]\n",
      "415 [D loss: 1.155418, acc.: 45.00%] [G loss: 9.128276]\n",
      "416 [D loss: 1.164948, acc.: 47.50%] [G loss: 8.874161]\n",
      "417 [D loss: 1.143393, acc.: 47.50%] [G loss: 9.900378]\n",
      "418 [D loss: 1.106820, acc.: 50.00%] [G loss: 7.744681]\n",
      "419 [D loss: 1.134005, acc.: 50.00%] [G loss: 10.468056]\n",
      "420 [D loss: 1.565387, acc.: 45.00%] [G loss: 7.906853]\n",
      "421 [D loss: 1.097467, acc.: 50.00%] [G loss: 9.298393]\n",
      "422 [D loss: 1.148643, acc.: 47.50%] [G loss: 9.964867]\n",
      "423 [D loss: 1.118268, acc.: 50.00%] [G loss: 8.458673]\n",
      "424 [D loss: 1.096514, acc.: 50.00%] [G loss: 10.598186]\n",
      "425 [D loss: 1.110277, acc.: 50.00%] [G loss: 10.902722]\n",
      "426 [D loss: 1.117841, acc.: 47.50%] [G loss: 9.945944]\n",
      "427 [D loss: 1.169867, acc.: 50.00%] [G loss: 10.141734]\n",
      "428 [D loss: 1.103668, acc.: 50.00%] [G loss: 8.999024]\n",
      "429 [D loss: 1.129593, acc.: 47.50%] [G loss: 10.211943]\n",
      "430 [D loss: 1.096778, acc.: 50.00%] [G loss: 8.393523]\n",
      "431 [D loss: 1.168231, acc.: 50.00%] [G loss: 11.192026]\n",
      "432 [D loss: 1.122170, acc.: 47.50%] [G loss: 9.846756]\n",
      "433 [D loss: 1.099949, acc.: 50.00%] [G loss: 10.770659]\n",
      "434 [D loss: 1.094397, acc.: 50.00%] [G loss: 9.832832]\n",
      "435 [D loss: 1.196647, acc.: 50.00%] [G loss: 8.526507]\n",
      "436 [D loss: 1.116242, acc.: 47.50%] [G loss: 8.041674]\n",
      "437 [D loss: 1.066112, acc.: 50.00%] [G loss: 11.297623]\n",
      "438 [D loss: 1.126604, acc.: 47.50%] [G loss: 10.865724]\n",
      "439 [D loss: 1.097442, acc.: 50.00%] [G loss: 12.577986]\n",
      "440 [D loss: 1.035025, acc.: 50.00%] [G loss: 10.029078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441 [D loss: 1.091241, acc.: 50.00%] [G loss: 7.300601]\n",
      "442 [D loss: 1.071594, acc.: 50.00%] [G loss: 10.554621]\n",
      "443 [D loss: 1.077555, acc.: 50.00%] [G loss: 12.782131]\n",
      "444 [D loss: 1.115425, acc.: 50.00%] [G loss: 13.341403]\n",
      "445 [D loss: 1.065237, acc.: 50.00%] [G loss: 12.663264]\n",
      "446 [D loss: 1.073748, acc.: 50.00%] [G loss: 11.395694]\n",
      "447 [D loss: 1.069370, acc.: 50.00%] [G loss: 12.512816]\n",
      "448 [D loss: 1.050629, acc.: 50.00%] [G loss: 10.750551]\n",
      "449 [D loss: 1.017184, acc.: 50.00%] [G loss: 13.354121]\n",
      "450 [D loss: 1.089700, acc.: 50.00%] [G loss: 8.820297]\n",
      "451 [D loss: 1.030349, acc.: 50.00%] [G loss: 10.709048]\n",
      "452 [D loss: 1.044129, acc.: 50.00%] [G loss: 8.547972]\n",
      "453 [D loss: 1.038156, acc.: 50.00%] [G loss: 9.911764]\n",
      "454 [D loss: 1.039884, acc.: 50.00%] [G loss: 9.836863]\n",
      "455 [D loss: 1.030546, acc.: 50.00%] [G loss: 10.075068]\n",
      "456 [D loss: 1.032762, acc.: 47.50%] [G loss: 11.043190]\n",
      "457 [D loss: 1.111686, acc.: 50.00%] [G loss: 9.967688]\n",
      "458 [D loss: 1.029902, acc.: 50.00%] [G loss: 12.013656]\n",
      "459 [D loss: 1.035098, acc.: 50.00%] [G loss: 13.583043]\n",
      "460 [D loss: 1.017565, acc.: 50.00%] [G loss: 12.137598]\n",
      "461 [D loss: 1.039261, acc.: 50.00%] [G loss: 11.950705]\n",
      "462 [D loss: 1.083320, acc.: 50.00%] [G loss: 10.622037]\n",
      "463 [D loss: 1.096452, acc.: 50.00%] [G loss: 12.691866]\n",
      "464 [D loss: 1.046904, acc.: 50.00%] [G loss: 9.788549]\n",
      "465 [D loss: 1.056414, acc.: 50.00%] [G loss: 10.534223]\n",
      "466 [D loss: 1.077403, acc.: 50.00%] [G loss: 11.396739]\n",
      "467 [D loss: 1.041631, acc.: 50.00%] [G loss: 8.332645]\n",
      "468 [D loss: 1.085945, acc.: 50.00%] [G loss: 12.130114]\n",
      "469 [D loss: 1.076545, acc.: 50.00%] [G loss: 11.356627]\n",
      "470 [D loss: 0.993629, acc.: 50.00%] [G loss: 12.716046]\n",
      "471 [D loss: 1.076071, acc.: 50.00%] [G loss: 9.900944]\n",
      "472 [D loss: 1.035119, acc.: 50.00%] [G loss: 10.677444]\n",
      "473 [D loss: 1.044066, acc.: 50.00%] [G loss: 10.677403]\n",
      "474 [D loss: 1.060493, acc.: 50.00%] [G loss: 9.357244]\n",
      "475 [D loss: 1.045845, acc.: 50.00%] [G loss: 9.891505]\n",
      "476 [D loss: 1.015657, acc.: 50.00%] [G loss: 11.767885]\n",
      "477 [D loss: 1.019314, acc.: 50.00%] [G loss: 12.638990]\n",
      "478 [D loss: 1.027973, acc.: 50.00%] [G loss: 8.525254]\n",
      "479 [D loss: 1.045006, acc.: 50.00%] [G loss: 9.991963]\n",
      "480 [D loss: 1.063290, acc.: 50.00%] [G loss: 10.010371]\n",
      "481 [D loss: 1.034497, acc.: 50.00%] [G loss: 8.833219]\n",
      "482 [D loss: 1.029401, acc.: 47.50%] [G loss: 10.783979]\n",
      "483 [D loss: 1.066855, acc.: 50.00%] [G loss: 9.985591]\n",
      "484 [D loss: 0.990799, acc.: 50.00%] [G loss: 10.714814]\n",
      "485 [D loss: 0.986046, acc.: 50.00%] [G loss: 10.595282]\n",
      "486 [D loss: 1.043194, acc.: 50.00%] [G loss: 11.853717]\n",
      "487 [D loss: 1.060176, acc.: 50.00%] [G loss: 9.420495]\n",
      "488 [D loss: 1.073614, acc.: 50.00%] [G loss: 8.196695]\n",
      "489 [D loss: 1.045367, acc.: 50.00%] [G loss: 11.938868]\n",
      "490 [D loss: 1.069366, acc.: 47.50%] [G loss: 10.679912]\n",
      "491 [D loss: 1.041509, acc.: 50.00%] [G loss: 11.367803]\n",
      "492 [D loss: 0.972967, acc.: 50.00%] [G loss: 12.627802]\n",
      "493 [D loss: 1.029636, acc.: 50.00%] [G loss: 12.641829]\n",
      "494 [D loss: 0.992213, acc.: 50.00%] [G loss: 11.613719]\n",
      "495 [D loss: 1.006692, acc.: 50.00%] [G loss: 9.865493]\n",
      "496 [D loss: 0.994293, acc.: 50.00%] [G loss: 11.152843]\n",
      "497 [D loss: 1.001868, acc.: 50.00%] [G loss: 12.688988]\n",
      "498 [D loss: 1.026800, acc.: 50.00%] [G loss: 11.180907]\n",
      "499 [D loss: 1.015565, acc.: 50.00%] [G loss: 8.226311]\n",
      "500 [D loss: 1.059012, acc.: 50.00%] [G loss: 10.566031]\n",
      "501 [D loss: 0.971891, acc.: 50.00%] [G loss: 10.307692]\n",
      "502 [D loss: 1.051698, acc.: 50.00%] [G loss: 8.806940]\n",
      "503 [D loss: 1.003401, acc.: 50.00%] [G loss: 10.753954]\n",
      "504 [D loss: 1.069119, acc.: 50.00%] [G loss: 11.434222]\n",
      "505 [D loss: 1.034453, acc.: 50.00%] [G loss: 10.401285]\n",
      "506 [D loss: 1.032232, acc.: 50.00%] [G loss: 11.537807]\n",
      "507 [D loss: 0.990221, acc.: 50.00%] [G loss: 11.194580]\n",
      "508 [D loss: 1.023618, acc.: 50.00%] [G loss: 11.222929]\n",
      "509 [D loss: 1.003469, acc.: 47.50%] [G loss: 5.876655]\n",
      "510 [D loss: 0.957853, acc.: 50.00%] [G loss: 8.503947]\n",
      "511 [D loss: 1.069369, acc.: 50.00%] [G loss: 11.894506]\n",
      "512 [D loss: 1.017025, acc.: 50.00%] [G loss: 7.507231]\n",
      "513 [D loss: 1.020879, acc.: 50.00%] [G loss: 5.397188]\n",
      "514 [D loss: 1.051719, acc.: 47.50%] [G loss: 8.565524]\n",
      "515 [D loss: 1.022225, acc.: 50.00%] [G loss: 12.174219]\n",
      "516 [D loss: 1.017027, acc.: 50.00%] [G loss: 9.924097]\n",
      "517 [D loss: 1.056058, acc.: 50.00%] [G loss: 10.926229]\n",
      "518 [D loss: 1.035172, acc.: 50.00%] [G loss: 11.427141]\n",
      "519 [D loss: 1.071941, acc.: 50.00%] [G loss: 7.710490]\n",
      "520 [D loss: 1.070520, acc.: 47.50%] [G loss: 9.103525]\n",
      "521 [D loss: 0.992610, acc.: 50.00%] [G loss: 9.353745]\n",
      "522 [D loss: 1.060156, acc.: 50.00%] [G loss: 9.185860]\n",
      "523 [D loss: 0.975623, acc.: 50.00%] [G loss: 10.381506]\n",
      "524 [D loss: 0.966136, acc.: 50.00%] [G loss: 10.801472]\n",
      "525 [D loss: 0.981104, acc.: 50.00%] [G loss: 8.754469]\n",
      "526 [D loss: 1.010846, acc.: 50.00%] [G loss: 7.895094]\n",
      "527 [D loss: 0.976383, acc.: 50.00%] [G loss: 9.471874]\n",
      "528 [D loss: 0.942231, acc.: 50.00%] [G loss: 8.273950]\n",
      "529 [D loss: 1.037689, acc.: 50.00%] [G loss: 8.448549]\n",
      "530 [D loss: 1.037824, acc.: 50.00%] [G loss: 12.658754]\n",
      "531 [D loss: 1.076655, acc.: 50.00%] [G loss: 12.008962]\n",
      "532 [D loss: 1.007261, acc.: 50.00%] [G loss: 9.418305]\n",
      "533 [D loss: 1.061994, acc.: 47.50%] [G loss: 11.867715]\n",
      "534 [D loss: 0.996258, acc.: 50.00%] [G loss: 9.710791]\n",
      "535 [D loss: 0.964398, acc.: 50.00%] [G loss: 9.269755]\n",
      "536 [D loss: 0.995322, acc.: 50.00%] [G loss: 8.126529]\n",
      "537 [D loss: 0.953062, acc.: 50.00%] [G loss: 8.113955]\n",
      "538 [D loss: 1.046164, acc.: 50.00%] [G loss: 10.711071]\n",
      "539 [D loss: 1.051118, acc.: 50.00%] [G loss: 10.601972]\n",
      "540 [D loss: 1.025756, acc.: 50.00%] [G loss: 10.279280]\n",
      "541 [D loss: 1.009676, acc.: 50.00%] [G loss: 9.454242]\n",
      "542 [D loss: 1.028556, acc.: 50.00%] [G loss: 9.504089]\n",
      "543 [D loss: 0.987419, acc.: 50.00%] [G loss: 11.306281]\n",
      "544 [D loss: 0.995520, acc.: 50.00%] [G loss: 10.578621]\n",
      "545 [D loss: 0.991680, acc.: 50.00%] [G loss: 9.691323]\n",
      "546 [D loss: 0.978128, acc.: 50.00%] [G loss: 9.453773]\n",
      "547 [D loss: 1.012252, acc.: 47.50%] [G loss: 6.684369]\n",
      "548 [D loss: 0.969110, acc.: 50.00%] [G loss: 7.652539]\n",
      "549 [D loss: 0.954346, acc.: 50.00%] [G loss: 9.627365]\n",
      "550 [D loss: 1.045274, acc.: 50.00%] [G loss: 7.511332]\n",
      "551 [D loss: 0.952119, acc.: 50.00%] [G loss: 12.102018]\n",
      "552 [D loss: 0.968466, acc.: 50.00%] [G loss: 8.571302]\n",
      "553 [D loss: 0.984707, acc.: 50.00%] [G loss: 7.485400]\n",
      "554 [D loss: 0.942974, acc.: 50.00%] [G loss: 8.834186]\n",
      "555 [D loss: 1.049679, acc.: 50.00%] [G loss: 11.436681]\n",
      "556 [D loss: 1.008546, acc.: 50.00%] [G loss: 10.048281]\n",
      "557 [D loss: 0.985216, acc.: 50.00%] [G loss: 8.557093]\n",
      "558 [D loss: 0.933777, acc.: 50.00%] [G loss: 10.602377]\n",
      "559 [D loss: 0.975148, acc.: 50.00%] [G loss: 9.399181]\n",
      "560 [D loss: 0.959985, acc.: 50.00%] [G loss: 9.270521]\n",
      "561 [D loss: 1.032152, acc.: 50.00%] [G loss: 9.984198]\n",
      "562 [D loss: 1.031217, acc.: 50.00%] [G loss: 8.505601]\n",
      "563 [D loss: 0.943450, acc.: 50.00%] [G loss: 9.286674]\n",
      "564 [D loss: 1.039304, acc.: 50.00%] [G loss: 8.405320]\n",
      "565 [D loss: 0.942918, acc.: 50.00%] [G loss: 7.022523]\n",
      "566 [D loss: 0.980250, acc.: 50.00%] [G loss: 12.237707]\n",
      "567 [D loss: 0.990958, acc.: 50.00%] [G loss: 12.784164]\n",
      "568 [D loss: 0.987934, acc.: 50.00%] [G loss: 8.519190]\n",
      "569 [D loss: 0.932025, acc.: 50.00%] [G loss: 8.165282]\n",
      "570 [D loss: 1.004342, acc.: 50.00%] [G loss: 12.080828]\n",
      "571 [D loss: 1.012553, acc.: 50.00%] [G loss: 6.641927]\n",
      "572 [D loss: 0.999731, acc.: 50.00%] [G loss: 9.112538]\n",
      "573 [D loss: 0.893294, acc.: 50.00%] [G loss: 6.740867]\n",
      "574 [D loss: 0.969282, acc.: 50.00%] [G loss: 9.459879]\n",
      "575 [D loss: 0.940207, acc.: 50.00%] [G loss: 7.291453]\n",
      "576 [D loss: 0.955183, acc.: 50.00%] [G loss: 10.696878]\n",
      "577 [D loss: 0.895843, acc.: 50.00%] [G loss: 11.869476]\n",
      "578 [D loss: 0.909811, acc.: 50.00%] [G loss: 7.156378]\n",
      "579 [D loss: 0.954317, acc.: 50.00%] [G loss: 10.776818]\n",
      "580 [D loss: 0.980683, acc.: 50.00%] [G loss: 9.697525]\n",
      "581 [D loss: 0.944249, acc.: 50.00%] [G loss: 7.901025]\n",
      "582 [D loss: 0.921451, acc.: 50.00%] [G loss: 10.004389]\n",
      "583 [D loss: 0.914251, acc.: 50.00%] [G loss: 11.327250]\n",
      "584 [D loss: 0.943037, acc.: 50.00%] [G loss: 8.854015]\n",
      "585 [D loss: 0.949359, acc.: 50.00%] [G loss: 11.287602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586 [D loss: 0.941623, acc.: 50.00%] [G loss: 9.910881]\n",
      "587 [D loss: 0.972920, acc.: 50.00%] [G loss: 9.239263]\n",
      "588 [D loss: 0.970980, acc.: 50.00%] [G loss: 9.849937]\n",
      "589 [D loss: 0.916255, acc.: 50.00%] [G loss: 9.335128]\n",
      "590 [D loss: 1.015603, acc.: 50.00%] [G loss: 7.012403]\n",
      "591 [D loss: 0.948860, acc.: 50.00%] [G loss: 11.144046]\n",
      "592 [D loss: 0.995781, acc.: 50.00%] [G loss: 8.428685]\n",
      "593 [D loss: 0.982192, acc.: 50.00%] [G loss: 10.600233]\n",
      "594 [D loss: 0.924894, acc.: 50.00%] [G loss: 9.729595]\n",
      "595 [D loss: 0.927971, acc.: 50.00%] [G loss: 10.209945]\n",
      "596 [D loss: 0.985325, acc.: 50.00%] [G loss: 8.576906]\n",
      "597 [D loss: 0.974042, acc.: 50.00%] [G loss: 8.515022]\n",
      "598 [D loss: 0.922827, acc.: 50.00%] [G loss: 9.216127]\n",
      "599 [D loss: 0.937426, acc.: 50.00%] [G loss: 8.488338]\n",
      "600 [D loss: 0.932842, acc.: 50.00%] [G loss: 8.404137]\n",
      "601 [D loss: 0.926636, acc.: 50.00%] [G loss: 8.691810]\n",
      "602 [D loss: 0.907377, acc.: 50.00%] [G loss: 9.396489]\n",
      "603 [D loss: 0.907972, acc.: 50.00%] [G loss: 9.300073]\n",
      "604 [D loss: 0.875163, acc.: 50.00%] [G loss: 10.155545]\n",
      "605 [D loss: 0.904528, acc.: 50.00%] [G loss: 8.706777]\n",
      "606 [D loss: 0.896824, acc.: 50.00%] [G loss: 9.812072]\n",
      "607 [D loss: 0.918679, acc.: 50.00%] [G loss: 9.244287]\n",
      "608 [D loss: 0.911405, acc.: 50.00%] [G loss: 11.300593]\n",
      "609 [D loss: 0.888336, acc.: 50.00%] [G loss: 10.217299]\n",
      "610 [D loss: 0.968857, acc.: 50.00%] [G loss: 10.097830]\n",
      "611 [D loss: 0.921645, acc.: 50.00%] [G loss: 10.871972]\n",
      "612 [D loss: 0.930021, acc.: 50.00%] [G loss: 8.622302]\n",
      "613 [D loss: 0.927430, acc.: 50.00%] [G loss: 11.391226]\n",
      "614 [D loss: 0.898340, acc.: 50.00%] [G loss: 10.624464]\n",
      "615 [D loss: 0.933893, acc.: 50.00%] [G loss: 8.551274]\n",
      "616 [D loss: 0.916333, acc.: 50.00%] [G loss: 9.327233]\n",
      "617 [D loss: 0.920553, acc.: 50.00%] [G loss: 8.966822]\n",
      "618 [D loss: 0.932222, acc.: 50.00%] [G loss: 8.352039]\n",
      "619 [D loss: 0.900567, acc.: 50.00%] [G loss: 5.945535]\n",
      "620 [D loss: 0.872069, acc.: 50.00%] [G loss: 8.354971]\n",
      "621 [D loss: 0.870064, acc.: 50.00%] [G loss: 9.205696]\n",
      "622 [D loss: 0.907947, acc.: 50.00%] [G loss: 7.957074]\n",
      "623 [D loss: 0.936620, acc.: 50.00%] [G loss: 8.247400]\n",
      "624 [D loss: 0.935453, acc.: 50.00%] [G loss: 10.596114]\n",
      "625 [D loss: 0.905585, acc.: 50.00%] [G loss: 8.729630]\n",
      "626 [D loss: 0.881583, acc.: 50.00%] [G loss: 9.719367]\n",
      "627 [D loss: 0.959035, acc.: 50.00%] [G loss: 7.063952]\n",
      "628 [D loss: 0.927931, acc.: 50.00%] [G loss: 7.725255]\n",
      "629 [D loss: 0.906077, acc.: 50.00%] [G loss: 9.571950]\n",
      "630 [D loss: 0.930946, acc.: 50.00%] [G loss: 8.958379]\n",
      "631 [D loss: 0.883357, acc.: 50.00%] [G loss: 9.227674]\n",
      "632 [D loss: 0.905906, acc.: 50.00%] [G loss: 8.514304]\n",
      "633 [D loss: 0.902679, acc.: 50.00%] [G loss: 6.425965]\n",
      "634 [D loss: 0.894297, acc.: 50.00%] [G loss: 6.899388]\n",
      "635 [D loss: 0.901909, acc.: 50.00%] [G loss: 7.682741]\n",
      "636 [D loss: 0.913702, acc.: 50.00%] [G loss: 10.668088]\n",
      "637 [D loss: 0.925422, acc.: 50.00%] [G loss: 7.942420]\n",
      "638 [D loss: 0.920211, acc.: 47.50%] [G loss: 6.959120]\n",
      "639 [D loss: 0.869953, acc.: 50.00%] [G loss: 8.977895]\n",
      "640 [D loss: 0.867377, acc.: 50.00%] [G loss: 8.512446]\n",
      "641 [D loss: 0.952106, acc.: 50.00%] [G loss: 7.336526]\n",
      "642 [D loss: 0.930827, acc.: 50.00%] [G loss: 8.600205]\n",
      "643 [D loss: 0.900225, acc.: 50.00%] [G loss: 9.221518]\n",
      "644 [D loss: 0.899547, acc.: 50.00%] [G loss: 9.250532]\n",
      "645 [D loss: 0.884368, acc.: 50.00%] [G loss: 8.639124]\n",
      "646 [D loss: 0.952935, acc.: 50.00%] [G loss: 9.625051]\n",
      "647 [D loss: 0.877284, acc.: 50.00%] [G loss: 6.991469]\n",
      "648 [D loss: 0.899784, acc.: 50.00%] [G loss: 5.710557]\n",
      "649 [D loss: 0.862911, acc.: 50.00%] [G loss: 9.740999]\n",
      "650 [D loss: 0.883949, acc.: 50.00%] [G loss: 8.638023]\n",
      "651 [D loss: 0.827322, acc.: 50.00%] [G loss: 8.414751]\n",
      "652 [D loss: 0.929147, acc.: 50.00%] [G loss: 9.657644]\n",
      "653 [D loss: 0.968594, acc.: 50.00%] [G loss: 9.496058]\n",
      "654 [D loss: 0.901048, acc.: 50.00%] [G loss: 7.498339]\n",
      "655 [D loss: 0.929357, acc.: 50.00%] [G loss: 7.524815]\n",
      "656 [D loss: 0.852113, acc.: 50.00%] [G loss: 9.102054]\n",
      "657 [D loss: 0.879321, acc.: 50.00%] [G loss: 6.834174]\n",
      "658 [D loss: 0.928404, acc.: 50.00%] [G loss: 8.260124]\n",
      "659 [D loss: 0.867880, acc.: 50.00%] [G loss: 9.182817]\n",
      "660 [D loss: 0.880857, acc.: 50.00%] [G loss: 7.760135]\n",
      "661 [D loss: 0.896963, acc.: 50.00%] [G loss: 8.585347]\n",
      "662 [D loss: 0.899520, acc.: 50.00%] [G loss: 6.487650]\n",
      "663 [D loss: 0.868533, acc.: 50.00%] [G loss: 12.158622]\n",
      "664 [D loss: 0.884543, acc.: 50.00%] [G loss: 8.509277]\n",
      "665 [D loss: 0.918662, acc.: 50.00%] [G loss: 11.188289]\n",
      "666 [D loss: 0.873091, acc.: 50.00%] [G loss: 8.258267]\n",
      "667 [D loss: 0.899743, acc.: 50.00%] [G loss: 10.047521]\n",
      "668 [D loss: 0.863722, acc.: 50.00%] [G loss: 6.680799]\n",
      "669 [D loss: 0.886021, acc.: 50.00%] [G loss: 7.181488]\n",
      "670 [D loss: 0.906614, acc.: 50.00%] [G loss: 8.394127]\n",
      "671 [D loss: 0.845656, acc.: 50.00%] [G loss: 8.990456]\n",
      "672 [D loss: 0.880532, acc.: 50.00%] [G loss: 9.278113]\n",
      "673 [D loss: 0.897655, acc.: 50.00%] [G loss: 7.676218]\n",
      "674 [D loss: 0.921949, acc.: 50.00%] [G loss: 9.226847]\n",
      "675 [D loss: 0.877103, acc.: 50.00%] [G loss: 8.101460]\n",
      "676 [D loss: 0.876386, acc.: 50.00%] [G loss: 8.592273]\n",
      "677 [D loss: 0.883455, acc.: 50.00%] [G loss: 8.470842]\n",
      "678 [D loss: 0.862571, acc.: 50.00%] [G loss: 8.269365]\n",
      "679 [D loss: 0.897282, acc.: 50.00%] [G loss: 9.875437]\n",
      "680 [D loss: 0.861527, acc.: 50.00%] [G loss: 6.072556]\n",
      "681 [D loss: 0.885198, acc.: 50.00%] [G loss: 11.376950]\n",
      "682 [D loss: 0.896912, acc.: 50.00%] [G loss: 8.629190]\n",
      "683 [D loss: 0.814540, acc.: 50.00%] [G loss: 7.138272]\n",
      "684 [D loss: 0.900287, acc.: 50.00%] [G loss: 8.435415]\n",
      "685 [D loss: 0.839725, acc.: 50.00%] [G loss: 6.995654]\n",
      "686 [D loss: 0.903556, acc.: 50.00%] [G loss: 9.414920]\n",
      "687 [D loss: 0.878253, acc.: 50.00%] [G loss: 8.214433]\n",
      "688 [D loss: 0.870892, acc.: 50.00%] [G loss: 9.736990]\n",
      "689 [D loss: 0.921774, acc.: 50.00%] [G loss: 9.145162]\n",
      "690 [D loss: 0.930620, acc.: 50.00%] [G loss: 8.577257]\n",
      "691 [D loss: 0.885391, acc.: 50.00%] [G loss: 6.326896]\n",
      "692 [D loss: 0.900699, acc.: 50.00%] [G loss: 10.658269]\n",
      "693 [D loss: 0.838896, acc.: 50.00%] [G loss: 11.287401]\n",
      "694 [D loss: 0.832603, acc.: 50.00%] [G loss: 10.021144]\n",
      "695 [D loss: 0.890051, acc.: 50.00%] [G loss: 9.316130]\n",
      "696 [D loss: 0.872805, acc.: 50.00%] [G loss: 13.282927]\n",
      "697 [D loss: 0.849066, acc.: 50.00%] [G loss: 9.373128]\n",
      "698 [D loss: 0.878529, acc.: 50.00%] [G loss: 7.010002]\n",
      "699 [D loss: 0.862712, acc.: 50.00%] [G loss: 8.305997]\n",
      "700 [D loss: 0.874227, acc.: 50.00%] [G loss: 7.897792]\n",
      "701 [D loss: 0.848685, acc.: 50.00%] [G loss: 8.391645]\n",
      "702 [D loss: 0.820094, acc.: 50.00%] [G loss: 9.203558]\n",
      "703 [D loss: 0.901397, acc.: 50.00%] [G loss: 7.976969]\n",
      "704 [D loss: 0.879366, acc.: 50.00%] [G loss: 7.891840]\n",
      "705 [D loss: 0.820551, acc.: 50.00%] [G loss: 7.836376]\n",
      "706 [D loss: 0.851718, acc.: 50.00%] [G loss: 8.647135]\n",
      "707 [D loss: 0.874437, acc.: 50.00%] [G loss: 8.489319]\n",
      "708 [D loss: 0.870313, acc.: 50.00%] [G loss: 7.275703]\n",
      "709 [D loss: 0.909530, acc.: 50.00%] [G loss: 7.952283]\n",
      "710 [D loss: 0.855825, acc.: 50.00%] [G loss: 7.820048]\n",
      "711 [D loss: 0.907067, acc.: 50.00%] [G loss: 7.649156]\n",
      "712 [D loss: 0.806173, acc.: 50.00%] [G loss: 6.612241]\n",
      "713 [D loss: 0.848036, acc.: 50.00%] [G loss: 7.509376]\n",
      "714 [D loss: 0.871979, acc.: 50.00%] [G loss: 8.330154]\n",
      "715 [D loss: 0.815645, acc.: 50.00%] [G loss: 11.305120]\n",
      "716 [D loss: 0.851553, acc.: 50.00%] [G loss: 6.496873]\n",
      "717 [D loss: 0.824878, acc.: 50.00%] [G loss: 5.814371]\n",
      "718 [D loss: 0.903925, acc.: 50.00%] [G loss: 7.714868]\n",
      "719 [D loss: 0.827590, acc.: 50.00%] [G loss: 6.339263]\n",
      "720 [D loss: 0.857267, acc.: 50.00%] [G loss: 7.785071]\n",
      "721 [D loss: 0.850000, acc.: 50.00%] [G loss: 8.035357]\n",
      "722 [D loss: 0.845157, acc.: 50.00%] [G loss: 7.118019]\n",
      "723 [D loss: 0.846124, acc.: 50.00%] [G loss: 7.312057]\n",
      "724 [D loss: 0.838698, acc.: 50.00%] [G loss: 7.077477]\n",
      "725 [D loss: 0.840689, acc.: 50.00%] [G loss: 7.814804]\n",
      "726 [D loss: 0.839829, acc.: 50.00%] [G loss: 7.080855]\n",
      "727 [D loss: 0.810195, acc.: 50.00%] [G loss: 5.842206]\n",
      "728 [D loss: 0.856336, acc.: 50.00%] [G loss: 6.175220]\n",
      "729 [D loss: 0.868200, acc.: 50.00%] [G loss: 7.978911]\n",
      "730 [D loss: 0.815212, acc.: 50.00%] [G loss: 9.912452]\n",
      "731 [D loss: 0.781963, acc.: 50.00%] [G loss: 9.830413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "732 [D loss: 0.919046, acc.: 50.00%] [G loss: 7.172371]\n",
      "733 [D loss: 0.838013, acc.: 50.00%] [G loss: 5.023862]\n",
      "734 [D loss: 0.880915, acc.: 50.00%] [G loss: 5.597873]\n",
      "735 [D loss: 0.860363, acc.: 50.00%] [G loss: 7.933673]\n",
      "736 [D loss: 0.879068, acc.: 50.00%] [G loss: 6.536603]\n",
      "737 [D loss: 0.847719, acc.: 50.00%] [G loss: 9.883392]\n",
      "738 [D loss: 0.867998, acc.: 50.00%] [G loss: 7.103528]\n",
      "739 [D loss: 0.799303, acc.: 50.00%] [G loss: 7.948142]\n",
      "740 [D loss: 0.839015, acc.: 50.00%] [G loss: 5.694060]\n",
      "741 [D loss: 0.836981, acc.: 50.00%] [G loss: 8.111029]\n",
      "742 [D loss: 0.808608, acc.: 50.00%] [G loss: 8.026803]\n",
      "743 [D loss: 0.893250, acc.: 50.00%] [G loss: 3.895937]\n",
      "744 [D loss: 0.839969, acc.: 50.00%] [G loss: 5.986240]\n",
      "745 [D loss: 0.810285, acc.: 50.00%] [G loss: 5.798322]\n",
      "746 [D loss: 0.852669, acc.: 50.00%] [G loss: 8.503776]\n",
      "747 [D loss: 0.867905, acc.: 50.00%] [G loss: 5.866831]\n",
      "748 [D loss: 0.865126, acc.: 50.00%] [G loss: 7.864645]\n",
      "749 [D loss: 0.820439, acc.: 50.00%] [G loss: 8.626218]\n",
      "750 [D loss: 0.842667, acc.: 50.00%] [G loss: 7.226717]\n",
      "751 [D loss: 0.841076, acc.: 50.00%] [G loss: 6.934058]\n",
      "752 [D loss: 0.800748, acc.: 50.00%] [G loss: 5.544068]\n",
      "753 [D loss: 0.763068, acc.: 50.00%] [G loss: 7.486863]\n",
      "754 [D loss: 0.821477, acc.: 50.00%] [G loss: 6.511902]\n",
      "755 [D loss: 0.790495, acc.: 50.00%] [G loss: 5.216599]\n",
      "756 [D loss: 0.858093, acc.: 50.00%] [G loss: 7.347051]\n",
      "757 [D loss: 0.847794, acc.: 50.00%] [G loss: 5.075841]\n",
      "758 [D loss: 0.821538, acc.: 50.00%] [G loss: 7.017279]\n",
      "759 [D loss: 0.798011, acc.: 50.00%] [G loss: 7.077753]\n",
      "760 [D loss: 0.816068, acc.: 50.00%] [G loss: 7.329968]\n",
      "761 [D loss: 0.843868, acc.: 50.00%] [G loss: 5.935853]\n",
      "762 [D loss: 0.817146, acc.: 50.00%] [G loss: 5.267601]\n",
      "763 [D loss: 0.833970, acc.: 50.00%] [G loss: 4.049068]\n",
      "764 [D loss: 0.824648, acc.: 50.00%] [G loss: 8.532399]\n",
      "765 [D loss: 0.867302, acc.: 50.00%] [G loss: 7.891706]\n",
      "766 [D loss: 0.746456, acc.: 50.00%] [G loss: 6.564340]\n",
      "767 [D loss: 0.815356, acc.: 50.00%] [G loss: 5.358165]\n",
      "768 [D loss: 0.844291, acc.: 50.00%] [G loss: 9.772906]\n",
      "769 [D loss: 0.824823, acc.: 50.00%] [G loss: 7.859057]\n",
      "770 [D loss: 0.825990, acc.: 50.00%] [G loss: 5.077762]\n",
      "771 [D loss: 0.782700, acc.: 50.00%] [G loss: 9.603650]\n",
      "772 [D loss: 0.777196, acc.: 50.00%] [G loss: 8.361587]\n",
      "773 [D loss: 0.797569, acc.: 50.00%] [G loss: 8.480423]\n",
      "774 [D loss: 0.794809, acc.: 50.00%] [G loss: 10.089399]\n",
      "775 [D loss: 0.804682, acc.: 50.00%] [G loss: 6.446126]\n",
      "776 [D loss: 0.852808, acc.: 50.00%] [G loss: 8.793375]\n",
      "777 [D loss: 0.765103, acc.: 50.00%] [G loss: 7.419196]\n",
      "778 [D loss: 0.836827, acc.: 50.00%] [G loss: 8.332850]\n",
      "779 [D loss: 0.800063, acc.: 50.00%] [G loss: 10.791594]\n",
      "780 [D loss: 0.842996, acc.: 50.00%] [G loss: 7.019366]\n",
      "781 [D loss: 0.805120, acc.: 50.00%] [G loss: 5.380925]\n",
      "782 [D loss: 0.767538, acc.: 50.00%] [G loss: 4.855994]\n",
      "783 [D loss: 0.817425, acc.: 50.00%] [G loss: 7.339253]\n",
      "784 [D loss: 0.811000, acc.: 50.00%] [G loss: 8.099780]\n",
      "785 [D loss: 0.787940, acc.: 50.00%] [G loss: 7.725781]\n",
      "786 [D loss: 0.762351, acc.: 50.00%] [G loss: 7.574399]\n",
      "787 [D loss: 0.798159, acc.: 50.00%] [G loss: 4.606885]\n",
      "788 [D loss: 0.785844, acc.: 50.00%] [G loss: 7.804109]\n",
      "789 [D loss: 0.791764, acc.: 50.00%] [G loss: 6.341540]\n",
      "790 [D loss: 0.768227, acc.: 50.00%] [G loss: 6.191779]\n",
      "791 [D loss: 0.777400, acc.: 50.00%] [G loss: 6.557973]\n",
      "792 [D loss: 0.795383, acc.: 50.00%] [G loss: 5.384190]\n",
      "793 [D loss: 0.837826, acc.: 50.00%] [G loss: 4.762880]\n",
      "794 [D loss: 0.815515, acc.: 50.00%] [G loss: 6.081912]\n",
      "795 [D loss: 0.788605, acc.: 50.00%] [G loss: 5.542255]\n",
      "796 [D loss: 0.778595, acc.: 50.00%] [G loss: 7.507207]\n",
      "797 [D loss: 0.758462, acc.: 50.00%] [G loss: 5.391729]\n",
      "798 [D loss: 0.915970, acc.: 50.00%] [G loss: 1.496565]\n",
      "799 [D loss: 0.966708, acc.: 50.00%] [G loss: 1.368786]\n",
      "800 [D loss: 0.911389, acc.: 50.00%] [G loss: 1.403249]\n",
      "801 [D loss: 0.877507, acc.: 50.00%] [G loss: 1.612287]\n",
      "802 [D loss: 0.924268, acc.: 50.00%] [G loss: 1.488848]\n",
      "803 [D loss: 0.855818, acc.: 50.00%] [G loss: 1.342437]\n",
      "804 [D loss: 0.901322, acc.: 50.00%] [G loss: 1.490111]\n",
      "805 [D loss: 0.916992, acc.: 50.00%] [G loss: 1.396133]\n",
      "806 [D loss: 0.906042, acc.: 50.00%] [G loss: 1.465615]\n",
      "807 [D loss: 0.912793, acc.: 50.00%] [G loss: 1.461313]\n",
      "808 [D loss: 0.925078, acc.: 50.00%] [G loss: 1.617007]\n",
      "809 [D loss: 0.900962, acc.: 50.00%] [G loss: 1.285959]\n",
      "810 [D loss: 0.915180, acc.: 50.00%] [G loss: 1.439533]\n",
      "811 [D loss: 0.920149, acc.: 47.50%] [G loss: 1.415321]\n",
      "812 [D loss: 0.904227, acc.: 50.00%] [G loss: 1.633265]\n",
      "813 [D loss: 0.938424, acc.: 50.00%] [G loss: 1.768278]\n",
      "814 [D loss: 0.895793, acc.: 50.00%] [G loss: 1.478021]\n",
      "815 [D loss: 0.896866, acc.: 50.00%] [G loss: 1.432029]\n",
      "816 [D loss: 0.931749, acc.: 50.00%] [G loss: 1.480028]\n",
      "817 [D loss: 0.906276, acc.: 50.00%] [G loss: 1.662243]\n",
      "818 [D loss: 0.861244, acc.: 50.00%] [G loss: 1.490123]\n",
      "819 [D loss: 0.899320, acc.: 50.00%] [G loss: 1.873524]\n",
      "820 [D loss: 0.868739, acc.: 50.00%] [G loss: 1.609938]\n",
      "821 [D loss: 0.860028, acc.: 50.00%] [G loss: 1.517590]\n",
      "822 [D loss: 0.930756, acc.: 50.00%] [G loss: 1.557815]\n",
      "823 [D loss: 0.842284, acc.: 50.00%] [G loss: 1.545209]\n",
      "824 [D loss: 0.858107, acc.: 50.00%] [G loss: 1.694771]\n",
      "825 [D loss: 0.873691, acc.: 50.00%] [G loss: 1.685913]\n",
      "826 [D loss: 0.829279, acc.: 50.00%] [G loss: 1.653397]\n",
      "827 [D loss: 0.862230, acc.: 50.00%] [G loss: 1.942467]\n",
      "828 [D loss: 0.876399, acc.: 50.00%] [G loss: 1.845503]\n",
      "829 [D loss: 0.785743, acc.: 50.00%] [G loss: 1.893914]\n",
      "830 [D loss: 0.812283, acc.: 50.00%] [G loss: 1.742864]\n",
      "831 [D loss: 0.807996, acc.: 50.00%] [G loss: 1.826148]\n",
      "832 [D loss: 0.837187, acc.: 50.00%] [G loss: 2.082023]\n",
      "833 [D loss: 0.801420, acc.: 50.00%] [G loss: 2.563744]\n",
      "834 [D loss: 0.858194, acc.: 50.00%] [G loss: 2.093773]\n",
      "835 [D loss: 0.877127, acc.: 50.00%] [G loss: 2.584591]\n",
      "836 [D loss: 0.899094, acc.: 50.00%] [G loss: 1.910536]\n",
      "837 [D loss: 0.833239, acc.: 50.00%] [G loss: 2.152336]\n",
      "838 [D loss: 0.803899, acc.: 50.00%] [G loss: 2.014444]\n",
      "839 [D loss: 0.855198, acc.: 50.00%] [G loss: 3.056889]\n",
      "840 [D loss: 0.818075, acc.: 50.00%] [G loss: 2.191317]\n",
      "841 [D loss: 0.818225, acc.: 50.00%] [G loss: 2.869191]\n",
      "842 [D loss: 0.825722, acc.: 50.00%] [G loss: 2.059345]\n",
      "843 [D loss: 0.791010, acc.: 50.00%] [G loss: 2.515821]\n",
      "844 [D loss: 0.781090, acc.: 50.00%] [G loss: 2.829348]\n",
      "845 [D loss: 0.770111, acc.: 50.00%] [G loss: 3.597192]\n",
      "846 [D loss: 0.823538, acc.: 50.00%] [G loss: 3.001687]\n",
      "847 [D loss: 0.814457, acc.: 50.00%] [G loss: 2.000994]\n",
      "848 [D loss: 0.792502, acc.: 50.00%] [G loss: 2.217114]\n",
      "849 [D loss: 0.778731, acc.: 50.00%] [G loss: 2.784161]\n",
      "850 [D loss: 0.769915, acc.: 50.00%] [G loss: 2.241064]\n",
      "851 [D loss: 0.806455, acc.: 50.00%] [G loss: 2.330868]\n",
      "852 [D loss: 0.781247, acc.: 50.00%] [G loss: 1.955136]\n",
      "853 [D loss: 0.800625, acc.: 50.00%] [G loss: 3.768518]\n",
      "854 [D loss: 0.783116, acc.: 50.00%] [G loss: 2.070371]\n",
      "855 [D loss: 0.838855, acc.: 50.00%] [G loss: 2.592494]\n",
      "856 [D loss: 0.788777, acc.: 50.00%] [G loss: 2.953165]\n",
      "857 [D loss: 0.782701, acc.: 50.00%] [G loss: 2.128892]\n",
      "858 [D loss: 0.794239, acc.: 50.00%] [G loss: 2.081616]\n",
      "859 [D loss: 0.854809, acc.: 50.00%] [G loss: 2.391171]\n",
      "860 [D loss: 0.825199, acc.: 50.00%] [G loss: 2.337408]\n",
      "861 [D loss: 0.781963, acc.: 50.00%] [G loss: 1.945790]\n",
      "862 [D loss: 0.800599, acc.: 50.00%] [G loss: 1.884824]\n",
      "863 [D loss: 0.832351, acc.: 50.00%] [G loss: 2.125551]\n",
      "864 [D loss: 0.813374, acc.: 50.00%] [G loss: 2.669948]\n",
      "865 [D loss: 0.790564, acc.: 50.00%] [G loss: 1.950215]\n",
      "866 [D loss: 0.839595, acc.: 50.00%] [G loss: 1.802034]\n",
      "867 [D loss: 0.800644, acc.: 50.00%] [G loss: 1.817135]\n",
      "868 [D loss: 0.821895, acc.: 50.00%] [G loss: 1.865010]\n",
      "869 [D loss: 0.765122, acc.: 50.00%] [G loss: 1.938263]\n",
      "870 [D loss: 0.838365, acc.: 50.00%] [G loss: 1.933569]\n",
      "871 [D loss: 0.823584, acc.: 50.00%] [G loss: 2.463075]\n",
      "872 [D loss: 0.823175, acc.: 50.00%] [G loss: 1.852885]\n",
      "873 [D loss: 0.865227, acc.: 50.00%] [G loss: 1.903824]\n",
      "874 [D loss: 0.822335, acc.: 50.00%] [G loss: 2.292663]\n",
      "875 [D loss: 0.862384, acc.: 50.00%] [G loss: 1.808374]\n",
      "876 [D loss: 0.815376, acc.: 50.00%] [G loss: 1.938224]\n",
      "877 [D loss: 0.820933, acc.: 50.00%] [G loss: 1.797489]\n",
      "878 [D loss: 0.772701, acc.: 50.00%] [G loss: 1.733713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879 [D loss: 0.795390, acc.: 50.00%] [G loss: 1.812755]\n",
      "880 [D loss: 0.833761, acc.: 50.00%] [G loss: 1.866622]\n",
      "881 [D loss: 0.848882, acc.: 50.00%] [G loss: 1.867940]\n",
      "882 [D loss: 0.797163, acc.: 50.00%] [G loss: 2.383981]\n",
      "883 [D loss: 0.834219, acc.: 50.00%] [G loss: 1.827074]\n",
      "884 [D loss: 0.807768, acc.: 50.00%] [G loss: 1.651980]\n",
      "885 [D loss: 0.805938, acc.: 50.00%] [G loss: 1.832569]\n",
      "886 [D loss: 0.811343, acc.: 50.00%] [G loss: 1.778077]\n",
      "887 [D loss: 0.804286, acc.: 50.00%] [G loss: 1.712408]\n",
      "888 [D loss: 0.851718, acc.: 50.00%] [G loss: 1.653491]\n",
      "889 [D loss: 0.840743, acc.: 50.00%] [G loss: 2.357967]\n",
      "890 [D loss: 0.812886, acc.: 50.00%] [G loss: 1.587971]\n",
      "891 [D loss: 0.786813, acc.: 50.00%] [G loss: 1.653867]\n",
      "892 [D loss: 0.820762, acc.: 50.00%] [G loss: 1.636382]\n",
      "893 [D loss: 0.857523, acc.: 50.00%] [G loss: 1.648164]\n",
      "894 [D loss: 0.841551, acc.: 50.00%] [G loss: 1.693061]\n",
      "895 [D loss: 0.864784, acc.: 50.00%] [G loss: 1.714064]\n",
      "896 [D loss: 0.852430, acc.: 50.00%] [G loss: 1.712890]\n",
      "897 [D loss: 0.829526, acc.: 50.00%] [G loss: 2.319077]\n",
      "898 [D loss: 0.849113, acc.: 50.00%] [G loss: 1.546303]\n",
      "899 [D loss: 0.828789, acc.: 50.00%] [G loss: 2.142978]\n",
      "900 [D loss: 0.800360, acc.: 50.00%] [G loss: 1.897715]\n",
      "901 [D loss: 0.853739, acc.: 50.00%] [G loss: 1.555739]\n",
      "902 [D loss: 0.821285, acc.: 50.00%] [G loss: 1.548032]\n",
      "903 [D loss: 0.849284, acc.: 50.00%] [G loss: 1.817727]\n",
      "904 [D loss: 0.806679, acc.: 50.00%] [G loss: 2.038747]\n",
      "905 [D loss: 0.791095, acc.: 50.00%] [G loss: 1.480448]\n",
      "906 [D loss: 0.823240, acc.: 50.00%] [G loss: 2.469283]\n",
      "907 [D loss: 0.859213, acc.: 50.00%] [G loss: 2.427663]\n",
      "908 [D loss: 0.787134, acc.: 50.00%] [G loss: 1.769080]\n",
      "909 [D loss: 0.788594, acc.: 50.00%] [G loss: 2.971294]\n",
      "910 [D loss: 0.833552, acc.: 50.00%] [G loss: 1.613535]\n",
      "911 [D loss: 0.856189, acc.: 50.00%] [G loss: 1.546122]\n",
      "912 [D loss: 0.853685, acc.: 50.00%] [G loss: 1.669160]\n",
      "913 [D loss: 0.851813, acc.: 50.00%] [G loss: 1.711122]\n",
      "914 [D loss: 0.857468, acc.: 50.00%] [G loss: 1.567258]\n",
      "915 [D loss: 0.844170, acc.: 50.00%] [G loss: 1.404579]\n",
      "916 [D loss: 0.784372, acc.: 50.00%] [G loss: 1.896091]\n",
      "917 [D loss: 0.875974, acc.: 50.00%] [G loss: 1.477045]\n",
      "918 [D loss: 0.827648, acc.: 50.00%] [G loss: 1.573316]\n",
      "919 [D loss: 0.822044, acc.: 50.00%] [G loss: 1.479864]\n",
      "920 [D loss: 0.828243, acc.: 50.00%] [G loss: 1.527679]\n",
      "921 [D loss: 0.814307, acc.: 50.00%] [G loss: 1.896879]\n",
      "922 [D loss: 0.838167, acc.: 50.00%] [G loss: 1.426157]\n",
      "923 [D loss: 0.845793, acc.: 50.00%] [G loss: 2.888905]\n",
      "924 [D loss: 0.864968, acc.: 50.00%] [G loss: 2.031878]\n",
      "925 [D loss: 0.817582, acc.: 50.00%] [G loss: 1.687888]\n",
      "926 [D loss: 0.817390, acc.: 50.00%] [G loss: 1.713427]\n",
      "927 [D loss: 0.869234, acc.: 50.00%] [G loss: 1.447342]\n",
      "928 [D loss: 0.828212, acc.: 50.00%] [G loss: 1.594593]\n",
      "929 [D loss: 0.843562, acc.: 50.00%] [G loss: 1.663110]\n",
      "930 [D loss: 0.826617, acc.: 50.00%] [G loss: 1.574942]\n",
      "931 [D loss: 0.784046, acc.: 50.00%] [G loss: 1.823038]\n",
      "932 [D loss: 0.776561, acc.: 50.00%] [G loss: 2.284093]\n",
      "933 [D loss: 0.838171, acc.: 50.00%] [G loss: 1.578179]\n",
      "934 [D loss: 0.875192, acc.: 50.00%] [G loss: 1.472369]\n",
      "935 [D loss: 0.847532, acc.: 50.00%] [G loss: 1.803702]\n",
      "936 [D loss: 0.812798, acc.: 50.00%] [G loss: 1.394503]\n",
      "937 [D loss: 0.854703, acc.: 50.00%] [G loss: 3.032105]\n",
      "938 [D loss: 0.842472, acc.: 50.00%] [G loss: 2.379337]\n",
      "939 [D loss: 0.892853, acc.: 50.00%] [G loss: 1.486400]\n",
      "940 [D loss: 0.834800, acc.: 50.00%] [G loss: 1.472861]\n",
      "941 [D loss: 0.820598, acc.: 50.00%] [G loss: 1.624842]\n",
      "942 [D loss: 0.812136, acc.: 50.00%] [G loss: 1.589998]\n",
      "943 [D loss: 0.828308, acc.: 50.00%] [G loss: 2.200942]\n",
      "944 [D loss: 0.803585, acc.: 50.00%] [G loss: 1.566499]\n",
      "945 [D loss: 0.795686, acc.: 50.00%] [G loss: 2.306198]\n",
      "946 [D loss: 0.813868, acc.: 50.00%] [G loss: 2.293783]\n",
      "947 [D loss: 0.784440, acc.: 50.00%] [G loss: 1.556389]\n",
      "948 [D loss: 0.841194, acc.: 50.00%] [G loss: 1.638249]\n",
      "949 [D loss: 0.794070, acc.: 50.00%] [G loss: 1.829971]\n",
      "950 [D loss: 0.821683, acc.: 50.00%] [G loss: 2.252031]\n",
      "951 [D loss: 0.850768, acc.: 50.00%] [G loss: 1.544454]\n",
      "952 [D loss: 0.805316, acc.: 50.00%] [G loss: 1.569124]\n",
      "953 [D loss: 0.847128, acc.: 50.00%] [G loss: 1.425854]\n",
      "954 [D loss: 0.827643, acc.: 50.00%] [G loss: 1.549628]\n",
      "955 [D loss: 0.807391, acc.: 50.00%] [G loss: 1.330942]\n",
      "956 [D loss: 0.855742, acc.: 50.00%] [G loss: 1.537404]\n",
      "957 [D loss: 0.791944, acc.: 50.00%] [G loss: 1.404148]\n",
      "958 [D loss: 0.845781, acc.: 50.00%] [G loss: 1.508521]\n",
      "959 [D loss: 0.839470, acc.: 50.00%] [G loss: 1.637185]\n",
      "960 [D loss: 0.804493, acc.: 50.00%] [G loss: 1.638268]\n",
      "961 [D loss: 0.885024, acc.: 50.00%] [G loss: 1.254667]\n",
      "962 [D loss: 0.819329, acc.: 50.00%] [G loss: 1.386205]\n",
      "963 [D loss: 0.816798, acc.: 50.00%] [G loss: 1.642697]\n",
      "964 [D loss: 0.788808, acc.: 50.00%] [G loss: 2.039458]\n",
      "965 [D loss: 0.814913, acc.: 50.00%] [G loss: 1.716596]\n",
      "966 [D loss: 0.797257, acc.: 50.00%] [G loss: 1.993766]\n",
      "967 [D loss: 0.860727, acc.: 50.00%] [G loss: 1.872733]\n",
      "968 [D loss: 0.792312, acc.: 50.00%] [G loss: 2.378332]\n",
      "969 [D loss: 0.835446, acc.: 50.00%] [G loss: 1.703975]\n",
      "970 [D loss: 0.866636, acc.: 50.00%] [G loss: 1.852725]\n",
      "971 [D loss: 0.791384, acc.: 50.00%] [G loss: 2.257036]\n",
      "972 [D loss: 0.817369, acc.: 50.00%] [G loss: 1.800084]\n",
      "973 [D loss: 0.770224, acc.: 50.00%] [G loss: 1.519453]\n",
      "974 [D loss: 0.799114, acc.: 50.00%] [G loss: 1.789845]\n",
      "975 [D loss: 0.801349, acc.: 50.00%] [G loss: 1.777967]\n",
      "976 [D loss: 0.804150, acc.: 50.00%] [G loss: 2.235649]\n",
      "977 [D loss: 0.855381, acc.: 50.00%] [G loss: 1.550372]\n",
      "978 [D loss: 0.796740, acc.: 50.00%] [G loss: 1.583128]\n",
      "979 [D loss: 0.812683, acc.: 50.00%] [G loss: 1.475322]\n",
      "980 [D loss: 0.805271, acc.: 50.00%] [G loss: 1.606933]\n",
      "981 [D loss: 0.822120, acc.: 50.00%] [G loss: 1.846312]\n",
      "982 [D loss: 0.786520, acc.: 50.00%] [G loss: 1.422407]\n",
      "983 [D loss: 0.780038, acc.: 50.00%] [G loss: 1.448743]\n",
      "984 [D loss: 0.805654, acc.: 50.00%] [G loss: 1.416466]\n",
      "985 [D loss: 0.798681, acc.: 50.00%] [G loss: 2.904654]\n",
      "986 [D loss: 0.827080, acc.: 50.00%] [G loss: 1.624460]\n",
      "987 [D loss: 0.818865, acc.: 50.00%] [G loss: 1.585220]\n",
      "988 [D loss: 0.845669, acc.: 50.00%] [G loss: 1.467550]\n",
      "989 [D loss: 0.773664, acc.: 50.00%] [G loss: 1.652499]\n",
      "990 [D loss: 0.822850, acc.: 50.00%] [G loss: 1.490076]\n",
      "991 [D loss: 0.811417, acc.: 50.00%] [G loss: 1.526416]\n",
      "992 [D loss: 0.741936, acc.: 50.00%] [G loss: 1.598881]\n",
      "993 [D loss: 0.745113, acc.: 50.00%] [G loss: 1.382589]\n",
      "994 [D loss: 0.801137, acc.: 50.00%] [G loss: 2.248487]\n",
      "995 [D loss: 0.783170, acc.: 50.00%] [G loss: 1.431943]\n",
      "996 [D loss: 0.816953, acc.: 50.00%] [G loss: 1.526023]\n",
      "997 [D loss: 0.793548, acc.: 50.00%] [G loss: 1.657152]\n",
      "998 [D loss: 0.809219, acc.: 50.00%] [G loss: 1.660984]\n",
      "999 [D loss: 0.796737, acc.: 50.00%] [G loss: 1.525735]\n"
     ]
    }
   ],
   "source": [
    "lstmgan.train(epochs=1000, batch_size=20, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0h 2m 22s\n"
     ]
    }
   ],
   "source": [
    "end = datetime.datetime.now()\n",
    "\n",
    "diff = (end - start)\n",
    "\n",
    "datetime.timedelta(seconds=10, microseconds=885206)\n",
    "\n",
    "diff_seconds = int(diff.total_seconds())\n",
    "\n",
    "minute_seconds, seconds = divmod(diff_seconds, 60)\n",
    "hours, minutes = divmod(minute_seconds, 60)\n",
    "hms = f\"{hours}h {minutes}m {seconds}s\"\n",
    "\n",
    "'0h 0m 10s'\n",
    "print(hms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.2890239 , -0.0299658 ,  0.05292111,  0.10401131, -0.07733387,\n",
      "         0.02455213,  0.27113712,  0.08692419,  0.09073351,  0.18798403,\n",
      "        -0.05588702,  0.18986444,  0.11102009,  0.16274503, -0.23760377,\n",
      "         0.13446203, -0.24883525,  0.01875264, -0.2238575 ,  0.18345259,\n",
      "        -0.25877887,  0.12200349,  0.08225156,  0.05428846,  0.10668732,\n",
      "        -0.26393247,  0.03825605, -0.07248595, -0.16391148, -0.17686659,\n",
      "         0.04490358,  0.03020021,  0.1785168 ,  0.14788982, -0.21864212,\n",
      "         0.1509062 ,  0.10879965,  0.28523964,  0.05388785,  0.09795637,\n",
      "         0.07378791,  0.24936241,  0.2412134 ,  0.23244055,  0.09657666,\n",
      "         0.02114021,  0.09318568, -0.22788598, -0.23523204, -0.06231563,\n",
      "        -0.08418474,  0.10413148, -0.29202878,  0.03961787, -0.22767153,\n",
      "        -0.22060324,  0.24771754,  0.14193511, -0.04150833, -0.29775086,\n",
      "         0.10951539,  0.21045081,  0.06458769, -0.26359484]],\n",
      "      dtype=float32), array([[[ 0.09412052,  0.01635505, -0.00130215, ..., -0.0892151 ,\n",
      "          0.11358909, -0.11134748],\n",
      "        [ 0.10865922,  0.08311893, -0.132981  , ...,  0.13151507,\n",
      "          0.01866042, -0.14411215],\n",
      "        [ 0.05113807, -0.04205897,  0.08417443, ...,  0.07703199,\n",
      "         -0.06150627,  0.05833996],\n",
      "        ...,\n",
      "        [ 0.01187071, -0.06650732,  0.13038617, ..., -0.06194216,\n",
      "          0.02783595,  0.08797462],\n",
      "        [ 0.11886115,  0.10128789, -0.05340636, ...,  0.10002911,\n",
      "         -0.0396196 ,  0.14909   ],\n",
      "        [-0.11342653,  0.07695683,  0.08072726, ...,  0.10161614,\n",
      "          0.10328074, -0.07265262]],\n",
      "\n",
      "       [[-0.02691631, -0.0263552 ,  0.11287612, ..., -0.0209149 ,\n",
      "          0.05817826, -0.06884458],\n",
      "        [-0.13271561,  0.11741006,  0.06933848, ...,  0.04276361,\n",
      "         -0.02788051, -0.09277527],\n",
      "        [-0.03510588, -0.0028715 , -0.14592656, ...,  0.14525235,\n",
      "          0.10715779, -0.05825459],\n",
      "        ...,\n",
      "        [-0.08794776, -0.04260629, -0.03639849, ..., -0.05139068,\n",
      "          0.13249728, -0.11584762],\n",
      "        [-0.12169818, -0.00754464,  0.0276183 , ..., -0.01087732,\n",
      "          0.02762373,  0.14212428],\n",
      "        [ 0.03263024,  0.09725364, -0.08495077, ..., -0.09539237,\n",
      "          0.03548175,  0.01810553]],\n",
      "\n",
      "       [[ 0.12755902, -0.03360118,  0.04878412, ..., -0.00863629,\n",
      "         -0.11559851, -0.07661408],\n",
      "        [-0.12220575,  0.1191228 , -0.02596263, ..., -0.03523929,\n",
      "          0.11887753,  0.06079588],\n",
      "        [-0.08081905, -0.12929758, -0.00986496, ...,  0.06727361,\n",
      "          0.07587588, -0.11427076],\n",
      "        ...,\n",
      "        [-0.08849985,  0.12975022, -0.04006155, ..., -0.05307674,\n",
      "         -0.06033215, -0.04283782],\n",
      "        [ 0.0204816 ,  0.05624204,  0.04653409, ...,  0.04127705,\n",
      "         -0.00952726, -0.11151053],\n",
      "        [-0.04410011, -0.0342743 , -0.03891955, ...,  0.04119997,\n",
      "          0.01865209,  0.04080186]]], dtype=float32), array([[[ 0.08924033],\n",
      "        [-0.09212124],\n",
      "        [ 0.21872097],\n",
      "        [ 0.12691635],\n",
      "        [-0.22857992],\n",
      "        [-0.1648229 ],\n",
      "        [ 0.18504283],\n",
      "        [ 0.1860591 ],\n",
      "        [-0.038687  ],\n",
      "        [-0.20423847],\n",
      "        [ 0.27898273],\n",
      "        [-0.18250763],\n",
      "        [-0.04553419],\n",
      "        [-0.01773548],\n",
      "        [-0.23825042],\n",
      "        [-0.21173581],\n",
      "        [-0.18061529],\n",
      "        [ 0.14762525],\n",
      "        [-0.1356668 ],\n",
      "        [ 0.24300073],\n",
      "        [-0.23249182],\n",
      "        [ 0.00054384],\n",
      "        [-0.14399143],\n",
      "        [ 0.06357808],\n",
      "        [-0.21596661],\n",
      "        [ 0.0232461 ],\n",
      "        [-0.24561317],\n",
      "        [ 0.1845559 ],\n",
      "        [-0.03481198],\n",
      "        [-0.11436404],\n",
      "        [-0.23885307],\n",
      "        [-0.08124629]],\n",
      "\n",
      "       [[-0.17905849],\n",
      "        [-0.07355786],\n",
      "        [ 0.06184599],\n",
      "        [-0.21683231],\n",
      "        [ 0.03226512],\n",
      "        [ 0.19945903],\n",
      "        [-0.10056864],\n",
      "        [ 0.19948162],\n",
      "        [ 0.29408607],\n",
      "        [-0.03729098],\n",
      "        [-0.15488262],\n",
      "        [ 0.16909559],\n",
      "        [ 0.12752914],\n",
      "        [-0.21077912],\n",
      "        [-0.11013727],\n",
      "        [ 0.15717293],\n",
      "        [-0.14345601],\n",
      "        [ 0.13297173],\n",
      "        [-0.0993203 ],\n",
      "        [-0.21015865],\n",
      "        [ 0.23321773],\n",
      "        [-0.04707945],\n",
      "        [ 0.09753425],\n",
      "        [ 0.01003829],\n",
      "        [ 0.01992251],\n",
      "        [-0.06379008],\n",
      "        [ 0.05616237],\n",
      "        [-0.06946044],\n",
      "        [ 0.05834873],\n",
      "        [ 0.09998652],\n",
      "        [ 0.21085474],\n",
      "        [ 0.11699326]],\n",
      "\n",
      "       [[-0.19542404],\n",
      "        [ 0.07986525],\n",
      "        [-0.2290421 ],\n",
      "        [ 0.21982169],\n",
      "        [ 0.06649407],\n",
      "        [ 0.21369353],\n",
      "        [-0.18765764],\n",
      "        [ 0.15532671],\n",
      "        [ 0.08522097],\n",
      "        [-0.11424855],\n",
      "        [ 0.07844421],\n",
      "        [ 0.06191437],\n",
      "        [-0.14817493],\n",
      "        [ 0.21987027],\n",
      "        [-0.0167553 ],\n",
      "        [-0.05880804],\n",
      "        [ 0.12204653],\n",
      "        [ 0.14483741],\n",
      "        [-0.05543931],\n",
      "        [-0.05422936],\n",
      "        [-0.20452085],\n",
      "        [ 0.0782544 ],\n",
      "        [-0.04356975],\n",
      "        [-0.17682658],\n",
      "        [-0.20482202],\n",
      "        [-0.12193885],\n",
      "        [ 0.233851  ],\n",
      "        [ 0.17798483],\n",
      "        [ 0.11057091],\n",
      "        [ 0.18843758],\n",
      "        [-0.04457851],\n",
      "        [-0.18306664]]], dtype=float32), array([[-0.21338949,  0.26931792, -0.03866392,  0.23702776, -0.01775828,\n",
      "         0.2606419 ,  0.16200967, -0.28740147, -0.41692922, -0.21696809,\n",
      "         0.34224898,  0.38337305,  0.33827674, -0.16671647, -0.23800881,\n",
      "         0.28295892,  0.1243781 , -0.15934478,  0.3184983 , -0.2568719 ,\n",
      "         0.2565322 ,  0.4037306 , -0.04591542, -0.10724898,  0.03622852,\n",
      "         0.32983798, -0.04490466, -0.163249  , -0.40676984, -0.40225962,\n",
      "         0.01701911, -0.23430285],\n",
      "       [ 0.3229401 ,  0.27371266, -0.13340802,  0.3180922 ,  0.10745675,\n",
      "         0.23820153, -0.21570465,  0.27808994,  0.36669332,  0.2126782 ,\n",
      "         0.07312747,  0.11974931,  0.3766938 , -0.14869969,  0.132106  ,\n",
      "         0.34911633,  0.20446332,  0.16672675,  0.13406366, -0.25846776,\n",
      "         0.11569776,  0.3692573 ,  0.20531052,  0.07190027,  0.18739773,\n",
      "        -0.3747552 ,  0.16131371, -0.10401015,  0.35048026, -0.3699016 ,\n",
      "        -0.1271269 , -0.10142021],\n",
      "       [ 0.2723814 , -0.22480735, -0.10002635, -0.26820356,  0.3701125 ,\n",
      "        -0.04616658, -0.09695302,  0.38034317, -0.15419693,  0.13969992,\n",
      "         0.16337992,  0.38363352, -0.18621099,  0.06746306,  0.06851264,\n",
      "         0.33271492,  0.13995382,  0.08490151, -0.08459195,  0.2530226 ,\n",
      "         0.37779516,  0.1623175 ,  0.1818246 ,  0.3430463 , -0.2744091 ,\n",
      "        -0.07574613, -0.15115835,  0.3013466 ,  0.0183298 ,  0.06645437,\n",
      "         0.04030062,  0.2409015 ],\n",
      "       [ 0.1036456 , -0.27278262, -0.1997338 , -0.10545956, -0.37458465,\n",
      "        -0.03003719, -0.17994717,  0.22678366, -0.15500312,  0.36182138,\n",
      "         0.2035749 ,  0.11466722, -0.35246727,  0.22967154,  0.15204068,\n",
      "         0.30530387,  0.2658436 ,  0.23776887, -0.2078583 , -0.05285007,\n",
      "         0.18643317, -0.17600802, -0.1379433 ,  0.3476894 , -0.38697195,\n",
      "         0.11138832,  0.24213527,  0.0242153 ,  0.36638418,  0.06162774,\n",
      "         0.18871951, -0.12992159],\n",
      "       [-0.09311295, -0.207271  ,  0.06642915, -0.3062376 , -0.07818288,\n",
      "        -0.349571  ,  0.19329701, -0.27481925, -0.08887786, -0.03757479,\n",
      "         0.29445305,  0.03589922,  0.35199696, -0.19923642, -0.36351636,\n",
      "        -0.29854935,  0.02227226,  0.22449598,  0.18838282,  0.26578516,\n",
      "         0.03276937, -0.34627795,  0.1995786 ,  0.2668504 , -0.22022516,\n",
      "         0.26504755,  0.05357638, -0.22153187, -0.0196379 ,  0.00177142,\n",
      "         0.32854393, -0.10775866],\n",
      "       [-0.35687095,  0.0531921 , -0.23845883,  0.2562287 ,  0.17378604,\n",
      "         0.19544838,  0.19677341, -0.10806528, -0.20824595,  0.28744718,\n",
      "        -0.16717662,  0.0454676 ,  0.31370717, -0.22083928, -0.2546978 ,\n",
      "        -0.07443832,  0.07127953,  0.15861146,  0.12461487,  0.25525305,\n",
      "         0.25213057,  0.07292099,  0.27127776, -0.12030736, -0.35070696,\n",
      "         0.08709023,  0.3428969 ,  0.1442977 , -0.09177273, -0.09840594,\n",
      "         0.08216162, -0.1311331 ],\n",
      "       [ 0.1783863 , -0.14049429,  0.1444471 ,  0.02109938, -0.27768442,\n",
      "         0.14662804, -0.24641755,  0.34563485,  0.2206979 ,  0.06614524,\n",
      "        -0.00612929,  0.0779211 , -0.3048685 , -0.27956623,  0.31689155,\n",
      "         0.2821835 , -0.3855457 , -0.06593269,  0.07491884, -0.01295037,\n",
      "        -0.2504589 , -0.21070325,  0.3274896 ,  0.33979207, -0.29401815,\n",
      "        -0.06535202, -0.15897149, -0.07577027, -0.06964536, -0.32264203,\n",
      "         0.05851104,  0.12675433],\n",
      "       [ 0.2441822 , -0.2969353 , -0.10668477, -0.14795732,  0.14816608,\n",
      "         0.1400973 ,  0.14919963, -0.27643278,  0.07591566, -0.20971647,\n",
      "        -0.01119591, -0.42035678, -0.36608872, -0.04419834,  0.18755895,\n",
      "        -0.13926792,  0.2588946 , -0.08096749, -0.3371479 , -0.3164511 ,\n",
      "        -0.06419006, -0.1558644 , -0.06180533,  0.2940914 ,  0.2883722 ,\n",
      "         0.34705514,  0.03613608, -0.24572168, -0.3535176 ,  0.29073253,\n",
      "        -0.23760976,  0.30014497]], dtype=float32), array([-0.01617594,  0.02880735,  0.03425853,  0.0656473 ,  0.02738424,\n",
      "        0.04729536,  0.06509268,  0.02363823, -0.0143841 ,  0.08533882,\n",
      "        0.04651925,  0.05309694,  0.05050281, -0.0126907 ,  0.02301615,\n",
      "        0.05316739,  0.07348177,  0.03996841,  0.02597742, -0.00833897,\n",
      "       -0.01188817,  0.02797552, -0.00695799, -0.00467021, -0.01806792,\n",
      "       -0.00813099, -0.0103734 , -0.01099622, -0.01416385, -0.00807112,\n",
      "       -0.00798191, -0.00532275], dtype=float32), array([[-0.34346688],\n",
      "       [ 0.3035045 ],\n",
      "       [ 0.20172358],\n",
      "       [ 0.29281196],\n",
      "       [ 0.14452653],\n",
      "       [ 0.20736647],\n",
      "       [ 0.26511022],\n",
      "       [ 0.38829505],\n",
      "       [-0.34431958],\n",
      "       [ 0.15334989],\n",
      "       [ 0.10836842],\n",
      "       [ 0.41593555],\n",
      "       [ 0.0954406 ],\n",
      "       [-0.25804582],\n",
      "       [ 0.418772  ],\n",
      "       [ 0.4402951 ],\n",
      "       [ 0.2966801 ],\n",
      "       [ 0.37589198],\n",
      "       [ 0.1047309 ],\n",
      "       [-0.2512466 ],\n",
      "       [-0.2973202 ],\n",
      "       [ 0.14330225],\n",
      "       [-0.02523244],\n",
      "       [-0.18711197],\n",
      "       [-0.37276962],\n",
      "       [-0.10911112],\n",
      "       [-0.30902484],\n",
      "       [-0.14307606],\n",
      "       [-0.34611315],\n",
      "       [-0.46165586],\n",
      "       [-0.07144938],\n",
      "       [-0.3795413 ]], dtype=float32), array([0.02973434], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(lstmgan.discriminator.layers[1].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[0.        , 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.2520521 , 0.        , ..., 0.1698239 ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.10135284, 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.1447332 , 0.        , ..., 0.        ,\n",
      "         0.        , 0.03086831]]], dtype=float32)]\n",
      "(1, 2271320, 1, 30)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "hidden_layers = keras.backend.function(\n",
    "[lstmgan.discriminator.layers[0].input],  # we will feed the function with the input of the first layer  \n",
    "[lstmgan.discriminator.layers[0].output,] # we want to get the output of the first layer\n",
    ")\n",
    "h=hidden_layers([df_train])\n",
    "print(h)\n",
    "print(np.shape(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=np.array(h)  \n",
    "\n",
    "arr_reshaped = np.array(h).reshape(2271320, 30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2271320, 30)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(arr_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('normal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.         0.2520521  0.         0.         0.23208602 0.\n",
      "   0.12132854 0.         0.         0.         0.         0.\n",
      "   0.         0.11006745 0.         0.16502865 0.10031924 0.08902921\n",
      "   0.2499162  0.         0.02730873 0.         0.05760938 0.\n",
      "   0.07271524 0.         0.         0.1698239  0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "#To check 1st row\n",
    "print(h[:,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_1 = pd.read_csv('normal.csv')\n",
    "csv_2 = pd.read_csv('normallabel.csv')\n",
    "\n",
    "result = pd.concat([csv_1, csv_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"lgcnormalresult.csv\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ada3143048>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['gen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ada3132208>,\n",
       " <matplotlib.lines.Line2D at 0x1ada3087448>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['dis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature22</th>\n",
       "      <th>feature23</th>\n",
       "      <th>feature24</th>\n",
       "      <th>feature25</th>\n",
       "      <th>feature26</th>\n",
       "      <th>feature27</th>\n",
       "      <th>feature28</th>\n",
       "      <th>feature29</th>\n",
       "      <th>feature30</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065464</td>\n",
       "      <td>0.027737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068303</td>\n",
       "      <td>0.165911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060979</td>\n",
       "      <td>0.008422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097872</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.084868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.186019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070595</td>\n",
       "      <td>0.247596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.249952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.086173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071894</td>\n",
       "      <td>0.246576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.086520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.152395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052850</td>\n",
       "      <td>0.246624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.229947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.065463</td>\n",
       "      <td>0.027741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068333</td>\n",
       "      <td>0.165912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060969</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097872</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0  0.065464  0.027737       0.0       0.0       0.0       0.0  0.185455   \n",
       "1  0.084868  0.000000       0.0       0.0       0.0       0.0  0.186019   \n",
       "2  0.086173  0.000000       0.0       0.0       0.0       0.0  0.185362   \n",
       "3  0.086520  0.000000       0.0       0.0       0.0       0.0  0.152395   \n",
       "4  0.065463  0.027741       0.0       0.0       0.0       0.0  0.185447   \n",
       "\n",
       "   feature8  feature9  feature10  ...  feature22  feature23  feature24  \\\n",
       "0       0.0  0.068303   0.165911  ...   0.060979   0.008422        0.0   \n",
       "1       0.0  0.070595   0.247596  ...   0.105729   0.000000        0.0   \n",
       "2       0.0  0.071894   0.246576  ...   0.106265   0.000000        0.0   \n",
       "3       0.0  0.052850   0.246624  ...   0.090389   0.000000        0.0   \n",
       "4       0.0  0.068333   0.165912  ...   0.060969   0.008425        0.0   \n",
       "\n",
       "   feature25  feature26  feature27  feature28  feature29  feature30  label  \n",
       "0   0.035205        0.0   0.250703        0.0        0.0   0.097872      0  \n",
       "1   0.014328        0.0   0.249952        0.0        0.0   0.000000      0  \n",
       "2   0.014574        0.0   0.250262        0.0        0.0   0.000000      0  \n",
       "3   0.018661        0.0   0.229947        0.0        0.0   0.000000      0  \n",
       "4   0.035205        0.0   0.250699        0.0        0.0   0.097872      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test=df1.label\n",
    "\n",
    "x_test=df1.drop('label',axis=1).values.reshape(-1,30,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.24397512]]\n",
      "\n",
      " [[0.23926233]]\n",
      "\n",
      " [[0.23876022]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.26098067]]\n",
      "\n",
      " [[0.2764783 ]]\n",
      "\n",
      " [[0.28419912]]]\n"
     ]
    }
   ],
   "source": [
    "#y_pred=lstmgan.generator.predict(x_test)\n",
    "y_pred=lstmgan.discriminator.predict(x_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "yhat_probs = lstmgan.discriminator.predict(x_test, verbose=0)\n",
    "yhat_classes=np.argmax(yhat_probs,axis=1)\n",
    "# predict crisp classes for test set\n",
    "#yhat_classes = model.predict_classes(x_val, verbose=0)\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "#yhat_classes = yhat_classes[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-0ee39934b722>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# We set the threshold equal to the training loss of the autoencoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtesting_set_predictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlstmgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "# Helper function that calculates the reconstruction loss of each data sample\n",
    "def calculate_losses(x,preds):\n",
    "    losses=np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        losses[i]=((preds[i] - x[i]) ** 2).mean(axis=None)\n",
    "        \n",
    "    return losses\n",
    "\n",
    "# We set the threshold equal to the training loss of the autoencoder\n",
    "threshold=history.history[\"loss\"][-1]\n",
    "\n",
    "testing_set_predictions=lstmgan.discriminator.predict(x_test)\n",
    "test_losses=calculate_losses(x_test,testing_set_predictions)\n",
    "testing_set_predictions=np.zeros(len(test_losses))\n",
    "testing_set_predictions[np.where(test_losses>threshold)]=1\n",
    "\n",
    "\n",
    "error_df = pd.DataFrame({'Reconstruction_error': testing_set_predictions, 'True_class': y_test})\n",
    "error_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,precision_score,f1_score\n",
    "accuracy=accuracy_score(y_val,testing_set_predictions)\n",
    "recall=recall_score(y_val,testing_set_predictions)\n",
    "precision=precision_score(y_val,testing_set_predictions)\n",
    "f1=f1_score(y_val,testing_set_predictions)\n",
    "print(\"Performance over the testing data set \\n\")\n",
    "print(\"Accuracy : {} , Recall : {} , Precision : {} , F1 : {}\\n\".format(accuracy,recall,precision,f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/#:~:text=Optimal%20Threshold%20for%20Precision%2DRecall%20Curve,-Unlike%20the%20ROC&text=Recall%20is%20calculated%20as%20the,positives%20and%20the%20false%20negatives.\n",
    "#Optimal Threshold Tuning\n",
    "# search thresholds for imbalanced classification\n",
    "from numpy import arange\n",
    "from numpy import argmax\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')\n",
    " \n",
    "\n",
    "# predict probabilities\n",
    "#yhat = model.predict_proba(x_val)\n",
    "# keep probabilities for the positive outcome only\n",
    "#probs = yhat[:, 1]\n",
    "# define thresholds\n",
    "thresholds = arange(0, 1, 0.001)\n",
    "# evaluate each threshold\n",
    "scores = [f1_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each threshold\n",
    "scores = [accuracy_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, Accuracy-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each threshold\n",
    "scores = [precision_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, Precision-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each threshold\n",
    "scores = [recall_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, recall-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# ROC AUC\n",
    "auc = roc_auc_score(y_test, yhat_probs)\n",
    "print('ROC AUC: %f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=8\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(df_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "#x_test = keras.preprocessing.sequence.pad_sequences(x_test , maxlen=maxlen)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "#https://www.kaggle.com/suicaokhoailang/10-fold-lstm-with-attention-0-991-lb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import sklearn.metrics as sk\n",
    "from sklearn.metrics import roc_curve,f1_score\n",
    "from sklearn.metrics import auc\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=42069)\n",
    "preds = []\n",
    "fold = 0\n",
    "aucs = 0\n",
    "for train_idx, val_idx in kf.split(x_train):\n",
    "    x_train_f = x_train[train_idx]\n",
    "    y_train_f = y_test[train_idx]\n",
    "    x_val_f = x_train[val_idx]\n",
    "    y_val_f = y_test[val_idx]\n",
    "    lstmgan.discriminator.compile(optimizer, \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    #model = get_model()\n",
    "    lstmgan.discriminator.fit(x_train_f, y_train_f,\n",
    "              batch_size=256,\n",
    "              epochs=10,\n",
    "              verbose = 1,\n",
    "              validation_data=(x_val_f, y_val_f))\n",
    "\n",
    "    # Get accuracy of model on validation data. It's not AUC but it's something at least!\n",
    "    preds_val = lstmgan.discriminator.predict([x_val_f], batch_size=512)\n",
    "    preds.append(lstmgan.discriminator.predict(x_test))\n",
    "    fold+=1\n",
    "    fpr, tpr, thresholds = roc_curve(y_val_f, preds_val, pos_label=1)\n",
    "    # calculate scores\n",
    "    #lr_f1, lr_auc = f1_score(testy, yhat), auc(lr_recall, lr_precision)\n",
    "    aucs += auc(fpr,tpr)\n",
    "    print('Fold {}, AUC = {}'.format(fold,auc(fpr, tpr)))\n",
    "print(\"Cross Validation AUC = {}\".format(aucs/10))\n",
    "#print(sk.confusion_matrix(y_val_f,preds_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, accuracy_score ,recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#range of output value\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(range(len(y_pred)),y_pred.reshape(-1,1))\n",
    "plt.ylabel('y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for the optimal probability threshold\n",
    "#prob=[0.5,0.55,0.6,0.65,0.75,0.8,0.85,0.9,0.95,1,2,2.5,5]\n",
    "prob=[-0.2,-0.1,-0.05,0,0.005,0.1,0.157,0.2,0.3,0.5,1,1.5,2]\n",
    "for p in prob:\n",
    "    pred_value =[1 if i<p   else 0 for i in y_pred]\n",
    "    f1=f1_score(y_test,pred_value)\n",
    "    acc=accuracy_score(y_test,pred_value)\n",
    "    precision=average_precision_score(y_test,pred_value)\n",
    "    recall= recall_score(y_test,pred_value)\n",
    "    print(f'prob ={p} and f1 score ={f1} : accuracy={acc} : precision={precision} : recall={recall}')\n",
    "    print(pred_value.count(0),pred_value.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=dict()\n",
    "history['gen']=[]\n",
    "history['dis']=[]\n",
    "def train1(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for batch in dataset:\n",
    "\n",
    "           gen_loss,dis_loss= lstmgan.train(batch)\n",
    "        history['gen'].append(gen_loss)\n",
    "        history['dis'].append(dis_loss)\n",
    "        print ('Time for epoch {} is {} sec '.format(epoch + 1, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = lstmgan.discriminator.evaluate(x_test, y_test)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = lstmgan.generator.predict_classes(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average=\"binary\")\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"binary\")\n",
    "#np.savetxt('results/expected1.txt', y_test, fmt='%01d')\n",
    "#np.savetxt('results/predicted1.txt', y_pred, fmt='%01d')\n",
    "\n",
    "print(\"confusion matrix\")\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"accuracy\")\n",
    "print(\"%.6f\" %accuracy)\n",
    "print(\"racall\")\n",
    "print(\"%.6f\" %recall)\n",
    "print(\"precision\")\n",
    "print(\"%.6f\" %precision)\n",
    "print(\"f1score\")\n",
    "print(\"%.6f\" %f1)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"==============================================\")\n",
    "print (cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
