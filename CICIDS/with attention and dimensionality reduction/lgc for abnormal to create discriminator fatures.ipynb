{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/function9/bidirectional-lstm-gan-music-generation\n",
    "#https://www.kaggle.com/abhisheksinha28/bidirectional-lstm/data\n",
    "#https://www.kaggle.com/sekfook97/gan-for-anomaly-detection\n",
    "# importing libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Bidirectional, LSTM, Reshape, RepeatVector, TimeDistributed\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "\"feature1\": np.float16,\n",
    "\"feature2\": np.float16,\n",
    "\"feature3\": np.float16,\n",
    "\"feature4\": np.float16,\n",
    "\"feature5\": np.float16,\n",
    "\"feature6\": np.float16,\n",
    "\"feature7\": np.float16,\n",
    "\"feature8\": np.float16,\n",
    "\"feature9\": np.float16,\n",
    "\"feature10\": np.float16,\n",
    "\"feature11\": np.float16,\n",
    "\"feature12\": np.float16,\n",
    "\"feature13\": np.float16,\n",
    "\"feature14\": np.float16,\n",
    "\"feature15\": np.float16,\n",
    "\"feature16\": np.float16,\n",
    "\"feature17\": np.float16,\n",
    "\"feature18\": np.float16,\n",
    "\"feature19\": np.float16,\n",
    "\"feature20\": np.float16,\n",
    "\"feature21\": np.float16,\n",
    "\"feature22\": np.float16,\n",
    "\"feature23\": np.float16,\n",
    "\"feature24\": np.float16,\n",
    "\"feature25\": np.float16,    \n",
    "\"feature26\": np.float16,\n",
    "\"feature27\": np.float16,\n",
    "\"feature28\": np.float16,\n",
    "\"feature29\": np.float16,\n",
    "\"feature30\": np.float16,    \n",
    "\"label\": np.object}    \n",
    "columns = [\"feature1\",\"feature2\",\"feature3\",\"feature4\",\"feature5\",\"feature6\",\"feature7\",\"feature8\",\"feature9\",\"feature10\",\"feature11\",\"feature12\",\"feature13\",\"feature14\",\"feature15\",\"feature16\",\"feature17\",\"feature18\",\"feature19\",\"feature20\",\"feature21\",\"feature22\",\"feature23\",\"feature24\",\"feature25\",\"feature26\",\"feature27\",\"feature28\",\"feature29\",\"feature30\",\"label\"]\n",
    "#df = pd.read_csv(\"/kaggle/input/kdd-cup-1999-data/kddcup.data_10_percent_corrected\", sep=\",\", names=columns, dtype=dtypes, index_col=None)\n",
    "df = pd.read_csv(r\"C:\\Users\\admin\\CICIDS - 30 neurons\\afterdimensionalityreduction\",sep=\",\", names=columns, dtype=str, index_col=None)\n",
    "#df_read = pd.read_csv(savefile, dtype=str, index_col=0)\n",
    "#print(type(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal      2271320\n",
       "Abnormal     556556\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             feature1     feature2 feature3     feature4    feature5  \\\n",
       "0        0.065463625  0.027736843      0.0          0.0         0.0   \n",
       "1          0.0848677          0.0      0.0          0.0         0.0   \n",
       "2         0.08617264          0.0      0.0          0.0         0.0   \n",
       "3         0.08651951          0.0      0.0          0.0         0.0   \n",
       "4        0.065462984   0.02774103      0.0          0.0         0.0   \n",
       "...              ...          ...      ...          ...         ...   \n",
       "2827871          0.0  0.030126706      0.0     0.045611  0.17092325   \n",
       "2827872          0.0          0.0      0.0          0.0  0.13751188   \n",
       "2827873          0.0  0.051236615      0.0          0.0         0.0   \n",
       "2827874          0.0          0.0      0.0   0.02014964  0.15557319   \n",
       "2827875          0.0          0.0      0.0  0.016282216  0.15501131   \n",
       "\n",
       "           feature6    feature7    feature8    feature9   feature10  ...  \\\n",
       "0               0.0  0.18545482         0.0  0.06830312  0.16591072  ...   \n",
       "1               0.0  0.18601944         0.0  0.07059492  0.24759579  ...   \n",
       "2               0.0  0.18536246         0.0  0.07189356  0.24657574  ...   \n",
       "3               0.0   0.1523949         0.0  0.05285023  0.24662375  ...   \n",
       "4               0.0  0.18544693         0.0  0.06833266  0.16591166  ...   \n",
       "...             ...         ...         ...         ...         ...  ...   \n",
       "2827871  0.18404113         0.0  0.07704185         0.0  0.07995535  ...   \n",
       "2827872  0.20500034         0.0         0.0         0.0         0.0  ...   \n",
       "2827873         0.0  0.10252521         0.0  0.18571489  0.15420453  ...   \n",
       "2827874  0.19371656         0.0         0.0         0.0   0.0209108  ...   \n",
       "2827875  0.19090527         0.0         0.0         0.0  0.03676054  ...   \n",
       "\n",
       "           feature22    feature23 feature24     feature25 feature26  \\\n",
       "0        0.060978614  0.008421805       0.0    0.03520457       0.0   \n",
       "1         0.10572878          0.0       0.0   0.014328428       0.0   \n",
       "2         0.10626503          0.0       0.0   0.014573529       0.0   \n",
       "3         0.09038926          0.0       0.0   0.018660925       0.0   \n",
       "4          0.0609689   0.00842534       0.0    0.03520482       0.0   \n",
       "...              ...          ...       ...           ...       ...   \n",
       "2827871          0.0   0.06389466       0.0     0.0486761       0.0   \n",
       "2827872          0.0  0.014810607       0.0    0.05983961       0.0   \n",
       "2827873   0.08684944          0.0       0.0   0.024336088       0.0   \n",
       "2827874          0.0   0.06913914       0.0           0.0       0.0   \n",
       "2827875          0.0   0.06263712       0.0  0.0008633882       0.0   \n",
       "\n",
       "          feature27 feature28 feature29    feature30   label  \n",
       "0        0.25070333       0.0       0.0   0.09787195  Normal  \n",
       "1         0.2499516       0.0       0.0          0.0  Normal  \n",
       "2         0.2502615       0.0       0.0          0.0  Normal  \n",
       "3         0.2299474       0.0       0.0          0.0  Normal  \n",
       "4        0.25069907       0.0       0.0  0.097872004  Normal  \n",
       "...             ...       ...       ...          ...     ...  \n",
       "2827871  0.09643841       0.0       0.0          0.0  Normal  \n",
       "2827872  0.08832435       0.0       0.0          0.0  Normal  \n",
       "2827873  0.25827682       0.0       0.0   0.09253141  Normal  \n",
       "2827874  0.08868496       0.0       0.0          0.0  Normal  \n",
       "2827875  0.09066569       0.0       0.0          0.0  Normal  \n",
       "\n",
       "[2827876 rows x 31 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 for normal data and 1 for abnormalities\n",
    "df.label=df.label.apply(lambda x: 0 if x == 'Normal' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2827876, 31)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             feature1     feature2 feature3     feature4    feature5  \\\n",
       "0        0.065463625  0.027736843      0.0          0.0         0.0   \n",
       "1          0.0848677          0.0      0.0          0.0         0.0   \n",
       "2         0.08617264          0.0      0.0          0.0         0.0   \n",
       "3         0.08651951          0.0      0.0          0.0         0.0   \n",
       "4        0.065462984   0.02774103      0.0          0.0         0.0   \n",
       "...              ...          ...      ...          ...         ...   \n",
       "2827871          0.0  0.030126706      0.0     0.045611  0.17092325   \n",
       "2827872          0.0          0.0      0.0          0.0  0.13751188   \n",
       "2827873          0.0  0.051236615      0.0          0.0         0.0   \n",
       "2827874          0.0          0.0      0.0   0.02014964  0.15557319   \n",
       "2827875          0.0          0.0      0.0  0.016282216  0.15501131   \n",
       "\n",
       "           feature6    feature7    feature8    feature9   feature10  ...  \\\n",
       "0               0.0  0.18545482         0.0  0.06830312  0.16591072  ...   \n",
       "1               0.0  0.18601944         0.0  0.07059492  0.24759579  ...   \n",
       "2               0.0  0.18536246         0.0  0.07189356  0.24657574  ...   \n",
       "3               0.0   0.1523949         0.0  0.05285023  0.24662375  ...   \n",
       "4               0.0  0.18544693         0.0  0.06833266  0.16591166  ...   \n",
       "...             ...         ...         ...         ...         ...  ...   \n",
       "2827871  0.18404113         0.0  0.07704185         0.0  0.07995535  ...   \n",
       "2827872  0.20500034         0.0         0.0         0.0         0.0  ...   \n",
       "2827873         0.0  0.10252521         0.0  0.18571489  0.15420453  ...   \n",
       "2827874  0.19371656         0.0         0.0         0.0   0.0209108  ...   \n",
       "2827875  0.19090527         0.0         0.0         0.0  0.03676054  ...   \n",
       "\n",
       "           feature22    feature23 feature24     feature25 feature26  \\\n",
       "0        0.060978614  0.008421805       0.0    0.03520457       0.0   \n",
       "1         0.10572878          0.0       0.0   0.014328428       0.0   \n",
       "2         0.10626503          0.0       0.0   0.014573529       0.0   \n",
       "3         0.09038926          0.0       0.0   0.018660925       0.0   \n",
       "4          0.0609689   0.00842534       0.0    0.03520482       0.0   \n",
       "...              ...          ...       ...           ...       ...   \n",
       "2827871          0.0   0.06389466       0.0     0.0486761       0.0   \n",
       "2827872          0.0  0.014810607       0.0    0.05983961       0.0   \n",
       "2827873   0.08684944          0.0       0.0   0.024336088       0.0   \n",
       "2827874          0.0   0.06913914       0.0           0.0       0.0   \n",
       "2827875          0.0   0.06263712       0.0  0.0008633882       0.0   \n",
       "\n",
       "          feature27 feature28 feature29    feature30 label  \n",
       "0        0.25070333       0.0       0.0   0.09787195     0  \n",
       "1         0.2499516       0.0       0.0          0.0     0  \n",
       "2         0.2502615       0.0       0.0          0.0     0  \n",
       "3         0.2299474       0.0       0.0          0.0     0  \n",
       "4        0.25069907       0.0       0.0  0.097872004     0  \n",
       "...             ...       ...       ...          ...   ...  \n",
       "2827871  0.09643841       0.0       0.0          0.0     0  \n",
       "2827872  0.08832435       0.0       0.0          0.0     0  \n",
       "2827873  0.25827682       0.0       0.0   0.09253141     0  \n",
       "2827874  0.08868496       0.0       0.0          0.0     0  \n",
       "2827875  0.09066569       0.0       0.0          0.0     0  \n",
       "\n",
       "[2827876 rows x 31 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     object\n",
       "feature2     object\n",
       "feature3     object\n",
       "feature4     object\n",
       "feature5     object\n",
       "feature6     object\n",
       "feature7     object\n",
       "feature8     object\n",
       "feature9     object\n",
       "feature10    object\n",
       "feature11    object\n",
       "feature12    object\n",
       "feature13    object\n",
       "feature14    object\n",
       "feature15    object\n",
       "feature16    object\n",
       "feature17    object\n",
       "feature18    object\n",
       "feature19    object\n",
       "feature20    object\n",
       "feature21    object\n",
       "feature22    object\n",
       "feature23    object\n",
       "feature24    object\n",
       "feature25    object\n",
       "feature26    object\n",
       "feature27    object\n",
       "feature28    object\n",
       "feature29    object\n",
       "feature30    object\n",
       "label         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df)   \n",
    " \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['feature1'] = df['feature1'].astype(float) \n",
    "df['feature2'] = df['feature2'].astype(float) \n",
    "df['feature3'] = df['feature3'].astype(float) \n",
    "df['feature4'] = df['feature4'].astype(float) \n",
    "df['feature5'] = df['feature5'].astype(float) \n",
    "df['feature6'] = df['feature6'].astype(float) \n",
    "df['feature7'] = df['feature7'].astype(float) \n",
    "df['feature8'] = df['feature8'].astype(float) \n",
    "df['feature9'] = df['feature9'].astype(float) \n",
    "df['feature10'] = df['feature10'].astype(float) \n",
    "df['feature11'] = df['feature11'].astype(float) \n",
    "df['feature12'] = df['feature12'].astype(float) \n",
    "df['feature13'] = df['feature13'].astype(float) \n",
    "df['feature14'] = df['feature14'].astype(float) \n",
    "df['feature15'] = df['feature15'].astype(float) \n",
    "df['feature16'] = df['feature16'].astype(float) \n",
    "df['feature17'] = df['feature17'].astype(float) \n",
    "df['feature18'] = df['feature18'].astype(float) \n",
    "df['feature19'] = df['feature19'].astype(float) \n",
    "df['feature20'] = df['feature20'].astype(float) \n",
    "df['feature21'] = df['feature21'].astype(float) \n",
    "df['feature22'] = df['feature22'].astype(float) \n",
    "df['feature23'] = df['feature23'].astype(float) \n",
    "df['feature24'] = df['feature24'].astype(float) \n",
    "df['feature25'] = df['feature25'].astype(float) \n",
    "df['feature26'] = df['feature26'].astype(float) \n",
    "df['feature27'] = df['feature27'].astype(float) \n",
    "df['feature28'] = df['feature28'].astype(float)\n",
    "df['feature29'] = df['feature29'].astype(float) \n",
    "df['feature30'] = df['feature30'].astype(float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly sample 500 data point for training\n",
    "df_train=df[df.label==1].sample(556556)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(556556, 31)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     float64\n",
       "feature2     float64\n",
       "feature3     float64\n",
       "feature4     float64\n",
       "feature5     float64\n",
       "feature6     float64\n",
       "feature7     float64\n",
       "feature8     float64\n",
       "feature9     float64\n",
       "feature10    float64\n",
       "feature11    float64\n",
       "feature12    float64\n",
       "feature13    float64\n",
       "feature14    float64\n",
       "feature15    float64\n",
       "feature16    float64\n",
       "feature17    float64\n",
       "feature18    float64\n",
       "feature19    float64\n",
       "feature20    float64\n",
       "feature21    float64\n",
       "feature22    float64\n",
       "feature23    float64\n",
       "feature24    float64\n",
       "feature25    float64\n",
       "feature26    float64\n",
       "feature27    float64\n",
       "feature28    float64\n",
       "feature29    float64\n",
       "feature30    float64\n",
       "label          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the item sampled from our dataset\n",
    "index_list=df_train.index\n",
    "df=df.drop(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     0.162374\n",
       "feature2     0.266411\n",
       "feature3     0.000000\n",
       "feature4     0.000000\n",
       "feature5     0.269083\n",
       "feature6     0.000000\n",
       "feature7     0.149491\n",
       "feature8     0.079647\n",
       "feature9     0.000000\n",
       "feature10    0.115767\n",
       "feature11    0.000000\n",
       "feature12    0.000000\n",
       "feature13    0.000000\n",
       "feature14    0.105497\n",
       "feature15    0.000000\n",
       "feature16    0.067341\n",
       "feature17    0.072982\n",
       "feature18    0.086237\n",
       "feature19    0.243680\n",
       "feature20    0.000000\n",
       "feature21    0.000000\n",
       "feature22    0.000000\n",
       "feature23    0.000000\n",
       "feature24    0.000000\n",
       "feature25    0.090651\n",
       "feature26    0.000000\n",
       "feature27    0.113022\n",
       "feature28    0.061046\n",
       "feature29    0.000000\n",
       "feature30    0.000000\n",
       "label        1.000000\n",
       "Name: 473294, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature22</th>\n",
       "      <th>feature23</th>\n",
       "      <th>feature24</th>\n",
       "      <th>feature25</th>\n",
       "      <th>feature26</th>\n",
       "      <th>feature27</th>\n",
       "      <th>feature28</th>\n",
       "      <th>feature29</th>\n",
       "      <th>feature30</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065464</td>\n",
       "      <td>0.027737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068303</td>\n",
       "      <td>0.165911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060979</td>\n",
       "      <td>0.008422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097872</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.084868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.186019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070595</td>\n",
       "      <td>0.247596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.249952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.086173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071894</td>\n",
       "      <td>0.246576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.086520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.152395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052850</td>\n",
       "      <td>0.246624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.229947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.065463</td>\n",
       "      <td>0.027741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068333</td>\n",
       "      <td>0.165912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060969</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097872</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0  0.065464  0.027737       0.0       0.0       0.0       0.0  0.185455   \n",
       "1  0.084868  0.000000       0.0       0.0       0.0       0.0  0.186019   \n",
       "2  0.086173  0.000000       0.0       0.0       0.0       0.0  0.185362   \n",
       "3  0.086520  0.000000       0.0       0.0       0.0       0.0  0.152395   \n",
       "4  0.065463  0.027741       0.0       0.0       0.0       0.0  0.185447   \n",
       "\n",
       "   feature8  feature9  feature10  ...  feature22  feature23  feature24  \\\n",
       "0       0.0  0.068303   0.165911  ...   0.060979   0.008422        0.0   \n",
       "1       0.0  0.070595   0.247596  ...   0.105729   0.000000        0.0   \n",
       "2       0.0  0.071894   0.246576  ...   0.106265   0.000000        0.0   \n",
       "3       0.0  0.052850   0.246624  ...   0.090389   0.000000        0.0   \n",
       "4       0.0  0.068333   0.165912  ...   0.060969   0.008425        0.0   \n",
       "\n",
       "   feature25  feature26  feature27  feature28  feature29  feature30  label  \n",
       "0   0.035205        0.0   0.250703        0.0        0.0   0.097872      0  \n",
       "1   0.014328        0.0   0.249952        0.0        0.0   0.000000      0  \n",
       "2   0.014574        0.0   0.250262        0.0        0.0   0.000000      0  \n",
       "3   0.018661        0.0   0.229947        0.0        0.0   0.000000      0  \n",
       "4   0.035205        0.0   0.250699        0.0        0.0   0.097872      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the label columns\n",
    "df_label=df_train.label\n",
    "df_train=df_train.drop('label',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label\n",
    "df_label.to_csv('abnormallabel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(556556, 30)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     0.162374\n",
       "feature2     0.266411\n",
       "feature3     0.000000\n",
       "feature4     0.000000\n",
       "feature5     0.269083\n",
       "feature6     0.000000\n",
       "feature7     0.149491\n",
       "feature8     0.079647\n",
       "feature9     0.000000\n",
       "feature10    0.115767\n",
       "feature11    0.000000\n",
       "feature12    0.000000\n",
       "feature13    0.000000\n",
       "feature14    0.105497\n",
       "feature15    0.000000\n",
       "feature16    0.067341\n",
       "feature17    0.072982\n",
       "feature18    0.086237\n",
       "feature19    0.243680\n",
       "feature20    0.000000\n",
       "feature21    0.000000\n",
       "feature22    0.000000\n",
       "feature23    0.000000\n",
       "feature24    0.000000\n",
       "feature25    0.090651\n",
       "feature26    0.000000\n",
       "feature27    0.113022\n",
       "feature28    0.061046\n",
       "feature29    0.000000\n",
       "feature30    0.000000\n",
       "Name: 473294, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(556556, 30)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.values.reshape((df_train.shape[0], 1, df_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(556556, 1, 30)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature22</th>\n",
       "      <th>feature23</th>\n",
       "      <th>feature24</th>\n",
       "      <th>feature25</th>\n",
       "      <th>feature26</th>\n",
       "      <th>feature27</th>\n",
       "      <th>feature28</th>\n",
       "      <th>feature29</th>\n",
       "      <th>feature30</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065464</td>\n",
       "      <td>0.027737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068303</td>\n",
       "      <td>0.165911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060979</td>\n",
       "      <td>0.008422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097872</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.084868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.186019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070595</td>\n",
       "      <td>0.247596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.249952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.086173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071894</td>\n",
       "      <td>0.246576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.086520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.152395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052850</td>\n",
       "      <td>0.246624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.229947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.065463</td>\n",
       "      <td>0.027741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068333</td>\n",
       "      <td>0.165912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060969</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097872</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0  0.065464  0.027737       0.0       0.0       0.0       0.0  0.185455   \n",
       "1  0.084868  0.000000       0.0       0.0       0.0       0.0  0.186019   \n",
       "2  0.086173  0.000000       0.0       0.0       0.0       0.0  0.185362   \n",
       "3  0.086520  0.000000       0.0       0.0       0.0       0.0  0.152395   \n",
       "4  0.065463  0.027741       0.0       0.0       0.0       0.0  0.185447   \n",
       "\n",
       "   feature8  feature9  feature10  ...  feature22  feature23  feature24  \\\n",
       "0       0.0  0.068303   0.165911  ...   0.060979   0.008422        0.0   \n",
       "1       0.0  0.070595   0.247596  ...   0.105729   0.000000        0.0   \n",
       "2       0.0  0.071894   0.246576  ...   0.106265   0.000000        0.0   \n",
       "3       0.0  0.052850   0.246624  ...   0.090389   0.000000        0.0   \n",
       "4       0.0  0.068333   0.165912  ...   0.060969   0.008425        0.0   \n",
       "\n",
       "   feature25  feature26  feature27  feature28  feature29  feature30  label  \n",
       "0   0.035205        0.0   0.250703        0.0        0.0   0.097872      0  \n",
       "1   0.014328        0.0   0.249952        0.0        0.0   0.000000      0  \n",
       "2   0.014574        0.0   0.250262        0.0        0.0   0.000000      0  \n",
       "3   0.018661        0.0   0.229947        0.0        0.0   0.000000      0  \n",
       "4   0.035205        0.0   0.250699        0.0        0.0   0.097872      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "timesteps =  df_train.shape[1] # equal to the lookback\n",
    "n_features =  df_train.shape[2] # 59\n",
    "\n",
    "epochs = 15\n",
    "batch = 64\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=dict()\n",
    "history['gen']=[]\n",
    "history['dis']=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error\n",
    "                             ,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 1\n",
    "        self.img_cols = 30\n",
    "        self.img_shape = (self.img_rows, self.img_cols)\n",
    "        self.latent_dim = 30\n",
    "\n",
    "        optimizer = Adam(0.0001, 0.4)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates song\n",
    "        z = Input(shape=(1,30))\n",
    "    \n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(1, 30)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #specifying output to have 40 timesteps\n",
    "        model.add(RepeatVector(30))\n",
    "        #specifying 1 feature as the output\n",
    "        \n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.3))   \n",
    "        model.add(TimeDistributed(Dense(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(1,30))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "        #return Model(img)\n",
    "    \n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(64, use_bias=False, input_shape=(30, 1)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Convolution1D(32, 3, strides=2, padding='same', use_bias=False))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Convolution1D(1, 3, strides=2, padding='same', use_bias=False))\n",
    "        model.add(Flatten())        \n",
    "        model.add(RepeatVector(1))        \n",
    "        model.add(TimeDistributed(Dense(32, activation = 'relu')))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(1, activation = 'relu')))\n",
    "        #model.add(TimeDistributed(Dense(1)))\n",
    "        #model.add(Dense(1, activation='tanh'))\n",
    "        #model.add(Dense(1))\n",
    "        #model.add(Activation('sigmoid'))\n",
    "        model.summary()\n",
    "        #model.add(layers.BatchNormalization())\n",
    "        #model.add(sigmoid())\n",
    "       \n",
    "        img = Input(shape=(30,1))\n",
    "        validity = model(img)\n",
    "        print(\"exit\")\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs=15, batch_size=64, save_interval=50):\n",
    "        print(\"enter\")  \n",
    "        # Load the dataset\n",
    "        X_train = df_train\n",
    "       \n",
    "        # Rescale 0 to 1\n",
    "      #  X_train = X_train / 128\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size,1,1))\n",
    "        fake = np.zeros((batch_size,1,1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of songs\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            imgs = np.array(imgs)\n",
    "            imgs = imgs.reshape(len(imgs),30,1)\n",
    "\n",
    "            # Sample noise and generate a batch of new songs\n",
    "            noise = np.random.normal(0, 1, (batch_size,1,30))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake songs as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "            history['gen'].append(g_loss)\n",
    "            \n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            history['dis'].append(d_loss) \n",
    "            #return (g_loss,d_loss)\n",
    "            # If at save interval => save model\n",
    "#            if epoch % save_interval == 0:\n",
    " #               self.generator.save(\"LSTM_generator.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30, 64)            64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 15, 32)            6144      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 15, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 8, 1)              96        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 1, 8)              0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 1, 32)             288       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 1)              33        \n",
      "=================================================================\n",
      "Total params: 6,625\n",
      "Trainable params: 6,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "exit\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 1, 256)            162816    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1, 256)            394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 30, 256)           394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 30, 256)           394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 30, 256)           394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 30, 128)           32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 30, 128)           16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 30, 1)             129       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 30, 1)             0         \n",
      "=================================================================\n",
      "Total params: 2,183,553\n",
      "Trainable params: 2,183,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstmgan = LSTMGAN()\n",
    "#lstmgan.train(epochs=1000, batch_size=20, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter\n",
      "0 [D loss: 6.943833, acc.: 50.00%] [G loss: 8.042493]\n",
      "1 [D loss: 6.986636, acc.: 50.00%] [G loss: 7.269892]\n",
      "2 [D loss: 5.459478, acc.: 50.00%] [G loss: 6.788233]\n",
      "3 [D loss: 5.980855, acc.: 50.00%] [G loss: 6.398957]\n",
      "4 [D loss: 5.921489, acc.: 50.00%] [G loss: 6.140289]\n",
      "5 [D loss: 5.911006, acc.: 50.00%] [G loss: 5.953754]\n",
      "6 [D loss: 5.690321, acc.: 50.00%] [G loss: 5.855115]\n",
      "7 [D loss: 5.624063, acc.: 50.00%] [G loss: 5.508645]\n",
      "8 [D loss: 4.496562, acc.: 50.00%] [G loss: 5.385685]\n",
      "9 [D loss: 4.779206, acc.: 50.00%] [G loss: 5.180002]\n",
      "10 [D loss: 4.886455, acc.: 50.00%] [G loss: 5.111807]\n",
      "11 [D loss: 4.556971, acc.: 50.00%] [G loss: 5.091269]\n",
      "12 [D loss: 5.008967, acc.: 50.00%] [G loss: 5.075984]\n",
      "13 [D loss: 4.306868, acc.: 50.00%] [G loss: 5.014394]\n",
      "14 [D loss: 5.340785, acc.: 50.00%] [G loss: 5.063470]\n",
      "15 [D loss: 3.971165, acc.: 50.00%] [G loss: 4.943028]\n",
      "16 [D loss: 4.415992, acc.: 50.00%] [G loss: 4.870657]\n",
      "17 [D loss: 5.100513, acc.: 50.00%] [G loss: 4.823085]\n",
      "18 [D loss: 4.261361, acc.: 50.00%] [G loss: 4.933547]\n",
      "19 [D loss: 5.044971, acc.: 50.00%] [G loss: 4.851439]\n",
      "20 [D loss: 4.398913, acc.: 50.00%] [G loss: 4.818487]\n",
      "21 [D loss: 3.014487, acc.: 50.00%] [G loss: 4.697918]\n",
      "22 [D loss: 4.473634, acc.: 50.00%] [G loss: 4.647685]\n",
      "23 [D loss: 2.456265, acc.: 50.00%] [G loss: 4.600738]\n",
      "24 [D loss: 3.753042, acc.: 50.00%] [G loss: 4.597903]\n",
      "25 [D loss: 3.208587, acc.: 50.00%] [G loss: 4.504984]\n",
      "26 [D loss: 3.013299, acc.: 50.00%] [G loss: 4.447651]\n",
      "27 [D loss: 3.648266, acc.: 50.00%] [G loss: 4.586841]\n",
      "28 [D loss: 3.176239, acc.: 50.00%] [G loss: 4.508337]\n",
      "29 [D loss: 3.728685, acc.: 50.00%] [G loss: 4.458537]\n",
      "30 [D loss: 3.521628, acc.: 50.00%] [G loss: 4.401905]\n",
      "31 [D loss: 3.099214, acc.: 50.00%] [G loss: 4.379336]\n",
      "32 [D loss: 3.043258, acc.: 50.00%] [G loss: 4.366652]\n",
      "33 [D loss: 3.033000, acc.: 50.00%] [G loss: 4.410841]\n",
      "34 [D loss: 3.383697, acc.: 50.00%] [G loss: 4.291654]\n",
      "35 [D loss: 2.352234, acc.: 50.00%] [G loss: 4.286590]\n",
      "36 [D loss: 2.438498, acc.: 50.00%] [G loss: 4.241292]\n",
      "37 [D loss: 2.303723, acc.: 50.00%] [G loss: 4.195609]\n",
      "38 [D loss: 2.684662, acc.: 50.00%] [G loss: 4.155962]\n",
      "39 [D loss: 2.721945, acc.: 50.00%] [G loss: 4.175711]\n",
      "40 [D loss: 2.232781, acc.: 50.00%] [G loss: 4.115385]\n",
      "41 [D loss: 2.650471, acc.: 50.00%] [G loss: 4.162383]\n",
      "42 [D loss: 2.089210, acc.: 50.00%] [G loss: 4.067717]\n",
      "43 [D loss: 2.055625, acc.: 50.00%] [G loss: 4.109993]\n",
      "44 [D loss: 2.672897, acc.: 50.00%] [G loss: 4.149406]\n",
      "45 [D loss: 2.033740, acc.: 50.00%] [G loss: 4.035985]\n",
      "46 [D loss: 2.001547, acc.: 50.00%] [G loss: 3.961071]\n",
      "47 [D loss: 2.114710, acc.: 50.00%] [G loss: 4.023255]\n",
      "48 [D loss: 2.623929, acc.: 50.00%] [G loss: 4.022231]\n",
      "49 [D loss: 2.041135, acc.: 50.00%] [G loss: 3.997230]\n",
      "50 [D loss: 3.125719, acc.: 50.00%] [G loss: 3.857487]\n",
      "51 [D loss: 2.325156, acc.: 50.00%] [G loss: 4.037990]\n",
      "52 [D loss: 2.111932, acc.: 50.00%] [G loss: 3.985910]\n",
      "53 [D loss: 1.927470, acc.: 50.00%] [G loss: 3.979816]\n",
      "54 [D loss: 2.252467, acc.: 50.00%] [G loss: 3.988500]\n",
      "55 [D loss: 2.938004, acc.: 50.00%] [G loss: 3.960498]\n",
      "56 [D loss: 2.318985, acc.: 50.00%] [G loss: 3.906127]\n",
      "57 [D loss: 2.000332, acc.: 50.00%] [G loss: 3.814771]\n",
      "58 [D loss: 2.047709, acc.: 50.00%] [G loss: 3.976067]\n",
      "59 [D loss: 2.025940, acc.: 50.00%] [G loss: 3.875732]\n",
      "60 [D loss: 2.402636, acc.: 50.00%] [G loss: 3.913160]\n",
      "61 [D loss: 2.000160, acc.: 50.00%] [G loss: 3.764255]\n",
      "62 [D loss: 1.942087, acc.: 50.00%] [G loss: 3.839010]\n",
      "63 [D loss: 1.992326, acc.: 50.00%] [G loss: 3.890208]\n",
      "64 [D loss: 2.379336, acc.: 50.00%] [G loss: 3.738100]\n",
      "65 [D loss: 1.957288, acc.: 50.00%] [G loss: 3.814263]\n",
      "66 [D loss: 1.999135, acc.: 50.00%] [G loss: 3.710864]\n",
      "67 [D loss: 2.647591, acc.: 50.00%] [G loss: 3.705625]\n",
      "68 [D loss: 1.933398, acc.: 50.00%] [G loss: 3.675396]\n",
      "69 [D loss: 1.912138, acc.: 50.00%] [G loss: 3.623567]\n",
      "70 [D loss: 2.038237, acc.: 50.00%] [G loss: 3.649723]\n",
      "71 [D loss: 1.899514, acc.: 50.00%] [G loss: 3.739743]\n",
      "72 [D loss: 1.875011, acc.: 50.00%] [G loss: 3.710391]\n",
      "73 [D loss: 1.937201, acc.: 50.00%] [G loss: 3.649829]\n",
      "74 [D loss: 1.939116, acc.: 50.00%] [G loss: 3.863351]\n",
      "75 [D loss: 1.854144, acc.: 50.00%] [G loss: 3.649101]\n",
      "76 [D loss: 1.929783, acc.: 50.00%] [G loss: 3.735265]\n",
      "77 [D loss: 1.859553, acc.: 50.00%] [G loss: 3.639447]\n",
      "78 [D loss: 2.195595, acc.: 50.00%] [G loss: 3.693932]\n",
      "79 [D loss: 1.852953, acc.: 50.00%] [G loss: 3.711165]\n",
      "80 [D loss: 2.198265, acc.: 50.00%] [G loss: 3.587963]\n",
      "81 [D loss: 1.886093, acc.: 50.00%] [G loss: 3.619577]\n",
      "82 [D loss: 2.023608, acc.: 50.00%] [G loss: 3.578059]\n",
      "83 [D loss: 1.899813, acc.: 50.00%] [G loss: 3.545859]\n",
      "84 [D loss: 1.869790, acc.: 50.00%] [G loss: 3.556863]\n",
      "85 [D loss: 1.929073, acc.: 50.00%] [G loss: 3.521243]\n",
      "86 [D loss: 2.128409, acc.: 50.00%] [G loss: 3.636942]\n",
      "87 [D loss: 1.867327, acc.: 50.00%] [G loss: 3.687280]\n",
      "88 [D loss: 1.748867, acc.: 50.00%] [G loss: 3.617049]\n",
      "89 [D loss: 1.817429, acc.: 50.00%] [G loss: 3.511616]\n",
      "90 [D loss: 1.784152, acc.: 50.00%] [G loss: 3.598134]\n",
      "91 [D loss: 1.835613, acc.: 50.00%] [G loss: 3.528444]\n",
      "92 [D loss: 1.850810, acc.: 50.00%] [G loss: 3.613396]\n",
      "93 [D loss: 1.837684, acc.: 50.00%] [G loss: 3.561952]\n",
      "94 [D loss: 1.747866, acc.: 50.00%] [G loss: 3.457073]\n",
      "95 [D loss: 1.784574, acc.: 50.00%] [G loss: 3.549317]\n",
      "96 [D loss: 1.800474, acc.: 50.00%] [G loss: 3.526918]\n",
      "97 [D loss: 1.975376, acc.: 50.00%] [G loss: 3.548428]\n",
      "98 [D loss: 1.729095, acc.: 50.00%] [G loss: 3.451181]\n",
      "99 [D loss: 1.816093, acc.: 50.00%] [G loss: 3.528673]\n",
      "100 [D loss: 1.790829, acc.: 50.00%] [G loss: 3.536182]\n",
      "101 [D loss: 1.818213, acc.: 50.00%] [G loss: 3.456111]\n",
      "102 [D loss: 1.815564, acc.: 50.00%] [G loss: 3.479801]\n",
      "103 [D loss: 1.818247, acc.: 50.00%] [G loss: 3.468172]\n",
      "104 [D loss: 1.767954, acc.: 50.00%] [G loss: 3.463364]\n",
      "105 [D loss: 1.764290, acc.: 50.00%] [G loss: 3.458024]\n",
      "106 [D loss: 1.754239, acc.: 50.00%] [G loss: 3.360497]\n",
      "107 [D loss: 1.765655, acc.: 50.00%] [G loss: 3.484852]\n",
      "108 [D loss: 1.851047, acc.: 50.00%] [G loss: 3.518191]\n",
      "109 [D loss: 1.726042, acc.: 50.00%] [G loss: 3.439381]\n",
      "110 [D loss: 1.781688, acc.: 50.00%] [G loss: 3.432299]\n",
      "111 [D loss: 1.754787, acc.: 50.00%] [G loss: 3.400890]\n",
      "112 [D loss: 1.751022, acc.: 50.00%] [G loss: 3.368579]\n",
      "113 [D loss: 1.797883, acc.: 50.00%] [G loss: 3.425392]\n",
      "114 [D loss: 1.753649, acc.: 50.00%] [G loss: 3.393602]\n",
      "115 [D loss: 1.719426, acc.: 50.00%] [G loss: 3.417604]\n",
      "116 [D loss: 1.685197, acc.: 50.00%] [G loss: 3.509765]\n",
      "117 [D loss: 1.707826, acc.: 50.00%] [G loss: 3.520756]\n",
      "118 [D loss: 1.682163, acc.: 50.00%] [G loss: 3.394773]\n",
      "119 [D loss: 1.696784, acc.: 50.00%] [G loss: 3.454325]\n",
      "120 [D loss: 1.746256, acc.: 50.00%] [G loss: 3.366064]\n",
      "121 [D loss: 1.773760, acc.: 50.00%] [G loss: 3.302887]\n",
      "122 [D loss: 1.719663, acc.: 50.00%] [G loss: 3.428463]\n",
      "123 [D loss: 1.670048, acc.: 50.00%] [G loss: 3.416843]\n",
      "124 [D loss: 1.764073, acc.: 50.00%] [G loss: 3.394164]\n",
      "125 [D loss: 1.728600, acc.: 50.00%] [G loss: 3.372115]\n",
      "126 [D loss: 1.691843, acc.: 50.00%] [G loss: 3.418881]\n",
      "127 [D loss: 1.743561, acc.: 50.00%] [G loss: 3.388425]\n",
      "128 [D loss: 1.762849, acc.: 50.00%] [G loss: 3.230493]\n",
      "129 [D loss: 1.753150, acc.: 50.00%] [G loss: 3.421855]\n",
      "130 [D loss: 1.684275, acc.: 50.00%] [G loss: 3.416913]\n",
      "131 [D loss: 1.720192, acc.: 50.00%] [G loss: 3.292590]\n",
      "132 [D loss: 1.671990, acc.: 50.00%] [G loss: 3.347444]\n",
      "133 [D loss: 1.799196, acc.: 50.00%] [G loss: 3.298457]\n",
      "134 [D loss: 1.731838, acc.: 50.00%] [G loss: 3.311890]\n",
      "135 [D loss: 1.660842, acc.: 50.00%] [G loss: 3.348907]\n",
      "136 [D loss: 1.744502, acc.: 50.00%] [G loss: 3.312400]\n",
      "137 [D loss: 1.730834, acc.: 50.00%] [G loss: 3.278773]\n",
      "138 [D loss: 1.669968, acc.: 50.00%] [G loss: 3.290584]\n",
      "139 [D loss: 1.617148, acc.: 50.00%] [G loss: 3.294221]\n",
      "140 [D loss: 1.707225, acc.: 50.00%] [G loss: 3.312618]\n",
      "141 [D loss: 1.700106, acc.: 50.00%] [G loss: 3.252270]\n",
      "142 [D loss: 1.737186, acc.: 50.00%] [G loss: 3.247141]\n",
      "143 [D loss: 1.661226, acc.: 50.00%] [G loss: 3.300047]\n",
      "144 [D loss: 1.661974, acc.: 50.00%] [G loss: 3.230960]\n",
      "145 [D loss: 1.657699, acc.: 50.00%] [G loss: 3.240588]\n",
      "146 [D loss: 1.614952, acc.: 50.00%] [G loss: 3.223118]\n",
      "147 [D loss: 1.639501, acc.: 50.00%] [G loss: 3.266687]\n",
      "148 [D loss: 1.722210, acc.: 50.00%] [G loss: 3.330585]\n",
      "149 [D loss: 1.727639, acc.: 50.00%] [G loss: 3.339124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 [D loss: 1.588206, acc.: 50.00%] [G loss: 3.339317]\n",
      "151 [D loss: 1.545399, acc.: 50.00%] [G loss: 3.210773]\n",
      "152 [D loss: 1.670727, acc.: 50.00%] [G loss: 3.181411]\n",
      "153 [D loss: 1.599732, acc.: 50.00%] [G loss: 3.221402]\n",
      "154 [D loss: 1.651225, acc.: 50.00%] [G loss: 3.174929]\n",
      "155 [D loss: 1.606096, acc.: 50.00%] [G loss: 3.209201]\n",
      "156 [D loss: 1.620821, acc.: 50.00%] [G loss: 3.224905]\n",
      "157 [D loss: 1.690832, acc.: 50.00%] [G loss: 3.111488]\n",
      "158 [D loss: 1.644522, acc.: 50.00%] [G loss: 3.182800]\n",
      "159 [D loss: 1.627561, acc.: 50.00%] [G loss: 3.136617]\n",
      "160 [D loss: 1.579955, acc.: 50.00%] [G loss: 3.270700]\n",
      "161 [D loss: 1.673850, acc.: 50.00%] [G loss: 3.137822]\n",
      "162 [D loss: 1.656637, acc.: 50.00%] [G loss: 3.179082]\n",
      "163 [D loss: 1.626731, acc.: 50.00%] [G loss: 3.257203]\n",
      "164 [D loss: 1.646970, acc.: 50.00%] [G loss: 3.230544]\n",
      "165 [D loss: 1.578369, acc.: 50.00%] [G loss: 3.187549]\n",
      "166 [D loss: 1.664343, acc.: 50.00%] [G loss: 3.081548]\n",
      "167 [D loss: 1.670784, acc.: 50.00%] [G loss: 3.117593]\n",
      "168 [D loss: 1.600949, acc.: 50.00%] [G loss: 3.167685]\n",
      "169 [D loss: 1.533466, acc.: 50.00%] [G loss: 3.097314]\n",
      "170 [D loss: 1.695120, acc.: 50.00%] [G loss: 3.247942]\n",
      "171 [D loss: 1.580747, acc.: 50.00%] [G loss: 3.160917]\n",
      "172 [D loss: 1.642984, acc.: 50.00%] [G loss: 3.106295]\n",
      "173 [D loss: 1.594683, acc.: 50.00%] [G loss: 3.062330]\n",
      "174 [D loss: 1.591950, acc.: 50.00%] [G loss: 3.202199]\n",
      "175 [D loss: 1.647505, acc.: 50.00%] [G loss: 3.185114]\n",
      "176 [D loss: 1.682501, acc.: 50.00%] [G loss: 3.104032]\n",
      "177 [D loss: 1.572130, acc.: 50.00%] [G loss: 3.056147]\n",
      "178 [D loss: 1.591018, acc.: 50.00%] [G loss: 3.154089]\n",
      "179 [D loss: 1.535511, acc.: 50.00%] [G loss: 3.204085]\n",
      "180 [D loss: 1.650980, acc.: 50.00%] [G loss: 3.115115]\n",
      "181 [D loss: 1.591043, acc.: 50.00%] [G loss: 3.101058]\n",
      "182 [D loss: 1.614132, acc.: 50.00%] [G loss: 3.077585]\n",
      "183 [D loss: 1.571656, acc.: 50.00%] [G loss: 2.943200]\n",
      "184 [D loss: 1.586085, acc.: 50.00%] [G loss: 3.142177]\n",
      "185 [D loss: 1.529175, acc.: 50.00%] [G loss: 3.139291]\n",
      "186 [D loss: 1.570300, acc.: 50.00%] [G loss: 3.039271]\n",
      "187 [D loss: 1.646274, acc.: 50.00%] [G loss: 3.093872]\n",
      "188 [D loss: 1.604319, acc.: 50.00%] [G loss: 3.020943]\n",
      "189 [D loss: 1.576194, acc.: 50.00%] [G loss: 3.118939]\n",
      "190 [D loss: 1.556197, acc.: 50.00%] [G loss: 3.014359]\n",
      "191 [D loss: 1.614610, acc.: 50.00%] [G loss: 3.114322]\n",
      "192 [D loss: 1.590663, acc.: 50.00%] [G loss: 2.975478]\n",
      "193 [D loss: 1.557060, acc.: 50.00%] [G loss: 2.974550]\n",
      "194 [D loss: 1.550614, acc.: 50.00%] [G loss: 2.918247]\n",
      "195 [D loss: 1.545805, acc.: 50.00%] [G loss: 3.074542]\n",
      "196 [D loss: 1.552781, acc.: 50.00%] [G loss: 3.025793]\n",
      "197 [D loss: 1.611884, acc.: 50.00%] [G loss: 3.056201]\n",
      "198 [D loss: 1.532496, acc.: 50.00%] [G loss: 3.018574]\n",
      "199 [D loss: 1.586565, acc.: 50.00%] [G loss: 3.035450]\n",
      "200 [D loss: 1.561159, acc.: 50.00%] [G loss: 3.107110]\n",
      "201 [D loss: 1.559131, acc.: 50.00%] [G loss: 3.070227]\n",
      "202 [D loss: 1.562441, acc.: 50.00%] [G loss: 3.041325]\n",
      "203 [D loss: 1.567670, acc.: 50.00%] [G loss: 2.952251]\n",
      "204 [D loss: 1.530698, acc.: 50.00%] [G loss: 2.961047]\n",
      "205 [D loss: 1.563754, acc.: 50.00%] [G loss: 2.973296]\n",
      "206 [D loss: 1.541751, acc.: 50.00%] [G loss: 2.978082]\n",
      "207 [D loss: 1.494332, acc.: 50.00%] [G loss: 3.006434]\n",
      "208 [D loss: 1.567541, acc.: 50.00%] [G loss: 3.027225]\n",
      "209 [D loss: 1.546224, acc.: 50.00%] [G loss: 3.044703]\n",
      "210 [D loss: 1.592742, acc.: 50.00%] [G loss: 2.933506]\n",
      "211 [D loss: 1.531674, acc.: 50.00%] [G loss: 3.017823]\n",
      "212 [D loss: 1.553928, acc.: 50.00%] [G loss: 3.024527]\n",
      "213 [D loss: 1.536266, acc.: 50.00%] [G loss: 2.985748]\n",
      "214 [D loss: 1.457193, acc.: 50.00%] [G loss: 2.966811]\n",
      "215 [D loss: 1.496365, acc.: 50.00%] [G loss: 2.959548]\n",
      "216 [D loss: 1.560886, acc.: 50.00%] [G loss: 2.901520]\n",
      "217 [D loss: 1.537603, acc.: 50.00%] [G loss: 2.813208]\n",
      "218 [D loss: 1.554551, acc.: 50.00%] [G loss: 2.769262]\n",
      "219 [D loss: 1.538837, acc.: 50.00%] [G loss: 4.984673]\n",
      "220 [D loss: 1.521338, acc.: 50.00%] [G loss: 2.959691]\n",
      "221 [D loss: 1.506103, acc.: 50.00%] [G loss: 3.014024]\n",
      "222 [D loss: 1.503328, acc.: 50.00%] [G loss: 2.968709]\n",
      "223 [D loss: 1.535658, acc.: 50.00%] [G loss: 2.985477]\n",
      "224 [D loss: 1.496733, acc.: 50.00%] [G loss: 2.945403]\n",
      "225 [D loss: 1.530409, acc.: 50.00%] [G loss: 2.917281]\n",
      "226 [D loss: 1.535140, acc.: 50.00%] [G loss: 2.913860]\n",
      "227 [D loss: 1.532731, acc.: 50.00%] [G loss: 3.054805]\n",
      "228 [D loss: 1.532061, acc.: 50.00%] [G loss: 2.992320]\n",
      "229 [D loss: 1.539402, acc.: 50.00%] [G loss: 2.989148]\n",
      "230 [D loss: 1.475551, acc.: 50.00%] [G loss: 2.934515]\n",
      "231 [D loss: 1.500798, acc.: 50.00%] [G loss: 2.799293]\n",
      "232 [D loss: 1.478729, acc.: 50.00%] [G loss: 2.889053]\n",
      "233 [D loss: 1.521219, acc.: 50.00%] [G loss: 2.941523]\n",
      "234 [D loss: 1.524351, acc.: 50.00%] [G loss: 2.930771]\n",
      "235 [D loss: 1.476261, acc.: 50.00%] [G loss: 2.888705]\n",
      "236 [D loss: 1.490526, acc.: 50.00%] [G loss: 2.842184]\n",
      "237 [D loss: 1.481342, acc.: 50.00%] [G loss: 2.783799]\n",
      "238 [D loss: 1.480325, acc.: 50.00%] [G loss: 2.821019]\n",
      "239 [D loss: 1.508252, acc.: 50.00%] [G loss: 2.928932]\n",
      "240 [D loss: 1.474587, acc.: 50.00%] [G loss: 2.813723]\n",
      "241 [D loss: 1.442318, acc.: 50.00%] [G loss: 2.732835]\n",
      "242 [D loss: 1.498207, acc.: 50.00%] [G loss: 2.909584]\n",
      "243 [D loss: 1.493031, acc.: 50.00%] [G loss: 2.839709]\n",
      "244 [D loss: 1.468026, acc.: 50.00%] [G loss: 2.780359]\n",
      "245 [D loss: 1.397001, acc.: 50.00%] [G loss: 2.842903]\n",
      "246 [D loss: 1.476317, acc.: 50.00%] [G loss: 2.835437]\n",
      "247 [D loss: 1.477091, acc.: 50.00%] [G loss: 2.888884]\n",
      "248 [D loss: 1.460690, acc.: 50.00%] [G loss: 2.780683]\n",
      "249 [D loss: 1.459718, acc.: 50.00%] [G loss: 2.776391]\n",
      "250 [D loss: 1.534258, acc.: 50.00%] [G loss: 2.836551]\n",
      "251 [D loss: 1.434448, acc.: 50.00%] [G loss: 2.908060]\n",
      "252 [D loss: 1.418443, acc.: 50.00%] [G loss: 2.777063]\n",
      "253 [D loss: 1.422165, acc.: 50.00%] [G loss: 2.827149]\n",
      "254 [D loss: 1.413887, acc.: 50.00%] [G loss: 2.716686]\n",
      "255 [D loss: 1.530161, acc.: 50.00%] [G loss: 2.759602]\n",
      "256 [D loss: 1.449625, acc.: 50.00%] [G loss: 2.716300]\n",
      "257 [D loss: 1.472079, acc.: 50.00%] [G loss: 2.835445]\n",
      "258 [D loss: 1.496886, acc.: 50.00%] [G loss: 2.650152]\n",
      "259 [D loss: 1.415114, acc.: 50.00%] [G loss: 2.752594]\n",
      "260 [D loss: 1.522371, acc.: 50.00%] [G loss: 2.657443]\n",
      "261 [D loss: 1.420354, acc.: 50.00%] [G loss: 3.401004]\n",
      "262 [D loss: 1.453903, acc.: 50.00%] [G loss: 2.827361]\n",
      "263 [D loss: 1.484057, acc.: 50.00%] [G loss: 2.842321]\n",
      "264 [D loss: 1.463089, acc.: 50.00%] [G loss: 2.766429]\n",
      "265 [D loss: 1.446682, acc.: 50.00%] [G loss: 2.833514]\n",
      "266 [D loss: 1.458256, acc.: 50.00%] [G loss: 2.762211]\n",
      "267 [D loss: 1.467276, acc.: 50.00%] [G loss: 2.821792]\n",
      "268 [D loss: 1.439642, acc.: 50.00%] [G loss: 2.689218]\n",
      "269 [D loss: 1.409105, acc.: 50.00%] [G loss: 2.823154]\n",
      "270 [D loss: 1.447161, acc.: 50.00%] [G loss: 2.686525]\n",
      "271 [D loss: 1.406958, acc.: 50.00%] [G loss: 2.866114]\n",
      "272 [D loss: 1.429148, acc.: 50.00%] [G loss: 2.766577]\n",
      "273 [D loss: 1.491870, acc.: 50.00%] [G loss: 2.804176]\n",
      "274 [D loss: 1.486537, acc.: 50.00%] [G loss: 2.789112]\n",
      "275 [D loss: 1.426617, acc.: 50.00%] [G loss: 2.748429]\n",
      "276 [D loss: 1.440391, acc.: 50.00%] [G loss: 2.863487]\n",
      "277 [D loss: 1.422850, acc.: 50.00%] [G loss: 2.673600]\n",
      "278 [D loss: 1.436565, acc.: 50.00%] [G loss: 2.713129]\n",
      "279 [D loss: 1.380056, acc.: 50.00%] [G loss: 2.655252]\n",
      "280 [D loss: 1.438509, acc.: 50.00%] [G loss: 2.874215]\n",
      "281 [D loss: 1.399905, acc.: 50.00%] [G loss: 2.695674]\n",
      "282 [D loss: 1.352704, acc.: 50.00%] [G loss: 2.780940]\n",
      "283 [D loss: 1.390455, acc.: 50.00%] [G loss: 2.777298]\n",
      "284 [D loss: 1.454344, acc.: 50.00%] [G loss: 2.764707]\n",
      "285 [D loss: 1.344509, acc.: 50.00%] [G loss: 2.709015]\n",
      "286 [D loss: 1.344630, acc.: 50.00%] [G loss: 2.663984]\n",
      "287 [D loss: 1.464380, acc.: 50.00%] [G loss: 2.711893]\n",
      "288 [D loss: 1.439713, acc.: 50.00%] [G loss: 2.661321]\n",
      "289 [D loss: 1.421606, acc.: 50.00%] [G loss: 2.819343]\n",
      "290 [D loss: 1.363455, acc.: 50.00%] [G loss: 2.685640]\n",
      "291 [D loss: 1.408453, acc.: 50.00%] [G loss: 2.626193]\n",
      "292 [D loss: 1.371049, acc.: 50.00%] [G loss: 2.672132]\n",
      "293 [D loss: 1.473444, acc.: 50.00%] [G loss: 2.721535]\n",
      "294 [D loss: 1.400452, acc.: 50.00%] [G loss: 2.696114]\n",
      "295 [D loss: 1.474095, acc.: 50.00%] [G loss: 2.707495]\n",
      "296 [D loss: 1.370234, acc.: 50.00%] [G loss: 2.650958]\n",
      "297 [D loss: 1.374020, acc.: 50.00%] [G loss: 2.737626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298 [D loss: 1.390496, acc.: 50.00%] [G loss: 2.701342]\n",
      "299 [D loss: 1.439687, acc.: 50.00%] [G loss: 2.669326]\n",
      "300 [D loss: 1.398539, acc.: 50.00%] [G loss: 2.709604]\n",
      "301 [D loss: 1.410280, acc.: 50.00%] [G loss: 2.693702]\n",
      "302 [D loss: 1.347769, acc.: 50.00%] [G loss: 2.584565]\n",
      "303 [D loss: 1.387698, acc.: 50.00%] [G loss: 2.725148]\n",
      "304 [D loss: 1.419271, acc.: 50.00%] [G loss: 2.665686]\n",
      "305 [D loss: 1.391383, acc.: 50.00%] [G loss: 2.786353]\n",
      "306 [D loss: 1.389575, acc.: 50.00%] [G loss: 2.686922]\n",
      "307 [D loss: 1.355439, acc.: 50.00%] [G loss: 2.681017]\n",
      "308 [D loss: 1.393532, acc.: 50.00%] [G loss: 2.612359]\n",
      "309 [D loss: 1.380939, acc.: 50.00%] [G loss: 2.520457]\n",
      "310 [D loss: 1.364683, acc.: 50.00%] [G loss: 2.594200]\n",
      "311 [D loss: 1.387408, acc.: 50.00%] [G loss: 2.709460]\n",
      "312 [D loss: 1.371437, acc.: 50.00%] [G loss: 2.632576]\n",
      "313 [D loss: 1.413314, acc.: 50.00%] [G loss: 2.540734]\n",
      "314 [D loss: 1.351643, acc.: 50.00%] [G loss: 2.623777]\n",
      "315 [D loss: 1.370261, acc.: 50.00%] [G loss: 2.593184]\n",
      "316 [D loss: 1.402100, acc.: 50.00%] [G loss: 2.681896]\n",
      "317 [D loss: 1.399323, acc.: 50.00%] [G loss: 2.605919]\n",
      "318 [D loss: 1.382756, acc.: 50.00%] [G loss: 2.588989]\n",
      "319 [D loss: 1.323122, acc.: 50.00%] [G loss: 2.708869]\n",
      "320 [D loss: 1.338774, acc.: 50.00%] [G loss: 2.562641]\n",
      "321 [D loss: 1.397324, acc.: 50.00%] [G loss: 2.585826]\n",
      "322 [D loss: 1.317793, acc.: 50.00%] [G loss: 2.553850]\n",
      "323 [D loss: 1.390087, acc.: 50.00%] [G loss: 2.819412]\n",
      "324 [D loss: 1.360984, acc.: 50.00%] [G loss: 2.660944]\n",
      "325 [D loss: 1.340593, acc.: 50.00%] [G loss: 2.578004]\n",
      "326 [D loss: 1.328897, acc.: 50.00%] [G loss: 2.607588]\n",
      "327 [D loss: 1.371577, acc.: 50.00%] [G loss: 2.592865]\n",
      "328 [D loss: 1.371579, acc.: 50.00%] [G loss: 2.632877]\n",
      "329 [D loss: 1.353350, acc.: 50.00%] [G loss: 2.669021]\n",
      "330 [D loss: 1.339896, acc.: 50.00%] [G loss: 2.604132]\n",
      "331 [D loss: 1.377194, acc.: 50.00%] [G loss: 2.735460]\n",
      "332 [D loss: 1.363118, acc.: 50.00%] [G loss: 2.652746]\n",
      "333 [D loss: 1.385692, acc.: 50.00%] [G loss: 2.539381]\n",
      "334 [D loss: 1.346517, acc.: 50.00%] [G loss: 2.663057]\n",
      "335 [D loss: 1.388912, acc.: 50.00%] [G loss: 2.543992]\n",
      "336 [D loss: 1.365392, acc.: 50.00%] [G loss: 2.659400]\n",
      "337 [D loss: 1.352312, acc.: 50.00%] [G loss: 2.610822]\n",
      "338 [D loss: 1.298534, acc.: 50.00%] [G loss: 2.588488]\n",
      "339 [D loss: 1.348603, acc.: 50.00%] [G loss: 2.650474]\n",
      "340 [D loss: 1.362420, acc.: 50.00%] [G loss: 2.538605]\n",
      "341 [D loss: 1.335339, acc.: 50.00%] [G loss: 2.455698]\n",
      "342 [D loss: 1.341255, acc.: 50.00%] [G loss: 2.536976]\n",
      "343 [D loss: 1.355283, acc.: 50.00%] [G loss: 2.493609]\n",
      "344 [D loss: 1.330478, acc.: 50.00%] [G loss: 2.359555]\n",
      "345 [D loss: 1.398845, acc.: 50.00%] [G loss: 2.519653]\n",
      "346 [D loss: 1.293739, acc.: 50.00%] [G loss: 2.662805]\n",
      "347 [D loss: 1.397821, acc.: 50.00%] [G loss: 2.420684]\n",
      "348 [D loss: 1.312868, acc.: 50.00%] [G loss: 2.686254]\n",
      "349 [D loss: 1.352946, acc.: 50.00%] [G loss: 2.398757]\n",
      "350 [D loss: 1.287990, acc.: 50.00%] [G loss: 2.630734]\n",
      "351 [D loss: 1.315504, acc.: 50.00%] [G loss: 2.585019]\n",
      "352 [D loss: 1.337517, acc.: 50.00%] [G loss: 2.606990]\n",
      "353 [D loss: 1.364297, acc.: 50.00%] [G loss: 2.564006]\n",
      "354 [D loss: 1.319627, acc.: 50.00%] [G loss: 2.527204]\n",
      "355 [D loss: 1.310980, acc.: 50.00%] [G loss: 2.622042]\n",
      "356 [D loss: 1.322667, acc.: 50.00%] [G loss: 2.612128]\n",
      "357 [D loss: 1.347064, acc.: 50.00%] [G loss: 2.613196]\n",
      "358 [D loss: 1.274537, acc.: 50.00%] [G loss: 2.551315]\n",
      "359 [D loss: 1.335117, acc.: 50.00%] [G loss: 2.571273]\n",
      "360 [D loss: 1.308007, acc.: 50.00%] [G loss: 2.650984]\n",
      "361 [D loss: 1.356682, acc.: 50.00%] [G loss: 2.580449]\n",
      "362 [D loss: 1.328947, acc.: 50.00%] [G loss: 2.577409]\n",
      "363 [D loss: 1.300212, acc.: 50.00%] [G loss: 2.531152]\n",
      "364 [D loss: 1.300060, acc.: 50.00%] [G loss: 2.677144]\n",
      "365 [D loss: 1.273136, acc.: 50.00%] [G loss: 2.678350]\n",
      "366 [D loss: 1.293373, acc.: 50.00%] [G loss: 2.524488]\n",
      "367 [D loss: 1.273127, acc.: 50.00%] [G loss: 2.500683]\n",
      "368 [D loss: 1.388986, acc.: 50.00%] [G loss: 2.531433]\n",
      "369 [D loss: 1.271053, acc.: 50.00%] [G loss: 2.500399]\n",
      "370 [D loss: 1.303583, acc.: 50.00%] [G loss: 2.523196]\n",
      "371 [D loss: 1.297453, acc.: 50.00%] [G loss: 2.565657]\n",
      "372 [D loss: 1.319384, acc.: 50.00%] [G loss: 2.501657]\n",
      "373 [D loss: 1.305239, acc.: 50.00%] [G loss: 2.671164]\n",
      "374 [D loss: 1.316185, acc.: 50.00%] [G loss: 2.554788]\n",
      "375 [D loss: 1.255339, acc.: 50.00%] [G loss: 2.553650]\n",
      "376 [D loss: 1.281515, acc.: 50.00%] [G loss: 2.619529]\n",
      "377 [D loss: 1.300177, acc.: 50.00%] [G loss: 2.507710]\n",
      "378 [D loss: 1.317103, acc.: 50.00%] [G loss: 2.541513]\n",
      "379 [D loss: 1.326022, acc.: 50.00%] [G loss: 2.560516]\n",
      "380 [D loss: 1.315629, acc.: 50.00%] [G loss: 2.561868]\n",
      "381 [D loss: 1.285292, acc.: 50.00%] [G loss: 2.465806]\n",
      "382 [D loss: 1.270542, acc.: 50.00%] [G loss: 2.466700]\n",
      "383 [D loss: 1.203775, acc.: 50.00%] [G loss: 2.744022]\n",
      "384 [D loss: 1.362817, acc.: 50.00%] [G loss: 2.478184]\n",
      "385 [D loss: 1.340453, acc.: 50.00%] [G loss: 2.607159]\n",
      "386 [D loss: 1.259772, acc.: 50.00%] [G loss: 2.509706]\n",
      "387 [D loss: 1.273668, acc.: 50.00%] [G loss: 2.589368]\n",
      "388 [D loss: 1.280581, acc.: 50.00%] [G loss: 2.505979]\n",
      "389 [D loss: 1.332829, acc.: 50.00%] [G loss: 2.531988]\n",
      "390 [D loss: 1.299572, acc.: 50.00%] [G loss: 2.497052]\n",
      "391 [D loss: 1.331126, acc.: 50.00%] [G loss: 2.519262]\n",
      "392 [D loss: 1.340722, acc.: 50.00%] [G loss: 2.522899]\n",
      "393 [D loss: 1.291955, acc.: 50.00%] [G loss: 2.465907]\n",
      "394 [D loss: 1.283665, acc.: 50.00%] [G loss: 2.486902]\n",
      "395 [D loss: 1.334352, acc.: 50.00%] [G loss: 2.451910]\n",
      "396 [D loss: 1.275917, acc.: 50.00%] [G loss: 2.489147]\n",
      "397 [D loss: 1.300136, acc.: 50.00%] [G loss: 2.460443]\n",
      "398 [D loss: 1.263907, acc.: 50.00%] [G loss: 2.480185]\n",
      "399 [D loss: 1.248934, acc.: 50.00%] [G loss: 2.502588]\n",
      "400 [D loss: 1.296004, acc.: 50.00%] [G loss: 2.507722]\n",
      "401 [D loss: 1.287590, acc.: 50.00%] [G loss: 2.482898]\n",
      "402 [D loss: 1.241418, acc.: 50.00%] [G loss: 2.404093]\n",
      "403 [D loss: 1.309754, acc.: 50.00%] [G loss: 2.388736]\n",
      "404 [D loss: 1.317646, acc.: 50.00%] [G loss: 2.557246]\n",
      "405 [D loss: 1.261080, acc.: 50.00%] [G loss: 2.466122]\n",
      "406 [D loss: 1.314160, acc.: 50.00%] [G loss: 2.388826]\n",
      "407 [D loss: 1.267171, acc.: 50.00%] [G loss: 2.437602]\n",
      "408 [D loss: 1.255005, acc.: 50.00%] [G loss: 2.428713]\n",
      "409 [D loss: 1.246161, acc.: 50.00%] [G loss: 2.385260]\n",
      "410 [D loss: 1.281288, acc.: 50.00%] [G loss: 2.424567]\n",
      "411 [D loss: 1.255859, acc.: 50.00%] [G loss: 2.370853]\n",
      "412 [D loss: 1.242467, acc.: 50.00%] [G loss: 2.381450]\n",
      "413 [D loss: 1.330705, acc.: 50.00%] [G loss: 2.409173]\n",
      "414 [D loss: 1.236204, acc.: 50.00%] [G loss: 2.257559]\n",
      "415 [D loss: 1.263980, acc.: 50.00%] [G loss: 2.369224]\n",
      "416 [D loss: 1.237160, acc.: 50.00%] [G loss: 2.331320]\n",
      "417 [D loss: 1.246865, acc.: 50.00%] [G loss: 2.448225]\n",
      "418 [D loss: 1.216254, acc.: 50.00%] [G loss: 2.352008]\n",
      "419 [D loss: 1.230355, acc.: 50.00%] [G loss: 2.399116]\n",
      "420 [D loss: 1.283652, acc.: 50.00%] [G loss: 2.418563]\n",
      "421 [D loss: 1.210201, acc.: 50.00%] [G loss: 2.376126]\n",
      "422 [D loss: 1.280218, acc.: 50.00%] [G loss: 2.385630]\n",
      "423 [D loss: 1.212498, acc.: 50.00%] [G loss: 2.359571]\n",
      "424 [D loss: 1.278704, acc.: 50.00%] [G loss: 2.359807]\n",
      "425 [D loss: 1.274268, acc.: 50.00%] [G loss: 2.430501]\n",
      "426 [D loss: 1.209379, acc.: 50.00%] [G loss: 2.376017]\n",
      "427 [D loss: 1.255239, acc.: 50.00%] [G loss: 2.448294]\n",
      "428 [D loss: 1.219283, acc.: 50.00%] [G loss: 2.451060]\n",
      "429 [D loss: 1.297882, acc.: 50.00%] [G loss: 2.289237]\n",
      "430 [D loss: 1.203887, acc.: 50.00%] [G loss: 2.284277]\n",
      "431 [D loss: 1.246020, acc.: 50.00%] [G loss: 2.367792]\n",
      "432 [D loss: 1.237838, acc.: 50.00%] [G loss: 2.432954]\n",
      "433 [D loss: 1.326309, acc.: 50.00%] [G loss: 2.508457]\n",
      "434 [D loss: 1.268892, acc.: 50.00%] [G loss: 2.344320]\n",
      "435 [D loss: 1.235520, acc.: 50.00%] [G loss: 2.454815]\n",
      "436 [D loss: 1.241283, acc.: 50.00%] [G loss: 2.246336]\n",
      "437 [D loss: 1.219539, acc.: 50.00%] [G loss: 2.350228]\n",
      "438 [D loss: 1.245861, acc.: 50.00%] [G loss: 2.339884]\n",
      "439 [D loss: 1.238854, acc.: 50.00%] [G loss: 2.341508]\n",
      "440 [D loss: 1.232907, acc.: 50.00%] [G loss: 2.413368]\n",
      "441 [D loss: 1.203279, acc.: 50.00%] [G loss: 2.302925]\n",
      "442 [D loss: 1.263110, acc.: 50.00%] [G loss: 2.371755]\n",
      "443 [D loss: 1.255637, acc.: 50.00%] [G loss: 2.356352]\n",
      "444 [D loss: 1.203405, acc.: 50.00%] [G loss: 2.349548]\n",
      "445 [D loss: 1.243623, acc.: 50.00%] [G loss: 2.416906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446 [D loss: 1.211328, acc.: 50.00%] [G loss: 2.311930]\n",
      "447 [D loss: 1.263301, acc.: 50.00%] [G loss: 2.247502]\n",
      "448 [D loss: 1.302162, acc.: 50.00%] [G loss: 2.248521]\n",
      "449 [D loss: 1.255655, acc.: 50.00%] [G loss: 2.186936]\n",
      "450 [D loss: 1.248091, acc.: 50.00%] [G loss: 2.314387]\n",
      "451 [D loss: 1.243590, acc.: 50.00%] [G loss: 3.660735]\n",
      "452 [D loss: 1.272935, acc.: 50.00%] [G loss: 2.554091]\n",
      "453 [D loss: 1.283533, acc.: 50.00%] [G loss: 2.256343]\n",
      "454 [D loss: 1.237268, acc.: 50.00%] [G loss: 2.341682]\n",
      "455 [D loss: 1.258959, acc.: 50.00%] [G loss: 2.297527]\n",
      "456 [D loss: 1.253648, acc.: 50.00%] [G loss: 2.303366]\n",
      "457 [D loss: 1.196836, acc.: 50.00%] [G loss: 2.285919]\n",
      "458 [D loss: 1.221256, acc.: 50.00%] [G loss: 2.251356]\n",
      "459 [D loss: 1.215313, acc.: 50.00%] [G loss: 2.114528]\n",
      "460 [D loss: 1.238255, acc.: 50.00%] [G loss: 2.405953]\n",
      "461 [D loss: 1.277164, acc.: 50.00%] [G loss: 2.333856]\n",
      "462 [D loss: 1.209219, acc.: 50.00%] [G loss: 2.116475]\n",
      "463 [D loss: 1.253181, acc.: 50.00%] [G loss: 2.489392]\n",
      "464 [D loss: 1.280560, acc.: 50.00%] [G loss: 2.259114]\n",
      "465 [D loss: 1.166228, acc.: 50.00%] [G loss: 2.254083]\n",
      "466 [D loss: 1.225605, acc.: 50.00%] [G loss: 2.335796]\n",
      "467 [D loss: 1.282450, acc.: 50.00%] [G loss: 2.345337]\n",
      "468 [D loss: 1.184263, acc.: 50.00%] [G loss: 2.311035]\n",
      "469 [D loss: 1.236760, acc.: 50.00%] [G loss: 2.391115]\n",
      "470 [D loss: 1.179771, acc.: 50.00%] [G loss: 2.372109]\n",
      "471 [D loss: 1.211817, acc.: 50.00%] [G loss: 2.317252]\n",
      "472 [D loss: 1.231071, acc.: 50.00%] [G loss: 2.429203]\n",
      "473 [D loss: 1.245965, acc.: 50.00%] [G loss: 2.374943]\n",
      "474 [D loss: 1.216318, acc.: 50.00%] [G loss: 2.222950]\n",
      "475 [D loss: 1.181486, acc.: 50.00%] [G loss: 2.286186]\n",
      "476 [D loss: 1.243102, acc.: 50.00%] [G loss: 2.222692]\n",
      "477 [D loss: 1.242412, acc.: 50.00%] [G loss: 2.349640]\n",
      "478 [D loss: 1.193698, acc.: 50.00%] [G loss: 2.228066]\n",
      "479 [D loss: 1.197208, acc.: 50.00%] [G loss: 2.269508]\n",
      "480 [D loss: 1.213388, acc.: 50.00%] [G loss: 2.089717]\n",
      "481 [D loss: 1.204852, acc.: 50.00%] [G loss: 2.222939]\n",
      "482 [D loss: 1.271245, acc.: 50.00%] [G loss: 2.458094]\n",
      "483 [D loss: 1.214402, acc.: 50.00%] [G loss: 2.247040]\n",
      "484 [D loss: 1.198682, acc.: 50.00%] [G loss: 2.245134]\n",
      "485 [D loss: 1.194413, acc.: 50.00%] [G loss: 2.315976]\n",
      "486 [D loss: 1.220704, acc.: 50.00%] [G loss: 2.206797]\n",
      "487 [D loss: 1.225658, acc.: 50.00%] [G loss: 2.274285]\n",
      "488 [D loss: 1.206312, acc.: 50.00%] [G loss: 2.074225]\n",
      "489 [D loss: 1.192148, acc.: 50.00%] [G loss: 2.168922]\n",
      "490 [D loss: 1.202812, acc.: 50.00%] [G loss: 2.313394]\n",
      "491 [D loss: 1.228966, acc.: 50.00%] [G loss: 2.245248]\n",
      "492 [D loss: 1.174343, acc.: 50.00%] [G loss: 2.250655]\n",
      "493 [D loss: 1.196327, acc.: 50.00%] [G loss: 2.884657]\n",
      "494 [D loss: 1.206649, acc.: 50.00%] [G loss: 2.220371]\n",
      "495 [D loss: 1.152390, acc.: 50.00%] [G loss: 2.243753]\n",
      "496 [D loss: 1.146311, acc.: 50.00%] [G loss: 2.262833]\n",
      "497 [D loss: 1.190464, acc.: 50.00%] [G loss: 2.238273]\n",
      "498 [D loss: 1.159212, acc.: 50.00%] [G loss: 2.268434]\n",
      "499 [D loss: 1.201814, acc.: 50.00%] [G loss: 2.253054]\n",
      "500 [D loss: 1.206495, acc.: 50.00%] [G loss: 2.162954]\n",
      "501 [D loss: 1.224851, acc.: 50.00%] [G loss: 2.237155]\n",
      "502 [D loss: 1.194293, acc.: 50.00%] [G loss: 2.198554]\n",
      "503 [D loss: 1.182867, acc.: 50.00%] [G loss: 2.306955]\n",
      "504 [D loss: 1.195246, acc.: 50.00%] [G loss: 2.209607]\n",
      "505 [D loss: 1.238665, acc.: 50.00%] [G loss: 2.231450]\n",
      "506 [D loss: 1.163341, acc.: 50.00%] [G loss: 2.353580]\n",
      "507 [D loss: 1.160597, acc.: 50.00%] [G loss: 2.192302]\n",
      "508 [D loss: 1.186330, acc.: 50.00%] [G loss: 2.280654]\n",
      "509 [D loss: 1.137862, acc.: 50.00%] [G loss: 2.198392]\n",
      "510 [D loss: 1.172764, acc.: 50.00%] [G loss: 2.185627]\n",
      "511 [D loss: 1.162543, acc.: 50.00%] [G loss: 2.175405]\n",
      "512 [D loss: 1.178246, acc.: 50.00%] [G loss: 2.334769]\n",
      "513 [D loss: 1.211650, acc.: 50.00%] [G loss: 2.176168]\n",
      "514 [D loss: 1.189708, acc.: 50.00%] [G loss: 2.375914]\n",
      "515 [D loss: 1.155794, acc.: 50.00%] [G loss: 2.142474]\n",
      "516 [D loss: 1.163611, acc.: 50.00%] [G loss: 2.290036]\n",
      "517 [D loss: 1.213855, acc.: 50.00%] [G loss: 2.209781]\n",
      "518 [D loss: 1.155292, acc.: 50.00%] [G loss: 2.217342]\n",
      "519 [D loss: 1.214757, acc.: 50.00%] [G loss: 2.162712]\n",
      "520 [D loss: 1.206628, acc.: 50.00%] [G loss: 2.227844]\n",
      "521 [D loss: 1.124940, acc.: 50.00%] [G loss: 2.301084]\n",
      "522 [D loss: 1.129090, acc.: 50.00%] [G loss: 2.112277]\n",
      "523 [D loss: 1.156889, acc.: 50.00%] [G loss: 2.167232]\n",
      "524 [D loss: 1.201587, acc.: 50.00%] [G loss: 2.225413]\n",
      "525 [D loss: 1.155377, acc.: 50.00%] [G loss: 2.132998]\n",
      "526 [D loss: 1.151919, acc.: 50.00%] [G loss: 2.108597]\n",
      "527 [D loss: 1.205933, acc.: 50.00%] [G loss: 2.155239]\n",
      "528 [D loss: 1.140638, acc.: 50.00%] [G loss: 2.181197]\n",
      "529 [D loss: 1.169822, acc.: 50.00%] [G loss: 2.155861]\n",
      "530 [D loss: 1.185597, acc.: 50.00%] [G loss: 2.311435]\n",
      "531 [D loss: 1.113880, acc.: 50.00%] [G loss: 2.257100]\n",
      "532 [D loss: 1.172260, acc.: 50.00%] [G loss: 2.251257]\n",
      "533 [D loss: 1.190077, acc.: 50.00%] [G loss: 2.157060]\n",
      "534 [D loss: 1.200300, acc.: 50.00%] [G loss: 2.341387]\n",
      "535 [D loss: 1.149638, acc.: 50.00%] [G loss: 2.205626]\n",
      "536 [D loss: 1.146517, acc.: 50.00%] [G loss: 2.224075]\n",
      "537 [D loss: 1.187083, acc.: 50.00%] [G loss: 2.123138]\n",
      "538 [D loss: 1.135624, acc.: 50.00%] [G loss: 2.131944]\n",
      "539 [D loss: 1.186795, acc.: 50.00%] [G loss: 2.079400]\n",
      "540 [D loss: 1.181639, acc.: 50.00%] [G loss: 2.175068]\n",
      "541 [D loss: 1.141176, acc.: 50.00%] [G loss: 2.180665]\n",
      "542 [D loss: 1.168301, acc.: 50.00%] [G loss: 2.086735]\n",
      "543 [D loss: 1.181780, acc.: 50.00%] [G loss: 2.069428]\n",
      "544 [D loss: 1.121946, acc.: 50.00%] [G loss: 2.337320]\n",
      "545 [D loss: 1.135870, acc.: 50.00%] [G loss: 2.176874]\n",
      "546 [D loss: 1.138481, acc.: 50.00%] [G loss: 2.075076]\n",
      "547 [D loss: 1.136980, acc.: 50.00%] [G loss: 2.112293]\n",
      "548 [D loss: 1.169047, acc.: 50.00%] [G loss: 2.144122]\n",
      "549 [D loss: 1.162164, acc.: 50.00%] [G loss: 2.066917]\n",
      "550 [D loss: 1.153880, acc.: 50.00%] [G loss: 2.176865]\n",
      "551 [D loss: 1.213933, acc.: 50.00%] [G loss: 2.128326]\n",
      "552 [D loss: 1.147021, acc.: 50.00%] [G loss: 2.200410]\n",
      "553 [D loss: 1.200126, acc.: 50.00%] [G loss: 2.248059]\n",
      "554 [D loss: 1.130066, acc.: 50.00%] [G loss: 2.108458]\n",
      "555 [D loss: 1.150774, acc.: 50.00%] [G loss: 2.383482]\n",
      "556 [D loss: 1.146204, acc.: 50.00%] [G loss: 2.217542]\n",
      "557 [D loss: 1.167108, acc.: 50.00%] [G loss: 2.101516]\n",
      "558 [D loss: 1.167175, acc.: 50.00%] [G loss: 2.072008]\n",
      "559 [D loss: 1.167371, acc.: 50.00%] [G loss: 2.089808]\n",
      "560 [D loss: 1.176997, acc.: 50.00%] [G loss: 2.194479]\n",
      "561 [D loss: 1.150399, acc.: 50.00%] [G loss: 2.178514]\n",
      "562 [D loss: 1.153994, acc.: 50.00%] [G loss: 2.274278]\n",
      "563 [D loss: 1.102143, acc.: 50.00%] [G loss: 2.297622]\n",
      "564 [D loss: 1.141629, acc.: 50.00%] [G loss: 2.102872]\n",
      "565 [D loss: 1.180481, acc.: 50.00%] [G loss: 2.045257]\n",
      "566 [D loss: 1.100298, acc.: 50.00%] [G loss: 2.083624]\n",
      "567 [D loss: 1.092590, acc.: 50.00%] [G loss: 2.203924]\n",
      "568 [D loss: 1.094981, acc.: 50.00%] [G loss: 2.062675]\n",
      "569 [D loss: 1.149641, acc.: 50.00%] [G loss: 1.907316]\n",
      "570 [D loss: 1.115178, acc.: 50.00%] [G loss: 2.178241]\n",
      "571 [D loss: 1.162320, acc.: 50.00%] [G loss: 2.010497]\n",
      "572 [D loss: 1.123755, acc.: 50.00%] [G loss: 2.067527]\n",
      "573 [D loss: 1.132015, acc.: 50.00%] [G loss: 2.077004]\n",
      "574 [D loss: 1.161934, acc.: 50.00%] [G loss: 2.166161]\n",
      "575 [D loss: 1.166863, acc.: 50.00%] [G loss: 2.013763]\n",
      "576 [D loss: 1.144297, acc.: 50.00%] [G loss: 2.167897]\n",
      "577 [D loss: 1.112828, acc.: 50.00%] [G loss: 2.059437]\n",
      "578 [D loss: 1.085665, acc.: 50.00%] [G loss: 1.963020]\n",
      "579 [D loss: 1.180226, acc.: 50.00%] [G loss: 2.105120]\n",
      "580 [D loss: 1.095507, acc.: 50.00%] [G loss: 2.089724]\n",
      "581 [D loss: 1.107387, acc.: 50.00%] [G loss: 2.114436]\n",
      "582 [D loss: 1.132407, acc.: 50.00%] [G loss: 2.207957]\n",
      "583 [D loss: 1.154617, acc.: 50.00%] [G loss: 2.066240]\n",
      "584 [D loss: 1.180041, acc.: 50.00%] [G loss: 2.170224]\n",
      "585 [D loss: 1.164065, acc.: 50.00%] [G loss: 2.299518]\n",
      "586 [D loss: 1.108710, acc.: 50.00%] [G loss: 2.170969]\n",
      "587 [D loss: 1.090907, acc.: 50.00%] [G loss: 2.120115]\n",
      "588 [D loss: 1.149933, acc.: 50.00%] [G loss: 2.120417]\n",
      "589 [D loss: 1.109104, acc.: 50.00%] [G loss: 2.213542]\n",
      "590 [D loss: 1.125086, acc.: 50.00%] [G loss: 2.052462]\n",
      "591 [D loss: 1.083920, acc.: 50.00%] [G loss: 2.110919]\n",
      "592 [D loss: 1.117156, acc.: 50.00%] [G loss: 2.094318]\n",
      "593 [D loss: 1.159047, acc.: 50.00%] [G loss: 1.983565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594 [D loss: 1.157194, acc.: 50.00%] [G loss: 2.051794]\n",
      "595 [D loss: 1.136433, acc.: 50.00%] [G loss: 1.910757]\n",
      "596 [D loss: 1.153014, acc.: 50.00%] [G loss: 2.093200]\n",
      "597 [D loss: 1.118895, acc.: 50.00%] [G loss: 2.106299]\n",
      "598 [D loss: 1.120973, acc.: 50.00%] [G loss: 2.134595]\n",
      "599 [D loss: 1.171580, acc.: 50.00%] [G loss: 2.098128]\n",
      "600 [D loss: 1.080220, acc.: 50.00%] [G loss: 2.058087]\n",
      "601 [D loss: 1.077956, acc.: 50.00%] [G loss: 2.069431]\n",
      "602 [D loss: 1.067372, acc.: 50.00%] [G loss: 2.084407]\n",
      "603 [D loss: 1.084303, acc.: 50.00%] [G loss: 2.166373]\n",
      "604 [D loss: 1.091072, acc.: 50.00%] [G loss: 2.076925]\n",
      "605 [D loss: 1.134337, acc.: 50.00%] [G loss: 2.155860]\n",
      "606 [D loss: 1.155938, acc.: 50.00%] [G loss: 2.013025]\n",
      "607 [D loss: 1.123155, acc.: 50.00%] [G loss: 2.106261]\n",
      "608 [D loss: 1.114625, acc.: 50.00%] [G loss: 2.074254]\n",
      "609 [D loss: 1.085048, acc.: 50.00%] [G loss: 2.114463]\n",
      "610 [D loss: 1.140267, acc.: 50.00%] [G loss: 1.940411]\n",
      "611 [D loss: 1.071114, acc.: 50.00%] [G loss: 2.037055]\n",
      "612 [D loss: 1.131644, acc.: 50.00%] [G loss: 2.199738]\n",
      "613 [D loss: 1.110232, acc.: 50.00%] [G loss: 2.084036]\n",
      "614 [D loss: 1.109449, acc.: 50.00%] [G loss: 2.102194]\n",
      "615 [D loss: 1.140658, acc.: 50.00%] [G loss: 2.046216]\n",
      "616 [D loss: 1.108272, acc.: 50.00%] [G loss: 2.214132]\n",
      "617 [D loss: 1.136551, acc.: 50.00%] [G loss: 1.994210]\n",
      "618 [D loss: 1.109324, acc.: 50.00%] [G loss: 2.076204]\n",
      "619 [D loss: 1.130379, acc.: 50.00%] [G loss: 2.067964]\n",
      "620 [D loss: 1.137499, acc.: 50.00%] [G loss: 2.024993]\n",
      "621 [D loss: 1.135467, acc.: 50.00%] [G loss: 2.020928]\n",
      "622 [D loss: 1.123649, acc.: 50.00%] [G loss: 2.059144]\n",
      "623 [D loss: 1.114740, acc.: 50.00%] [G loss: 2.048093]\n",
      "624 [D loss: 1.112488, acc.: 50.00%] [G loss: 2.172721]\n",
      "625 [D loss: 1.138440, acc.: 50.00%] [G loss: 2.047768]\n",
      "626 [D loss: 1.089378, acc.: 50.00%] [G loss: 1.885312]\n",
      "627 [D loss: 1.064944, acc.: 50.00%] [G loss: 2.029404]\n",
      "628 [D loss: 1.087042, acc.: 50.00%] [G loss: 2.073781]\n",
      "629 [D loss: 1.086353, acc.: 50.00%] [G loss: 2.125581]\n",
      "630 [D loss: 1.100992, acc.: 50.00%] [G loss: 2.048600]\n",
      "631 [D loss: 1.041953, acc.: 50.00%] [G loss: 2.047406]\n",
      "632 [D loss: 1.129482, acc.: 50.00%] [G loss: 2.098318]\n",
      "633 [D loss: 1.079342, acc.: 50.00%] [G loss: 1.968343]\n",
      "634 [D loss: 1.080080, acc.: 50.00%] [G loss: 2.038523]\n",
      "635 [D loss: 1.110186, acc.: 50.00%] [G loss: 2.070119]\n",
      "636 [D loss: 1.120153, acc.: 50.00%] [G loss: 1.983942]\n",
      "637 [D loss: 1.099692, acc.: 50.00%] [G loss: 1.983690]\n",
      "638 [D loss: 1.079390, acc.: 50.00%] [G loss: 1.988306]\n",
      "639 [D loss: 1.063861, acc.: 50.00%] [G loss: 1.998818]\n",
      "640 [D loss: 1.090298, acc.: 50.00%] [G loss: 2.041382]\n",
      "641 [D loss: 1.079877, acc.: 50.00%] [G loss: 2.096975]\n",
      "642 [D loss: 1.060117, acc.: 50.00%] [G loss: 1.991361]\n",
      "643 [D loss: 1.121122, acc.: 50.00%] [G loss: 2.151987]\n",
      "644 [D loss: 1.079684, acc.: 50.00%] [G loss: 2.026038]\n",
      "645 [D loss: 1.076126, acc.: 50.00%] [G loss: 2.051099]\n",
      "646 [D loss: 1.100589, acc.: 50.00%] [G loss: 2.097281]\n",
      "647 [D loss: 1.142353, acc.: 50.00%] [G loss: 2.019988]\n",
      "648 [D loss: 1.091132, acc.: 50.00%] [G loss: 2.069152]\n",
      "649 [D loss: 1.057338, acc.: 50.00%] [G loss: 1.966864]\n",
      "650 [D loss: 1.111320, acc.: 50.00%] [G loss: 2.103111]\n",
      "651 [D loss: 1.060271, acc.: 50.00%] [G loss: 2.029332]\n",
      "652 [D loss: 1.071364, acc.: 50.00%] [G loss: 2.072452]\n",
      "653 [D loss: 1.034005, acc.: 50.00%] [G loss: 2.057427]\n",
      "654 [D loss: 1.081980, acc.: 50.00%] [G loss: 2.044968]\n",
      "655 [D loss: 1.110821, acc.: 50.00%] [G loss: 1.895380]\n",
      "656 [D loss: 1.085284, acc.: 50.00%] [G loss: 1.943233]\n",
      "657 [D loss: 1.090300, acc.: 50.00%] [G loss: 1.921054]\n",
      "658 [D loss: 1.029224, acc.: 50.00%] [G loss: 2.030003]\n",
      "659 [D loss: 1.100750, acc.: 50.00%] [G loss: 2.070444]\n",
      "660 [D loss: 1.081507, acc.: 50.00%] [G loss: 2.006346]\n",
      "661 [D loss: 1.059238, acc.: 50.00%] [G loss: 2.072794]\n",
      "662 [D loss: 1.088656, acc.: 50.00%] [G loss: 2.029348]\n",
      "663 [D loss: 1.069633, acc.: 50.00%] [G loss: 2.022453]\n",
      "664 [D loss: 1.079621, acc.: 50.00%] [G loss: 2.007465]\n",
      "665 [D loss: 1.077913, acc.: 50.00%] [G loss: 2.010492]\n",
      "666 [D loss: 1.101692, acc.: 50.00%] [G loss: 2.007694]\n",
      "667 [D loss: 1.077245, acc.: 50.00%] [G loss: 1.965026]\n",
      "668 [D loss: 1.045882, acc.: 50.00%] [G loss: 2.068129]\n",
      "669 [D loss: 1.087254, acc.: 50.00%] [G loss: 2.043254]\n",
      "670 [D loss: 1.105827, acc.: 50.00%] [G loss: 1.981767]\n",
      "671 [D loss: 1.047449, acc.: 50.00%] [G loss: 1.892197]\n",
      "672 [D loss: 1.068695, acc.: 50.00%] [G loss: 1.998678]\n",
      "673 [D loss: 1.080929, acc.: 50.00%] [G loss: 1.995418]\n",
      "674 [D loss: 1.102545, acc.: 50.00%] [G loss: 1.911975]\n",
      "675 [D loss: 1.092337, acc.: 50.00%] [G loss: 1.931166]\n",
      "676 [D loss: 1.069340, acc.: 50.00%] [G loss: 1.901546]\n",
      "677 [D loss: 1.033993, acc.: 50.00%] [G loss: 1.928388]\n",
      "678 [D loss: 1.039681, acc.: 50.00%] [G loss: 1.934676]\n",
      "679 [D loss: 1.058240, acc.: 50.00%] [G loss: 1.910965]\n",
      "680 [D loss: 1.111528, acc.: 50.00%] [G loss: 1.883847]\n",
      "681 [D loss: 1.087111, acc.: 50.00%] [G loss: 1.992227]\n",
      "682 [D loss: 1.090616, acc.: 50.00%] [G loss: 2.067254]\n",
      "683 [D loss: 1.039127, acc.: 50.00%] [G loss: 1.854874]\n",
      "684 [D loss: 1.074056, acc.: 50.00%] [G loss: 2.060857]\n",
      "685 [D loss: 1.082066, acc.: 50.00%] [G loss: 1.985257]\n",
      "686 [D loss: 1.079205, acc.: 50.00%] [G loss: 2.018774]\n",
      "687 [D loss: 1.092390, acc.: 50.00%] [G loss: 1.905053]\n",
      "688 [D loss: 1.118497, acc.: 50.00%] [G loss: 1.902517]\n",
      "689 [D loss: 1.061609, acc.: 50.00%] [G loss: 1.924089]\n",
      "690 [D loss: 1.082893, acc.: 50.00%] [G loss: 1.946855]\n",
      "691 [D loss: 1.061345, acc.: 50.00%] [G loss: 1.896487]\n",
      "692 [D loss: 1.054833, acc.: 50.00%] [G loss: 1.984066]\n",
      "693 [D loss: 1.054587, acc.: 50.00%] [G loss: 1.976686]\n",
      "694 [D loss: 1.098668, acc.: 50.00%] [G loss: 2.022418]\n",
      "695 [D loss: 1.005459, acc.: 50.00%] [G loss: 1.917030]\n",
      "696 [D loss: 1.061254, acc.: 50.00%] [G loss: 2.012088]\n",
      "697 [D loss: 1.069751, acc.: 50.00%] [G loss: 1.970146]\n",
      "698 [D loss: 1.065082, acc.: 50.00%] [G loss: 1.857700]\n",
      "699 [D loss: 1.027989, acc.: 50.00%] [G loss: 1.887129]\n",
      "700 [D loss: 1.117089, acc.: 50.00%] [G loss: 1.942237]\n",
      "701 [D loss: 1.100618, acc.: 50.00%] [G loss: 1.883924]\n",
      "702 [D loss: 1.094413, acc.: 50.00%] [G loss: 1.908089]\n",
      "703 [D loss: 1.036884, acc.: 50.00%] [G loss: 1.945226]\n",
      "704 [D loss: 1.069679, acc.: 50.00%] [G loss: 1.795325]\n",
      "705 [D loss: 1.055079, acc.: 50.00%] [G loss: 1.826522]\n",
      "706 [D loss: 1.004862, acc.: 50.00%] [G loss: 1.840525]\n",
      "707 [D loss: 1.063708, acc.: 50.00%] [G loss: 1.893510]\n",
      "708 [D loss: 1.053231, acc.: 50.00%] [G loss: 1.749120]\n",
      "709 [D loss: 1.008161, acc.: 50.00%] [G loss: 1.831560]\n",
      "710 [D loss: 1.017829, acc.: 50.00%] [G loss: 1.985695]\n",
      "711 [D loss: 1.024537, acc.: 50.00%] [G loss: 2.024858]\n",
      "712 [D loss: 1.079407, acc.: 50.00%] [G loss: 2.003342]\n",
      "713 [D loss: 1.043223, acc.: 50.00%] [G loss: 2.042495]\n",
      "714 [D loss: 1.021940, acc.: 50.00%] [G loss: 2.023479]\n",
      "715 [D loss: 1.058008, acc.: 50.00%] [G loss: 1.784950]\n",
      "716 [D loss: 1.046256, acc.: 50.00%] [G loss: 1.998992]\n",
      "717 [D loss: 1.062979, acc.: 50.00%] [G loss: 1.924014]\n",
      "718 [D loss: 1.035594, acc.: 50.00%] [G loss: 1.873528]\n",
      "719 [D loss: 1.068645, acc.: 50.00%] [G loss: 1.994291]\n",
      "720 [D loss: 1.000594, acc.: 50.00%] [G loss: 1.866435]\n",
      "721 [D loss: 1.082609, acc.: 50.00%] [G loss: 2.094157]\n",
      "722 [D loss: 1.023116, acc.: 50.00%] [G loss: 1.897575]\n",
      "723 [D loss: 0.995292, acc.: 50.00%] [G loss: 1.894819]\n",
      "724 [D loss: 1.029524, acc.: 50.00%] [G loss: 1.947775]\n",
      "725 [D loss: 1.038081, acc.: 50.00%] [G loss: 1.987294]\n",
      "726 [D loss: 1.032283, acc.: 50.00%] [G loss: 1.927636]\n",
      "727 [D loss: 1.052867, acc.: 50.00%] [G loss: 1.884526]\n",
      "728 [D loss: 1.004570, acc.: 50.00%] [G loss: 1.952413]\n",
      "729 [D loss: 1.087885, acc.: 50.00%] [G loss: 1.805983]\n",
      "730 [D loss: 1.077213, acc.: 50.00%] [G loss: 1.962893]\n",
      "731 [D loss: 1.015816, acc.: 50.00%] [G loss: 1.836533]\n",
      "732 [D loss: 1.025318, acc.: 50.00%] [G loss: 1.883567]\n",
      "733 [D loss: 1.019941, acc.: 50.00%] [G loss: 1.839970]\n",
      "734 [D loss: 1.079779, acc.: 50.00%] [G loss: 1.894547]\n",
      "735 [D loss: 1.013556, acc.: 50.00%] [G loss: 1.815454]\n",
      "736 [D loss: 1.010627, acc.: 50.00%] [G loss: 2.234530]\n",
      "737 [D loss: 1.055023, acc.: 50.00%] [G loss: 1.887047]\n",
      "738 [D loss: 1.043527, acc.: 50.00%] [G loss: 1.825698]\n",
      "739 [D loss: 0.991570, acc.: 50.00%] [G loss: 1.905010]\n",
      "740 [D loss: 1.044063, acc.: 50.00%] [G loss: 1.881788]\n",
      "741 [D loss: 1.015651, acc.: 50.00%] [G loss: 1.842303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742 [D loss: 1.110886, acc.: 50.00%] [G loss: 1.865503]\n",
      "743 [D loss: 1.014412, acc.: 50.00%] [G loss: 1.916545]\n",
      "744 [D loss: 0.991532, acc.: 50.00%] [G loss: 1.835583]\n",
      "745 [D loss: 1.082145, acc.: 50.00%] [G loss: 2.011422]\n",
      "746 [D loss: 1.012116, acc.: 50.00%] [G loss: 2.065216]\n",
      "747 [D loss: 1.005053, acc.: 50.00%] [G loss: 1.881618]\n",
      "748 [D loss: 1.037478, acc.: 50.00%] [G loss: 1.935342]\n",
      "749 [D loss: 1.005402, acc.: 50.00%] [G loss: 1.914132]\n",
      "750 [D loss: 1.017698, acc.: 50.00%] [G loss: 1.914311]\n",
      "751 [D loss: 0.990161, acc.: 50.00%] [G loss: 1.850553]\n",
      "752 [D loss: 1.047402, acc.: 50.00%] [G loss: 1.971495]\n",
      "753 [D loss: 0.986473, acc.: 50.00%] [G loss: 1.975939]\n",
      "754 [D loss: 0.992631, acc.: 50.00%] [G loss: 1.985065]\n",
      "755 [D loss: 1.021651, acc.: 50.00%] [G loss: 1.835372]\n",
      "756 [D loss: 0.967465, acc.: 50.00%] [G loss: 1.875282]\n",
      "757 [D loss: 1.009258, acc.: 50.00%] [G loss: 1.884529]\n",
      "758 [D loss: 1.046447, acc.: 50.00%] [G loss: 1.857889]\n",
      "759 [D loss: 1.051569, acc.: 50.00%] [G loss: 1.847693]\n",
      "760 [D loss: 0.989077, acc.: 50.00%] [G loss: 1.852239]\n",
      "761 [D loss: 1.046153, acc.: 50.00%] [G loss: 1.904731]\n",
      "762 [D loss: 1.039524, acc.: 50.00%] [G loss: 1.841783]\n",
      "763 [D loss: 0.984716, acc.: 50.00%] [G loss: 1.877799]\n",
      "764 [D loss: 1.039872, acc.: 50.00%] [G loss: 1.860561]\n",
      "765 [D loss: 0.977931, acc.: 50.00%] [G loss: 1.833481]\n",
      "766 [D loss: 0.982987, acc.: 50.00%] [G loss: 1.859013]\n",
      "767 [D loss: 0.974641, acc.: 50.00%] [G loss: 1.890633]\n",
      "768 [D loss: 1.033279, acc.: 50.00%] [G loss: 1.847438]\n",
      "769 [D loss: 0.995259, acc.: 50.00%] [G loss: 1.927102]\n",
      "770 [D loss: 1.007026, acc.: 50.00%] [G loss: 1.814036]\n",
      "771 [D loss: 1.012000, acc.: 50.00%] [G loss: 1.822358]\n",
      "772 [D loss: 0.950521, acc.: 50.00%] [G loss: 1.800094]\n",
      "773 [D loss: 0.998491, acc.: 50.00%] [G loss: 1.869179]\n",
      "774 [D loss: 1.043312, acc.: 50.00%] [G loss: 1.848550]\n",
      "775 [D loss: 0.927894, acc.: 50.00%] [G loss: 1.848015]\n",
      "776 [D loss: 1.006533, acc.: 50.00%] [G loss: 1.893204]\n",
      "777 [D loss: 0.985993, acc.: 50.00%] [G loss: 1.877495]\n",
      "778 [D loss: 1.000561, acc.: 50.00%] [G loss: 1.798651]\n",
      "779 [D loss: 0.998938, acc.: 50.00%] [G loss: 1.840751]\n",
      "780 [D loss: 0.987989, acc.: 50.00%] [G loss: 1.928604]\n",
      "781 [D loss: 1.030035, acc.: 50.00%] [G loss: 1.825897]\n",
      "782 [D loss: 1.024789, acc.: 50.00%] [G loss: 1.730526]\n",
      "783 [D loss: 1.055784, acc.: 50.00%] [G loss: 1.882644]\n",
      "784 [D loss: 0.958694, acc.: 50.00%] [G loss: 1.880508]\n",
      "785 [D loss: 1.000996, acc.: 50.00%] [G loss: 1.897313]\n",
      "786 [D loss: 0.998025, acc.: 50.00%] [G loss: 1.886409]\n",
      "787 [D loss: 1.001683, acc.: 50.00%] [G loss: 1.752407]\n",
      "788 [D loss: 0.961297, acc.: 50.00%] [G loss: 1.719031]\n",
      "789 [D loss: 0.996234, acc.: 50.00%] [G loss: 1.729378]\n",
      "790 [D loss: 1.035242, acc.: 50.00%] [G loss: 1.850574]\n",
      "791 [D loss: 1.012653, acc.: 50.00%] [G loss: 1.845205]\n",
      "792 [D loss: 0.992063, acc.: 50.00%] [G loss: 1.914020]\n",
      "793 [D loss: 0.995121, acc.: 50.00%] [G loss: 1.852917]\n",
      "794 [D loss: 1.037410, acc.: 50.00%] [G loss: 1.783019]\n",
      "795 [D loss: 0.976587, acc.: 50.00%] [G loss: 1.756430]\n",
      "796 [D loss: 0.951897, acc.: 50.00%] [G loss: 1.807053]\n",
      "797 [D loss: 0.981586, acc.: 50.00%] [G loss: 1.762836]\n",
      "798 [D loss: 0.963869, acc.: 50.00%] [G loss: 1.872568]\n",
      "799 [D loss: 0.970061, acc.: 50.00%] [G loss: 1.845158]\n",
      "800 [D loss: 0.971895, acc.: 50.00%] [G loss: 1.724944]\n",
      "801 [D loss: 1.004585, acc.: 50.00%] [G loss: 1.811331]\n",
      "802 [D loss: 1.057756, acc.: 50.00%] [G loss: 1.749147]\n",
      "803 [D loss: 0.989091, acc.: 50.00%] [G loss: 1.848515]\n",
      "804 [D loss: 0.944076, acc.: 50.00%] [G loss: 1.842459]\n",
      "805 [D loss: 0.987319, acc.: 50.00%] [G loss: 1.824886]\n",
      "806 [D loss: 1.004507, acc.: 50.00%] [G loss: 1.765588]\n",
      "807 [D loss: 0.950074, acc.: 50.00%] [G loss: 1.844464]\n",
      "808 [D loss: 0.988774, acc.: 50.00%] [G loss: 1.765895]\n",
      "809 [D loss: 0.964031, acc.: 50.00%] [G loss: 1.645499]\n",
      "810 [D loss: 0.994491, acc.: 50.00%] [G loss: 1.750614]\n",
      "811 [D loss: 0.993380, acc.: 50.00%] [G loss: 1.699314]\n",
      "812 [D loss: 1.033688, acc.: 50.00%] [G loss: 1.871914]\n",
      "813 [D loss: 1.047150, acc.: 50.00%] [G loss: 1.803224]\n",
      "814 [D loss: 1.016951, acc.: 50.00%] [G loss: 1.837969]\n",
      "815 [D loss: 1.002249, acc.: 50.00%] [G loss: 1.833552]\n",
      "816 [D loss: 1.016023, acc.: 50.00%] [G loss: 1.749073]\n",
      "817 [D loss: 0.973840, acc.: 50.00%] [G loss: 1.877122]\n",
      "818 [D loss: 0.976694, acc.: 50.00%] [G loss: 1.744902]\n",
      "819 [D loss: 0.955295, acc.: 50.00%] [G loss: 1.757620]\n",
      "820 [D loss: 0.993304, acc.: 50.00%] [G loss: 1.833556]\n",
      "821 [D loss: 0.988852, acc.: 50.00%] [G loss: 1.740434]\n",
      "822 [D loss: 1.018223, acc.: 50.00%] [G loss: 1.772535]\n",
      "823 [D loss: 0.967314, acc.: 50.00%] [G loss: 1.871792]\n",
      "824 [D loss: 0.997161, acc.: 50.00%] [G loss: 1.746888]\n",
      "825 [D loss: 1.010840, acc.: 50.00%] [G loss: 1.717571]\n",
      "826 [D loss: 1.014866, acc.: 50.00%] [G loss: 1.710745]\n",
      "827 [D loss: 1.034416, acc.: 50.00%] [G loss: 1.767299]\n",
      "828 [D loss: 0.963765, acc.: 50.00%] [G loss: 1.831642]\n",
      "829 [D loss: 0.998980, acc.: 50.00%] [G loss: 1.804329]\n",
      "830 [D loss: 0.971252, acc.: 50.00%] [G loss: 1.712540]\n",
      "831 [D loss: 0.998683, acc.: 50.00%] [G loss: 1.819033]\n",
      "832 [D loss: 0.994028, acc.: 50.00%] [G loss: 1.779842]\n",
      "833 [D loss: 0.990679, acc.: 50.00%] [G loss: 1.854270]\n",
      "834 [D loss: 0.965242, acc.: 50.00%] [G loss: 1.851363]\n",
      "835 [D loss: 0.968919, acc.: 50.00%] [G loss: 1.667167]\n",
      "836 [D loss: 0.964088, acc.: 50.00%] [G loss: 1.759710]\n",
      "837 [D loss: 0.981944, acc.: 50.00%] [G loss: 1.805561]\n",
      "838 [D loss: 1.022690, acc.: 50.00%] [G loss: 1.811884]\n",
      "839 [D loss: 0.898899, acc.: 50.00%] [G loss: 1.835767]\n",
      "840 [D loss: 1.033906, acc.: 50.00%] [G loss: 1.769133]\n",
      "841 [D loss: 1.009232, acc.: 50.00%] [G loss: 1.683972]\n",
      "842 [D loss: 0.954828, acc.: 50.00%] [G loss: 1.796803]\n",
      "843 [D loss: 0.951291, acc.: 50.00%] [G loss: 1.711552]\n",
      "844 [D loss: 1.019184, acc.: 50.00%] [G loss: 1.777206]\n",
      "845 [D loss: 1.003749, acc.: 50.00%] [G loss: 1.853418]\n",
      "846 [D loss: 0.996996, acc.: 50.00%] [G loss: 1.819273]\n",
      "847 [D loss: 1.009049, acc.: 50.00%] [G loss: 1.723940]\n",
      "848 [D loss: 0.981008, acc.: 50.00%] [G loss: 1.759004]\n",
      "849 [D loss: 0.977122, acc.: 50.00%] [G loss: 1.723721]\n",
      "850 [D loss: 0.959993, acc.: 50.00%] [G loss: 1.776125]\n",
      "851 [D loss: 0.954436, acc.: 50.00%] [G loss: 1.724470]\n",
      "852 [D loss: 0.965666, acc.: 50.00%] [G loss: 1.721473]\n",
      "853 [D loss: 1.015675, acc.: 50.00%] [G loss: 1.678284]\n",
      "854 [D loss: 0.951323, acc.: 50.00%] [G loss: 1.726598]\n",
      "855 [D loss: 0.974808, acc.: 50.00%] [G loss: 1.743307]\n",
      "856 [D loss: 0.950813, acc.: 50.00%] [G loss: 1.656084]\n",
      "857 [D loss: 0.975543, acc.: 50.00%] [G loss: 1.671455]\n",
      "858 [D loss: 0.995216, acc.: 50.00%] [G loss: 1.749139]\n",
      "859 [D loss: 0.980519, acc.: 50.00%] [G loss: 1.675980]\n",
      "860 [D loss: 0.975527, acc.: 50.00%] [G loss: 1.800298]\n",
      "861 [D loss: 0.947560, acc.: 50.00%] [G loss: 1.771220]\n",
      "862 [D loss: 0.954824, acc.: 50.00%] [G loss: 1.696542]\n",
      "863 [D loss: 0.999468, acc.: 50.00%] [G loss: 1.739645]\n",
      "864 [D loss: 0.953599, acc.: 50.00%] [G loss: 1.738153]\n",
      "865 [D loss: 0.960224, acc.: 50.00%] [G loss: 1.729743]\n",
      "866 [D loss: 0.968938, acc.: 50.00%] [G loss: 1.795171]\n",
      "867 [D loss: 0.973144, acc.: 50.00%] [G loss: 1.788316]\n",
      "868 [D loss: 0.980710, acc.: 50.00%] [G loss: 1.784612]\n",
      "869 [D loss: 0.930011, acc.: 50.00%] [G loss: 1.724648]\n",
      "870 [D loss: 0.961822, acc.: 50.00%] [G loss: 1.777987]\n",
      "871 [D loss: 0.936728, acc.: 50.00%] [G loss: 1.756420]\n",
      "872 [D loss: 0.972080, acc.: 50.00%] [G loss: 1.714616]\n",
      "873 [D loss: 0.939492, acc.: 50.00%] [G loss: 1.752203]\n",
      "874 [D loss: 0.951235, acc.: 50.00%] [G loss: 1.704227]\n",
      "875 [D loss: 0.922206, acc.: 50.00%] [G loss: 1.727523]\n",
      "876 [D loss: 0.945734, acc.: 50.00%] [G loss: 1.790062]\n",
      "877 [D loss: 0.920555, acc.: 50.00%] [G loss: 1.756899]\n",
      "878 [D loss: 0.922545, acc.: 50.00%] [G loss: 1.817151]\n",
      "879 [D loss: 0.962179, acc.: 50.00%] [G loss: 1.730161]\n",
      "880 [D loss: 0.960117, acc.: 50.00%] [G loss: 1.707808]\n",
      "881 [D loss: 0.965055, acc.: 50.00%] [G loss: 1.739459]\n",
      "882 [D loss: 1.012350, acc.: 50.00%] [G loss: 1.852024]\n",
      "883 [D loss: 0.956837, acc.: 50.00%] [G loss: 1.751318]\n",
      "884 [D loss: 0.912794, acc.: 50.00%] [G loss: 1.668325]\n",
      "885 [D loss: 0.923585, acc.: 50.00%] [G loss: 1.709595]\n",
      "886 [D loss: 0.938556, acc.: 50.00%] [G loss: 1.739101]\n",
      "887 [D loss: 0.944948, acc.: 50.00%] [G loss: 1.700849]\n",
      "888 [D loss: 0.970082, acc.: 50.00%] [G loss: 1.816570]\n",
      "889 [D loss: 0.957816, acc.: 50.00%] [G loss: 1.706936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "890 [D loss: 0.905969, acc.: 50.00%] [G loss: 1.721017]\n",
      "891 [D loss: 0.920203, acc.: 50.00%] [G loss: 1.771580]\n",
      "892 [D loss: 0.947304, acc.: 50.00%] [G loss: 1.692300]\n",
      "893 [D loss: 0.935105, acc.: 50.00%] [G loss: 1.717202]\n",
      "894 [D loss: 0.982476, acc.: 50.00%] [G loss: 1.877356]\n",
      "895 [D loss: 0.951818, acc.: 50.00%] [G loss: 1.823235]\n",
      "896 [D loss: 0.922792, acc.: 50.00%] [G loss: 1.689761]\n",
      "897 [D loss: 0.942644, acc.: 50.00%] [G loss: 1.866622]\n",
      "898 [D loss: 0.981586, acc.: 50.00%] [G loss: 1.691881]\n",
      "899 [D loss: 0.957249, acc.: 50.00%] [G loss: 1.672654]\n",
      "900 [D loss: 0.945722, acc.: 50.00%] [G loss: 1.776724]\n",
      "901 [D loss: 0.964763, acc.: 50.00%] [G loss: 1.836696]\n",
      "902 [D loss: 1.008184, acc.: 50.00%] [G loss: 1.777580]\n",
      "903 [D loss: 0.922591, acc.: 50.00%] [G loss: 1.662134]\n",
      "904 [D loss: 0.963689, acc.: 50.00%] [G loss: 1.711887]\n",
      "905 [D loss: 0.931004, acc.: 50.00%] [G loss: 1.727746]\n",
      "906 [D loss: 0.965713, acc.: 50.00%] [G loss: 1.691005]\n",
      "907 [D loss: 0.955522, acc.: 50.00%] [G loss: 1.672350]\n",
      "908 [D loss: 0.987040, acc.: 50.00%] [G loss: 1.622714]\n",
      "909 [D loss: 0.920320, acc.: 50.00%] [G loss: 1.780530]\n",
      "910 [D loss: 0.963854, acc.: 50.00%] [G loss: 1.684890]\n",
      "911 [D loss: 0.964625, acc.: 50.00%] [G loss: 1.672289]\n",
      "912 [D loss: 0.911789, acc.: 50.00%] [G loss: 1.712517]\n",
      "913 [D loss: 0.945265, acc.: 50.00%] [G loss: 1.675098]\n",
      "914 [D loss: 0.929049, acc.: 50.00%] [G loss: 1.734835]\n",
      "915 [D loss: 0.980619, acc.: 50.00%] [G loss: 1.724125]\n",
      "916 [D loss: 0.936360, acc.: 50.00%] [G loss: 1.648655]\n",
      "917 [D loss: 0.992585, acc.: 50.00%] [G loss: 1.646422]\n",
      "918 [D loss: 0.955492, acc.: 50.00%] [G loss: 1.715305]\n",
      "919 [D loss: 1.006317, acc.: 50.00%] [G loss: 1.705447]\n",
      "920 [D loss: 0.881720, acc.: 50.00%] [G loss: 1.609524]\n",
      "921 [D loss: 0.939921, acc.: 50.00%] [G loss: 1.727578]\n",
      "922 [D loss: 0.956546, acc.: 50.00%] [G loss: 1.656680]\n",
      "923 [D loss: 0.926254, acc.: 50.00%] [G loss: 1.788258]\n",
      "924 [D loss: 0.932136, acc.: 50.00%] [G loss: 1.741749]\n",
      "925 [D loss: 0.951545, acc.: 50.00%] [G loss: 1.690049]\n",
      "926 [D loss: 0.927308, acc.: 50.00%] [G loss: 1.777414]\n",
      "927 [D loss: 0.943011, acc.: 50.00%] [G loss: 1.718123]\n",
      "928 [D loss: 0.895854, acc.: 50.00%] [G loss: 1.665213]\n",
      "929 [D loss: 0.945737, acc.: 50.00%] [G loss: 1.793641]\n",
      "930 [D loss: 0.970016, acc.: 50.00%] [G loss: 1.708823]\n",
      "931 [D loss: 0.964300, acc.: 50.00%] [G loss: 1.686662]\n",
      "932 [D loss: 0.964112, acc.: 50.00%] [G loss: 1.627308]\n",
      "933 [D loss: 0.937231, acc.: 50.00%] [G loss: 1.700559]\n",
      "934 [D loss: 0.914773, acc.: 50.00%] [G loss: 1.729369]\n",
      "935 [D loss: 0.938721, acc.: 50.00%] [G loss: 1.679424]\n",
      "936 [D loss: 0.970865, acc.: 50.00%] [G loss: 1.676672]\n",
      "937 [D loss: 0.951176, acc.: 50.00%] [G loss: 1.630976]\n",
      "938 [D loss: 0.927321, acc.: 50.00%] [G loss: 1.592608]\n",
      "939 [D loss: 0.959228, acc.: 50.00%] [G loss: 1.618662]\n",
      "940 [D loss: 0.972157, acc.: 50.00%] [G loss: 1.756462]\n",
      "941 [D loss: 0.945363, acc.: 50.00%] [G loss: 1.643176]\n",
      "942 [D loss: 0.926678, acc.: 50.00%] [G loss: 1.643815]\n",
      "943 [D loss: 0.915580, acc.: 50.00%] [G loss: 1.675335]\n",
      "944 [D loss: 0.936851, acc.: 50.00%] [G loss: 1.691998]\n",
      "945 [D loss: 0.894110, acc.: 50.00%] [G loss: 1.760720]\n",
      "946 [D loss: 0.884734, acc.: 50.00%] [G loss: 1.578537]\n",
      "947 [D loss: 0.910125, acc.: 50.00%] [G loss: 1.748182]\n",
      "948 [D loss: 0.876847, acc.: 50.00%] [G loss: 1.642288]\n",
      "949 [D loss: 0.946091, acc.: 50.00%] [G loss: 1.664739]\n",
      "950 [D loss: 0.936986, acc.: 50.00%] [G loss: 1.778622]\n",
      "951 [D loss: 0.889525, acc.: 50.00%] [G loss: 1.681178]\n",
      "952 [D loss: 0.947014, acc.: 50.00%] [G loss: 1.681832]\n",
      "953 [D loss: 0.875006, acc.: 50.00%] [G loss: 1.606393]\n",
      "954 [D loss: 0.881610, acc.: 50.00%] [G loss: 1.670144]\n",
      "955 [D loss: 0.942438, acc.: 50.00%] [G loss: 1.664709]\n",
      "956 [D loss: 0.924773, acc.: 50.00%] [G loss: 1.691406]\n",
      "957 [D loss: 0.890250, acc.: 50.00%] [G loss: 1.688524]\n",
      "958 [D loss: 0.962367, acc.: 50.00%] [G loss: 1.684861]\n",
      "959 [D loss: 0.915306, acc.: 50.00%] [G loss: 1.697948]\n",
      "960 [D loss: 0.891748, acc.: 50.00%] [G loss: 1.580839]\n",
      "961 [D loss: 0.913288, acc.: 50.00%] [G loss: 1.625833]\n",
      "962 [D loss: 0.904669, acc.: 50.00%] [G loss: 1.507836]\n",
      "963 [D loss: 0.921644, acc.: 50.00%] [G loss: 1.588342]\n",
      "964 [D loss: 0.890659, acc.: 50.00%] [G loss: 1.745601]\n",
      "965 [D loss: 0.938826, acc.: 50.00%] [G loss: 1.661789]\n",
      "966 [D loss: 0.937428, acc.: 50.00%] [G loss: 1.657272]\n",
      "967 [D loss: 0.908256, acc.: 50.00%] [G loss: 1.565482]\n",
      "968 [D loss: 0.973732, acc.: 50.00%] [G loss: 1.676547]\n",
      "969 [D loss: 0.865636, acc.: 50.00%] [G loss: 1.597496]\n",
      "970 [D loss: 0.921708, acc.: 50.00%] [G loss: 1.646969]\n",
      "971 [D loss: 0.906061, acc.: 50.00%] [G loss: 1.711917]\n",
      "972 [D loss: 0.902552, acc.: 50.00%] [G loss: 1.531699]\n",
      "973 [D loss: 0.908189, acc.: 50.00%] [G loss: 1.611175]\n",
      "974 [D loss: 0.934070, acc.: 50.00%] [G loss: 1.673408]\n",
      "975 [D loss: 0.938140, acc.: 50.00%] [G loss: 1.597782]\n",
      "976 [D loss: 0.956858, acc.: 50.00%] [G loss: 1.685980]\n",
      "977 [D loss: 0.975079, acc.: 50.00%] [G loss: 1.723114]\n",
      "978 [D loss: 0.889493, acc.: 50.00%] [G loss: 1.622439]\n",
      "979 [D loss: 0.964981, acc.: 50.00%] [G loss: 1.637146]\n",
      "980 [D loss: 0.909571, acc.: 50.00%] [G loss: 1.718323]\n",
      "981 [D loss: 0.926656, acc.: 50.00%] [G loss: 1.608226]\n",
      "982 [D loss: 0.871464, acc.: 50.00%] [G loss: 1.623776]\n",
      "983 [D loss: 0.898330, acc.: 50.00%] [G loss: 1.678274]\n",
      "984 [D loss: 0.973198, acc.: 50.00%] [G loss: 1.514401]\n",
      "985 [D loss: 0.928032, acc.: 50.00%] [G loss: 1.608079]\n",
      "986 [D loss: 0.886265, acc.: 50.00%] [G loss: 1.689210]\n",
      "987 [D loss: 0.891279, acc.: 50.00%] [G loss: 1.649996]\n",
      "988 [D loss: 0.902652, acc.: 50.00%] [G loss: 1.674366]\n",
      "989 [D loss: 0.936737, acc.: 50.00%] [G loss: 1.613785]\n",
      "990 [D loss: 0.941611, acc.: 50.00%] [G loss: 1.627476]\n",
      "991 [D loss: 0.924202, acc.: 50.00%] [G loss: 1.628596]\n",
      "992 [D loss: 0.918159, acc.: 50.00%] [G loss: 1.665102]\n",
      "993 [D loss: 0.904393, acc.: 50.00%] [G loss: 1.629116]\n",
      "994 [D loss: 0.906670, acc.: 50.00%] [G loss: 1.666432]\n",
      "995 [D loss: 0.926083, acc.: 50.00%] [G loss: 1.577419]\n",
      "996 [D loss: 0.928968, acc.: 50.00%] [G loss: 1.652763]\n",
      "997 [D loss: 0.922828, acc.: 50.00%] [G loss: 1.588673]\n",
      "998 [D loss: 0.865827, acc.: 50.00%] [G loss: 1.699536]\n",
      "999 [D loss: 0.932519, acc.: 50.00%] [G loss: 1.599985]\n"
     ]
    }
   ],
   "source": [
    "lstmgan.train(epochs=1000, batch_size=20, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0h 2m 24s\n"
     ]
    }
   ],
   "source": [
    "end = datetime.datetime.now()\n",
    "\n",
    "diff = (end - start)\n",
    "\n",
    "datetime.timedelta(seconds=10, microseconds=885206)\n",
    "\n",
    "diff_seconds = int(diff.total_seconds())\n",
    "\n",
    "minute_seconds, seconds = divmod(diff_seconds, 60)\n",
    "hours, minutes = divmod(minute_seconds, 60)\n",
    "hms = f\"{hours}h {minutes}m {seconds}s\"\n",
    "\n",
    "'0h 0m 10s'\n",
    "print(hms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.28163284,  0.12766781,  0.05548269, -0.26448318, -0.06211267,\n",
      "        -0.155702  ,  0.15262923,  0.14719923, -0.07292137, -0.29388356,\n",
      "        -0.0687419 ,  0.08685065,  0.16005976,  0.26806402,  0.1332532 ,\n",
      "         0.17550126,  0.29987764,  0.05141502,  0.07975449,  0.05960162,\n",
      "        -0.15501156,  0.04840528,  0.23657744,  0.00845145,  0.1624974 ,\n",
      "         0.0918509 ,  0.10359066, -0.21814741, -0.24310622,  0.06882069,\n",
      "        -0.25291204,  0.06758331, -0.0546484 ,  0.10610233, -0.29031694,\n",
      "         0.03777502, -0.110967  ,  0.05426177, -0.10608154,  0.17529184,\n",
      "         0.10239258,  0.20268376, -0.20867047,  0.18113036, -0.29099673,\n",
      "         0.16977574, -0.01071215, -0.2612994 ,  0.08479512,  0.29576927,\n",
      "        -0.29999268,  0.12407863, -0.10785764, -0.2225118 , -0.02355813,\n",
      "         0.28160676, -0.20028664, -0.18503149,  0.11644305, -0.08165821,\n",
      "         0.14920361,  0.2458437 ,  0.01376188,  0.2457754 ]],\n",
      "      dtype=float32), array([[[-0.06741646,  0.06448835, -0.0954169 , ...,  0.00946309,\n",
      "          0.00122555, -0.0001884 ],\n",
      "        [-0.06030238,  0.04835522,  0.06528206, ...,  0.11377499,\n",
      "          0.09556571,  0.02297917],\n",
      "        [ 0.12146815, -0.09544205,  0.1215857 , ...,  0.04859   ,\n",
      "         -0.11483659, -0.09950515],\n",
      "        ...,\n",
      "        [ 0.0093784 , -0.11465981, -0.08682812, ..., -0.10314433,\n",
      "         -0.14219122,  0.00751276],\n",
      "        [-0.04213882, -0.01411028, -0.05325067, ...,  0.05669635,\n",
      "         -0.03158484,  0.10970334],\n",
      "        [-0.12229919,  0.00209765, -0.08230069, ...,  0.08886434,\n",
      "         -0.01450456,  0.11201947]],\n",
      "\n",
      "       [[-0.05177653, -0.0048998 , -0.12146749, ...,  0.08323797,\n",
      "          0.1016904 ,  0.0118006 ],\n",
      "        [-0.07587388,  0.03069115, -0.1304841 , ..., -0.03789119,\n",
      "         -0.10647669, -0.11899816],\n",
      "        [-0.02710219, -0.03652615, -0.03135577, ..., -0.1042804 ,\n",
      "          0.02427196, -0.00524524],\n",
      "        ...,\n",
      "        [ 0.11395945, -0.0826743 ,  0.05368586, ...,  0.11207923,\n",
      "          0.10960414, -0.00114937],\n",
      "        [-0.1339835 ,  0.09540412, -0.09679379, ..., -0.05364098,\n",
      "          0.1520727 ,  0.05996875],\n",
      "        [ 0.0090046 , -0.0295291 , -0.00697508, ..., -0.05659055,\n",
      "         -0.05534768, -0.06492747]],\n",
      "\n",
      "       [[ 0.03839511,  0.03679791,  0.07590762, ..., -0.12727687,\n",
      "          0.07677934,  0.02457087],\n",
      "        [ 0.10120961, -0.09321675, -0.0095651 , ..., -0.05903058,\n",
      "         -0.02427168, -0.01273691],\n",
      "        [-0.13591173,  0.07417361, -0.12712675, ..., -0.13971423,\n",
      "         -0.02829487,  0.08735804],\n",
      "        ...,\n",
      "        [-0.12138391, -0.02869849,  0.06249087, ..., -0.06436015,\n",
      "          0.05386639,  0.09827765],\n",
      "        [ 0.069966  , -0.10596659,  0.0211336 , ...,  0.11646375,\n",
      "         -0.090852  , -0.07190954],\n",
      "        [-0.11675452,  0.05961167,  0.06120566, ..., -0.01947313,\n",
      "          0.01817157,  0.03124473]]], dtype=float32), array([[[-0.18003096],\n",
      "        [-0.18532327],\n",
      "        [-0.02234131],\n",
      "        [-0.16832387],\n",
      "        [ 0.23347193],\n",
      "        [ 0.19496872],\n",
      "        [ 0.09383248],\n",
      "        [ 0.18655851],\n",
      "        [ 0.12427951],\n",
      "        [ 0.12930965],\n",
      "        [-0.19455935],\n",
      "        [ 0.18822975],\n",
      "        [ 0.10741682],\n",
      "        [-0.13407817],\n",
      "        [ 0.07943378],\n",
      "        [-0.22084111],\n",
      "        [-0.05101433],\n",
      "        [-0.02775738],\n",
      "        [-0.10009588],\n",
      "        [-0.14349851],\n",
      "        [-0.1726845 ],\n",
      "        [-0.00938653],\n",
      "        [-0.14828785],\n",
      "        [-0.20572561],\n",
      "        [ 0.03197614],\n",
      "        [-0.18493497],\n",
      "        [-0.00407638],\n",
      "        [ 0.14973003],\n",
      "        [ 0.08324125],\n",
      "        [-0.22198832],\n",
      "        [-0.06751809],\n",
      "        [ 0.13922438]],\n",
      "\n",
      "       [[ 0.01323436],\n",
      "        [-0.17568654],\n",
      "        [ 0.23532991],\n",
      "        [ 0.06216614],\n",
      "        [-0.03911177],\n",
      "        [-0.12976024],\n",
      "        [ 0.0419301 ],\n",
      "        [ 0.15426396],\n",
      "        [ 0.21776572],\n",
      "        [ 0.2134118 ],\n",
      "        [-0.2105824 ],\n",
      "        [ 0.19133207],\n",
      "        [-0.11274341],\n",
      "        [ 0.10383137],\n",
      "        [ 0.11711644],\n",
      "        [ 0.10970133],\n",
      "        [ 0.1213843 ],\n",
      "        [-0.20546898],\n",
      "        [ 0.22867602],\n",
      "        [-0.10095693],\n",
      "        [-0.11917851],\n",
      "        [ 0.04523312],\n",
      "        [-0.02780236],\n",
      "        [-0.20998475],\n",
      "        [ 0.09650667],\n",
      "        [-0.15855104],\n",
      "        [ 0.23633008],\n",
      "        [-0.188432  ],\n",
      "        [ 0.12107662],\n",
      "        [-0.08003074],\n",
      "        [-0.22310463],\n",
      "        [-0.02498097]],\n",
      "\n",
      "       [[ 0.14945605],\n",
      "        [ 0.08418585],\n",
      "        [ 0.23997276],\n",
      "        [-0.14990191],\n",
      "        [-0.23489532],\n",
      "        [-0.0010825 ],\n",
      "        [ 0.11351664],\n",
      "        [-0.11476304],\n",
      "        [-0.08938721],\n",
      "        [ 0.15808365],\n",
      "        [-0.21720655],\n",
      "        [-0.15302502],\n",
      "        [ 0.01232425],\n",
      "        [-0.17435792],\n",
      "        [ 0.07577376],\n",
      "        [ 0.0637868 ],\n",
      "        [ 0.10049612],\n",
      "        [-0.2171884 ],\n",
      "        [ 0.09220187],\n",
      "        [-0.12755322],\n",
      "        [-0.10291253],\n",
      "        [ 0.0527068 ],\n",
      "        [ 0.11127367],\n",
      "        [-0.18113372],\n",
      "        [-0.1736979 ],\n",
      "        [ 0.03775403],\n",
      "        [-0.23854578],\n",
      "        [ 0.04081038],\n",
      "        [ 0.16871999],\n",
      "        [-0.04389997],\n",
      "        [ 0.10365237],\n",
      "        [ 0.03541851]]], dtype=float32), array([[-3.95797044e-02, -3.78158279e-02,  3.50010455e-01,\n",
      "        -2.75812626e-01,  7.82412067e-02, -5.63505813e-02,\n",
      "        -4.20018971e-01, -2.30445132e-01, -2.19040930e-01,\n",
      "         1.15238376e-01,  5.39857708e-02,  1.93342298e-01,\n",
      "         5.62519825e-04,  8.25459883e-02, -1.81954458e-01,\n",
      "        -4.48461249e-02, -3.98407057e-02,  3.26478511e-01,\n",
      "        -2.67136514e-01, -3.51879925e-01,  1.51594549e-01,\n",
      "         3.80564392e-01, -7.79124796e-02, -2.95616835e-01,\n",
      "        -1.46266893e-01, -7.67199770e-02, -2.72270977e-01,\n",
      "         3.28548282e-01, -3.92967194e-01,  1.87306046e-01,\n",
      "        -7.43290707e-02,  3.49583954e-01],\n",
      "       [-3.50197583e-01, -2.13512436e-01,  3.77838165e-01,\n",
      "        -8.74554086e-03, -2.50105202e-01, -2.62405187e-01,\n",
      "         1.77773818e-01,  1.06869526e-01,  9.17750597e-02,\n",
      "         6.68059513e-02,  2.89376110e-01,  2.33263850e-01,\n",
      "         1.72044802e-02,  1.41420707e-01, -8.81949440e-02,\n",
      "         1.83582634e-01, -3.18259537e-01,  3.61983851e-02,\n",
      "        -3.78376693e-01, -3.19953233e-01,  1.53218463e-01,\n",
      "         1.10922586e-02, -3.06658912e-02, -1.43884271e-01,\n",
      "         1.19031385e-01,  8.62181112e-02,  2.55052149e-01,\n",
      "        -2.82730550e-01,  4.58597168e-02,  2.70448059e-01,\n",
      "         3.32752854e-01, -1.63359963e-03],\n",
      "       [ 1.05693668e-01, -5.02423458e-02, -3.29041570e-01,\n",
      "         2.18063071e-01,  1.48593858e-02, -1.86401885e-02,\n",
      "         3.84824544e-01, -2.22624454e-04,  7.99521059e-02,\n",
      "         3.27181488e-01,  1.97442412e-01, -2.70011753e-01,\n",
      "        -8.84365216e-02,  3.74390274e-01, -1.67652696e-01,\n",
      "        -1.75507084e-01, -1.28690138e-01,  5.24778776e-02,\n",
      "         5.50603978e-02, -6.77761436e-02,  1.76173389e-01,\n",
      "        -3.58261138e-01,  1.53861672e-01, -2.39784569e-01,\n",
      "        -2.96773523e-01,  2.73074985e-01,  1.44869730e-01,\n",
      "        -2.74958938e-01,  4.92066443e-02,  3.15963566e-01,\n",
      "        -2.60958254e-01,  7.62804896e-02],\n",
      "       [ 1.12200126e-01, -2.34304175e-01,  1.32943243e-01,\n",
      "        -3.42079699e-01, -1.61944449e-01, -2.63142794e-01,\n",
      "         1.56994358e-01,  2.13557795e-01,  3.00695151e-01,\n",
      "         3.00146222e-01, -3.37367713e-01, -3.23606163e-01,\n",
      "         4.42986153e-02,  2.39925291e-02,  1.27023131e-01,\n",
      "        -1.20336190e-01, -1.11466974e-01, -3.32003593e-01,\n",
      "        -1.35635227e-01, -2.39133522e-01, -3.05755973e-01,\n",
      "         3.48733109e-03, -6.03411458e-02,  2.09521845e-01,\n",
      "        -1.69987917e-01,  2.24495307e-01,  1.58119082e-01,\n",
      "        -1.99955747e-01,  2.92408317e-01, -2.63396204e-01,\n",
      "        -1.42324656e-01, -1.32983282e-01],\n",
      "       [-1.40498504e-01, -3.24184984e-01, -4.32952903e-02,\n",
      "        -1.58287048e-01,  3.45323652e-01,  1.50142416e-01,\n",
      "         3.92755955e-01, -2.71102160e-01,  3.37749347e-02,\n",
      "        -1.81125868e-02,  1.04393736e-01,  1.66139066e-01,\n",
      "         1.39032766e-01,  8.44390616e-02, -4.29735854e-02,\n",
      "         1.22960627e-01,  2.85290569e-01, -2.78918117e-01,\n",
      "         6.95165098e-02,  1.12580128e-01,  9.25700590e-02,\n",
      "        -1.44718841e-01, -3.88250589e-01, -8.10492486e-02,\n",
      "         8.15398470e-02, -4.85129021e-02,  1.84906274e-01,\n",
      "         1.77205771e-01,  2.70534098e-01,  2.05096200e-01,\n",
      "         1.28458694e-01, -1.88997731e-01],\n",
      "       [-7.10618049e-02,  2.02195019e-01, -2.40212992e-01,\n",
      "         1.56797841e-01,  3.17364752e-01,  7.00792745e-02,\n",
      "         8.99321809e-02,  2.18355969e-01,  1.63672850e-01,\n",
      "        -2.74966329e-01,  3.82750854e-02, -2.61703916e-02,\n",
      "        -3.66929024e-01,  3.03300619e-01, -1.73511147e-01,\n",
      "         3.58897239e-01, -2.74415836e-02, -1.50286078e-01,\n",
      "        -2.73932040e-01, -2.29656640e-02,  3.37665647e-01,\n",
      "         3.15177351e-01, -4.30912040e-02,  5.05170263e-02,\n",
      "        -1.35294199e-01,  1.21321194e-01, -9.98904034e-02,\n",
      "        -1.12290613e-01, -1.84437633e-01, -2.33988658e-01,\n",
      "        -2.10009456e-01, -2.58335799e-01],\n",
      "       [ 2.74597436e-01, -3.68438482e-01,  1.28939468e-02,\n",
      "        -2.26553947e-01,  4.96262796e-02,  4.64372896e-02,\n",
      "        -7.90286139e-02,  2.32040286e-01, -2.65624553e-01,\n",
      "         5.86201698e-02, -2.20409676e-01, -2.74226546e-01,\n",
      "        -3.18763584e-01,  2.84953386e-01, -2.05983698e-01,\n",
      "         1.14971831e-01, -1.79659352e-01,  3.53163965e-02,\n",
      "        -4.16170172e-02,  7.25195259e-02, -2.65582889e-01,\n",
      "        -2.16536984e-01,  3.81327093e-01, -1.48075983e-01,\n",
      "         8.39857236e-02,  1.54708982e-01,  1.31593272e-01,\n",
      "        -5.13644665e-02,  3.36835593e-01,  3.18387300e-01,\n",
      "        -1.79393068e-01,  1.39818788e-01],\n",
      "       [-1.24971606e-01, -8.05501118e-02,  1.00494185e-02,\n",
      "         3.52845609e-01, -5.63064702e-02,  2.81690508e-01,\n",
      "        -2.10422501e-01, -4.81786653e-02, -1.85976401e-01,\n",
      "        -3.02696615e-01, -1.52987927e-01,  2.01984555e-01,\n",
      "        -6.17283173e-02, -3.09495538e-01, -4.36346009e-02,\n",
      "         2.86003109e-02,  6.31616265e-02,  4.79638167e-02,\n",
      "        -3.58242810e-01,  3.41492802e-01, -1.82949796e-01,\n",
      "        -2.19056353e-01,  1.35676078e-02, -2.94138044e-01,\n",
      "        -3.06522489e-01, -2.92712580e-02, -1.18901379e-01,\n",
      "         8.65807086e-02,  2.89395660e-01, -7.69296438e-02,\n",
      "         2.08780453e-01,  1.30572617e-01]], dtype=float32), array([-0.01013453,  0.02888852, -0.009334  ,  0.03565942,  0.04561084,\n",
      "       -0.01136703,  0.08905734, -0.01456356,  0.03538626, -0.00767452,\n",
      "       -0.01302479, -0.00882936, -0.00700577, -0.01226748, -0.00764894,\n",
      "        0.04045167, -0.01023831,  0.02683214, -0.00997903, -0.01573666,\n",
      "       -0.01193189, -0.01114778,  0.03843132, -0.01302233,  0.04136014,\n",
      "       -0.00975511,  0.09209263, -0.01003763,  0.06493326, -0.00907918,\n",
      "       -0.01239736, -0.00714857], dtype=float32), array([[-0.32023928],\n",
      "       [ 0.25547683],\n",
      "       [-0.222129  ],\n",
      "       [ 0.18141982],\n",
      "       [ 0.2626018 ],\n",
      "       [-0.2842432 ],\n",
      "       [ 0.19305897],\n",
      "       [-0.1551815 ],\n",
      "       [ 0.44536188],\n",
      "       [-0.39187562],\n",
      "       [-0.33348784],\n",
      "       [-0.02604985],\n",
      "       [-0.01490121],\n",
      "       [-0.1244519 ],\n",
      "       [-0.3602085 ],\n",
      "       [ 0.4386316 ],\n",
      "       [-0.1897504 ],\n",
      "       [ 0.3528922 ],\n",
      "       [-0.00987511],\n",
      "       [-0.25056127],\n",
      "       [-0.16651969],\n",
      "       [-0.18254367],\n",
      "       [ 0.4171418 ],\n",
      "       [-0.12716895],\n",
      "       [ 0.32950225],\n",
      "       [-0.293128  ],\n",
      "       [ 0.14376807],\n",
      "       [-0.22933033],\n",
      "       [ 0.49837482],\n",
      "       [-0.11253477],\n",
      "       [-0.06251002],\n",
      "       [-0.12551823]], dtype=float32), array([0.03381212], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(lstmgan.discriminator.layers[1].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[0.        , 0.        , 0.        , ..., 0.06588751,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       [[0.16237384, 0.266411  , 0.        , ..., 0.06104565,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       [[0.18120235, 0.2786518 , 0.        , ..., 0.07490608,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.07108489, 0.15501471, 0.        , ..., 0.21506804,\n",
      "         0.12459307, 0.10231607]],\n",
      "\n",
      "       [[0.05813237, 0.13183069, 0.        , ..., 0.19188169,\n",
      "         0.        , 0.07842781]],\n",
      "\n",
      "       [[0.07108489, 0.15501471, 0.        , ..., 0.21506804,\n",
      "         0.12459307, 0.10231607]]], dtype=float32)]\n",
      "(1, 556556, 1, 30)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "hidden_layers = keras.backend.function(\n",
    "[lstmgan.discriminator.layers[0].input],  # we will feed the function with the input of the first layer  \n",
    "[lstmgan.discriminator.layers[0].output,] # we want to get the output of the first layer\n",
    ")\n",
    "h=hidden_layers([df_train])\n",
    "print(h)\n",
    "print(np.shape(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=np.array(h)  \n",
    "\n",
    "arr_reshaped = np.array(h).reshape(556556, 30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(556556, 30)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(arr_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('abnormal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.16237384 0.266411   0.         0.         0.26908255 0.\n",
      "   0.14949085 0.07964714 0.         0.11576729 0.         0.\n",
      "   0.         0.10549675 0.         0.06734058 0.07298152 0.0862366\n",
      "   0.24368042 0.         0.         0.         0.         0.\n",
      "   0.09065105 0.         0.11302204 0.06104565 0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "#To check 1st row\n",
    "print(h[:,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_1 = pd.read_csv('abnormal.csv')\n",
    "csv_2 = pd.read_csv('abnormallabel.csv')\n",
    "\n",
    "result = pd.concat([csv_1, csv_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"lgcabnormalresult.csv\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27da47675c8>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['gen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27da4073708>,\n",
       " <matplotlib.lines.Line2D at 0x27da44849c8>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAewklEQVR4nO3deXzU1b3/8ddnshBCAiGQsENAkUWQLYqK5aq4IVjtvVpp66217aW9t/3d6u211VYfba9ttYtt9dFeeykurXWtW61bte4rGGTft0DCGggJWUgmM3N+f8wQkjCQCWSS78y8n49HHpn5zndmPmcI75yc7/merznnEBER7/J1dwEiInJ8CmoREY9TUIuIeJyCWkTE4xTUIiIelx6PF+3fv78rKiqKx0uLiCSlJUuW7HPOFUR7LC5BXVRURElJSTxeWkQkKZnZtmM9pqEPERGPU1CLiHicglpExOMU1CIiHqegFhHxuHaD2szGmNmyFl8HzezGLqhNRESIYXqec249MBnAzNKAHcCz8S1LREQO6+jQxyxgs3PumPP9TsaGPTV8uHl/PF5aRCRhdTSo5wGPRXvAzOabWYmZlVRUVJxQMZf8+h0+94ePTui5IiLJKuagNrNM4NPAX6I97pxb4Jwrds4VFxREPQvyuJqCoQ4/R0QkFXSkRz0b+MQ5tycehRgwsHdWPF5aRCShdSSoP8cxhj06Q3qaj3NP6cew/J7xegsRkYQUU1CbWTZwMfBMXKsxCGkERESklZhWz3PO1QP94lwLPrN4v4WISMLx1JmJPoOQroouItKKp4LaMAW1iEgbngpqnw9CymkRkVY8FdRmhjrUIiKteSqofQZOSS0i0oqnglpj1CIiR/NUUPsMFNMiIq15KqjNjJCOJoqItOKxoEYHE0VE2vBUUPvMNPQhItKGx4JaZyaKiLTlqaA206wPEZG2PBbUGqMWEWnLU0Ht05mJIiJH8VRQGxqjFhFpy1NBrVkfIiJH81hQq0ctItKWp4IajVGLiBzFU0Hti1yJSyvoiYgc4bGgDie1lvsQETnCU0F9+NK2GqcWETkipqA2szwze8rM1pnZWjM7Jy7FRMY+lNMiIkekx7jfPcArzrmrzSwTyI5HMZGRD/WoRURaaDeozaw3MBP4EoBzzg/441GMoR61iEhbsQx9jAIqgAfNbKmZLTSzXm13MrP5ZlZiZiUVFRUnVdTbG07u+SIiySSWoE4HpgL3OeemAHXALW13cs4tcM4VO+eKCwoKTqiYen8AgK//eckJPV9EJBnFEtTlQLlzblHk/lOEg7vTtRyb1iW5RETC2g1q59xuoMzMxkQ2zQLWxKOYQItw3l8Xl2FwEZGEE+usj/8HPBKZ8bEFuCEexbTsRR/yB+PxFiIiCSemoHbOLQOK41sKBENHbtdFxqtFRFKdp85MDIaOJHW9etQiIoDHgrrl8cN69ahFRACPBXWwxawP9ahFRMK8FdTBI0Fd26AetYgIeC2oW/Soyw8c6sZKRES8w1NB3XJ63tZ9td1YiYiId3gqqFue8HKoSWPUIiLgsaBuOfTRck61iEgq81RQtxz60HUTRUTCPBXUwRZBrYsHiIiEeSqo++VkNt/W4nkiImGxLsrUJW6fO57Jw/J4/OMy9ahFRCI81aPOzkzn2jOHk2amoBYRifBUUB/mMyMUgj0HGyirrO/uckREupWnhj4OMwsfTJz+09cBKL1rTjdXJCLSfbzbo9bQh4gI4NGgTvOZZn2IiER4MqgPD32IiIhHgzp8MFFBLSICng1qnfAiInKYJ4M6PEatpBYRAY8GtVnrg4nzFnzYfcWIiHSzmOZRm1kpUAMEgYBzrjieRfms9Up6H22pJBRy+HwWz7cVEfGkjvSoL3DOTY53SEN46CPYZujjvrc3x/ttRUQ8ybNDH5v2tr4U15qdB7upGhGR7hVrUDvgVTNbYmbzo+1gZvPNrMTMSioqKk6uKDt6iMOhg4sikppiDeoZzrmpwGzgG2Y2s+0OzrkFzrli51xxQUHByRUVZShak0BEJFXFFNTOuZ2R73uBZ4Gz4llUWrQetYJaRFJUu0FtZr3MLPfwbeASYFU8i7IoQS0ikqpimZ43AHg2Ep7pwKPOuVfiWZQ/yiXINUYtIqmq3aB2zm0BJnVBLc0q6xqj1NGVFYiIeIcnp+cdqGvq7hJERDzDk0HdEAgetU0dahFJVZ4M6qYoY9QiIqnKk0HtD0Q5mKgutYikqIQJahGRVJVAQa0utYikJm8GdbR51MppEUlRngzqpuDRqaycFpFU5cmgFhGRIzwZ1L/9/BTGDMhtte2NdXsp3VfXTRWJiHQfTwb13DMG8+TXzzlq+/PLd3ZDNSIi3cuTQQ2QHmVRaq2pJyKpyLNBnaYL2YqIAApqERHP825QR7l4gK4nICKpyLNB7Ys2Rq2kFpEU5NmgBvj0pMHdXYKISLfzdFAHQzofUUTE00EdCGkVPRERTwd127WZNEQtIqnI40GtHrWISMxBbWZpZrbUzF6IZ0EtBTRGLSLSoR71t4C18SokmrYHE00nkYtICoopqM1sKDAHWBjfclpr26PWGLWIpKJYe9S/Ab4DHHPQ2Mzmm1mJmZVUVFR0Rm2aniciQgxBbWZzgb3OuSXH2885t8A5V+ycKy4oKOiU4m6dPZaiftmd8loiIokqlh71DODTZlYKPA5caGZ/jmtVEcVF+bx18wXN9zXyISKpqN2gds7d6pwb6pwrAuYBbzjnrot7ZSIiAnh8HrWIiEB6R3Z2zr0FvBWXSmKgWR8ikooSqketedQikooSKqh/8tJa9hxsIKRpeyKSQhIqqAGm//R1fvfmpu4uQ0SkyyRcUAP8Y93e7i5BRKTLJGRQNzYFu7sEEZEuk5BB7W+7ULWISBJLyKBubFJQi0jqSMygDiioRSR1JGZQa4xaRFJIQgZ10GketYikjoQMauW0iKSSxAxqlNQikjoSMqgbmkI88N7W7i5DRKRLJGRQA/zPC2u6uwQRkS6RsEEtIpIqFNQiIh6noBYR8TgFtYiIxymoRUQ8TkEtIuJxSRPU976+kTtfXtvdZYiIdLqkCepfvbaB/3t7S3eXISLS6doNajPLMrPFZrbczFab2Y+6ojAREQlLj2GfRuBC51ytmWUA75nZy865j+JcW7ucc5hZd5chIhJX7faoXVht5G5G5KtLV0X63FnDmm/ffOkYJg3LAyCktZlEJAXENEZtZmlmtgzYC7zmnFsUZZ/5ZlZiZiUVFRWdWuQPrji9+fY3LjiVi8cVAhCMktSNAV1UQESSS0xB7ZwLOucmA0OBs8xsQpR9Fjjnip1zxQUFBZ1aZNvRDZ8vvCEUZWHqMbe9wmtr9nTq+4uIdKcOzfpwzlUBbwGXxaOYY/G1SerD96MFNcAb6/bGvSYRka4Sy6yPAjPLi9zuCVwErItzXa1raHM/rTmoj/UMDV6LSPKIZdbHIOCPZpZGONifdM69EN+yWms7s+Pw3Whj1KBLdYlIcmk3qJ1zK4ApXVDLMR3Vo46MUTslsoikgIQ4M/Gog4mRDcfqUYuIJJMECeo2BxMjPerHPy6joqbxqP3V0RaRZJIQQd1WJKf5xd/Xc9fLXXpcU0Sky8VyMNFzWk7Xe/qTcp7+pDzqfvX+AAfqmxiS17OrShMR6XQJ2aNOa2d9DxeZnvfF+xcz4643uqIkEZG4ScigjnUdppJtB+JbiIhIF0jIoD48Pe9YdDBRRJJJQgZ121PK26P51iKSyBIzqNvrUbe5r+nWIpLIEjOoO3itgEAoFJ9CRES6QEIF9ZfOLQI6PvShnBaRRJYw86hL75rTfLumoem4+z61pJzBfbKa74d71GnxKk1EJK4Sqkd92O7qo08bb+veNzY139aaICKSyBIyqE8bkNOh/RXUIpLIEjKoZ08c1KH9FdQiksgSMqgB7r++OOZ9g5pHLSIJLGGDeta4ATHvGwgqqEUkcSVsUANMHZ4X034h53DO8ZMX1/Ds0ugr7YmIeFVCB/Xtc8fHtF8g5Pj76t384d2t3PTE8jhXJSLSuRI6qNN9sZUfDDmq6o/MvV63+yDrd9fEqywRkU6VMCe8RJOeFtsZim1nfVz2m3eB1ifRiIh4VbtdUjMbZmZvmtlaM1ttZt/qisJi0TMjtrMNNT1PRBJZLGMHAeDbzrlxwNnAN8wstsHhOBvRLzum/R54f2ucKxERiZ92g9o5t8s590nkdg2wFhgS78JiYWY88tXp7e73zCc7uqAaEZH46NDBRDMrAqYAi6I8Nt/MSsyspKKiopPKa9+5p/TjU6P7c8+8ycfd71iDHxv21HDj40sJBLXEnoh4U8xBbWY5wNPAjc65g20fd84tcM4VO+eKCwoKOrPG9uri4a9M58rJRzr5r94086j9bn1mZdTn/+djS3lu2U427q2NW40iIicjpqA2swzCIf2Ic+6Z+JZ0ckb0y+aUghxumFHE9y4f2+7+TZGedHvXYRQR6S7tTs8zMwPuB9Y6534V/5JO3Lo7LsNnRprP+MEVpwNQUnqAV9fsibp/UzDUPCPEH9DQh4h4Uyw96hnAvwIXmtmyyNflca7rhGRlpJGZ3rpJ9103jekj86Puf+Hdb1G6vx6Aen+QhqYgRbe8yFNLdJq5iHiHxeMK3cXFxa6kpKTTX/dkvLV+L7WNAaYO78vHpZV86/FlR+3Tp2cG1YeayO2RzsNfnc7kYXmtHq9paGJ5WTXnje7fNUWLSMowsyXOuajLgib0KeQdcf6YQuaeMZjBeT3J75UZdZ/qQ+HTzGsaA1z1u/epPtTE3a+u59ml5WzaW8M3H13KdfcvYmfVoa4sXURSXEKfQn6iMtNi+/30y7+v5+GPth21/foHFnPnP0/EHwhxxrA86v0BCnOzoryCiMjJS5mhj5ZCIcd9b29mzsRB/P7tzTz+cRn/OWs0cyYO4tLfvNOh1+qfk0nIwSe3X0xDU5CsFqe1b99fz7D8nlgHr5ouIqnneEMfKRnUbe2taaB/rx74fMbemgYAzvrJ6x16jV9eM4n//stypo3oy3cvG8tLK3fx0AelzDtzGFv21XH/9cXkZmXEo3wRSQIK6hOwemc1N/9lBacP7s1flpTz+Pyzmbfgo5N6zSe/dg4vrdzFpacP5JxT+nVSpSKSDBTUnWh3dQNn3xnubffNzuBAi3WuO+K2OeNoaApyZlE+00eFQ7uhKUhmmg+fTr4RSTkK6k62dPsBtlTUMXZQLt99egWrdhx1Rn2H/PCK8Ywd1Jt5Cz7ihhlF3DjrNPpka5hEJJUoqONoV/UhzrnzDQBW/PASfvDX1YwekMPPX1kPwEXjBvCPtdHPjGzPgzecSUVNIw1NQT5bPKzVgUqAssp60tMM52BwXs+Ta4iIdCsFdZzd9txKLho3gPPHFDZv++Hzq3nog1LW//gy/vTBNn7y0lpGFfRiS0Ud98ybzLKyKh58v7RD73PuKf24cGwhU0f0ZdLQPE753kutHr9oXCELrz+zM5okIl1MQd0NQiFHrT9A76wMnHMEQo56f5Dq+iaGRy540NAUpDEQwh8IceZP/kH/nEz21foBuGryYJ5btrPD7/tfF5/GZ6YM4a6X17Fpby0vfetTpPkMfyBE1SE/+2v9PLJoG9++eAx9j3Hij4h0PQV1Atiwp4b8XpkU//gfQPh6jruqD+Ez49/+VMKK8uoTet2LxhUycUgev/7Hhlbb87IzeObfz2VUQc5J1y4iJ09BnUCWlVVhwKQ264wc9uii7Xzv2ehra5+IaSP6suBfp7G5oo53NlRw+cRBDOqTRV52RtQTdarq/dQ0BBiWH9tl0EQkNgrqJLPw3S38+MW1LPxiMU+UlJGfncnO6kP06ZnB3Z+dxJjbXgHgikmD+dvyjg+fHPbrayfx0AfbWF5WxW1zxvHFc4o47baXAXjgS8VMGda31fDJ1x4u4bQBuXz7kjHN2zTlUCQ2CuokEwo5VuyoPmp1v2iu/N37LC+rilstsycM5P1N+7ht7ni+89QKIDxsA+CcY+StL3HZ6QP5+vmnUFnXyITBfSjs3XpdlLrGANmZaTrVXlKagjqFOef40d/W8NAHpdwwo4irpw3l1MIcnAvPTBk7MJeahgDTRvTlr8t28kRJWae876dG92fjnlp2H2w46rF1d1zGj/62hpLSSn73halc8ut3+OlnJvL56cOb96mq9zPljte448oJXHf2CAAaA0F2VTVQ1L9Xp9Qo4iUKaonZm+v3svDdLVwwppDionwmDulDIBRizG2vMLowJ27Xlpw6PI/Rhbm8tGoXV04ezJ8/2g6Az2DLnXMIhRzffXoFf1lSzmeLh3LVlCEU9evFg+9vZUfVIS4YU8g1xcOaX29FeRXjBvUmI8aVEkW6m4JaTtqGPTUM6pNFbWOAxVsrWbPrIDdddBrTf/o6F44t5NmlO456TstT7E8bkENGmo/VOzt+FuflEwfy0srd7e73m2sns3FvDYPzevL9Z1dxw4wifnDF6TjnWg2rLC+rYuygXHqkpx3n1US6loJa4i4QDNEQCOGc4+PSSqoPNfHOhn08u3QHk4fl8dTXzyE9zceHm/fzuT+EF7f6bPFQnizpmsue3feFqeRlZ3Ld/YsIhhxfmzmKKyYNZuWOav66bAf/9qlRnDYgl/11fiYPy2N/bSP9cno0P985x8J3t3LB2EJOLdSURul8CmrpFi+u2MU3Hv2EJ+af3bzwFHBUD7eyzs8fPyjlntc3tnr+pGF5jB+Uy2OLj4ybL/xiMeeN7s/Y21+JfwOAM4v6MnvCIMYMzOULCxcB8Ldvnkf5gXoy0nz07ZXJtBF9OeQPsnVfHZsraqmq9/NPpxU2n9jUGAhSVlnPqYW5XVKzJCYFtXSb3dUNDOzT8avf7K9tJCcrnR7paRzyB9lzsIG87AzyssPTATftrcEfcKwor+KWZ8LzyqePzGd/nZ/zTu3PVVOGsLysitrGAL/4+/pObVOsbp87njteWNN8/7lvzODXr23gy+eNZHRhDhv21FCYm0W/nEwO+YNRD5Ku3llN76wMzVtPAQpqSVqhkGNxaSXTR+Yfc3pfVb2fLyxcxNZ9dZTcdhGrdx7kmt9/CMBXzhvJ/e9t7ZJa83tlUlnnP+bjd/3zRG55ZiWXnj6A718+nvve3tT818Tfb5zJ1n21nDe6gJwe6Ryo81PnD5CZ5qNnZhqbK+rISDP6Zmey4J0t3Hr5WDLTfLyzcR97Dza0OtDaniXbKhnQO4uhffXLoSspqEXa2F/bSHZmOj0z02hoCnLv6xu5etpQzIzP/O/7PHTDWeT0SCcrw8e9r2/kqilDOGdUP55fvpNLxg/krpfX8scPj1xP80TXZukM6T4jEDr+/+N75k3m05MG88qq3Zw5Mh/n4LHF2/n380/h/97eTE6PdLJ7pHPNtKGMvDW82Ndb/32+pkJ2oZMKajN7AJgL7HXOTYjlDRXUkuwaA0FKSg/w/qZ9zDljEKcP7sMPn1/N8vIqHrj+TH7/zmaWbqvi6uKhzBpbyLTIGi63zh7LnS+vA2DlDy8hKyONA/X+Dl/6rbNcMKaAN9dXHPPxh79yFgW5PVhRVs27m/ZxzbShnHdqf8zgw837KS7KZ8u+Wha8vYWfXX0Gh5qCNDQFyc/O5MWVu1hRXs3tc8dT7w/w9JJyrj1zOJnp4SmTDU1BeqT7mv8SagqGSPdZyp74dLJBPROoBf6koBY5MftrG9lZ1cDEoX3YV9tIhs/X6uIQ723cx5cf+piemWks+t4s7n9vK03BEJ+fPpx/ue8DyioPNe979bShjB2Yy49fXMvgPln0yc5k7a7wtMexA3OZNa6Q3725uUvaNeeMQby4YhcA371sLD97JfxL6D/OP4X/fStcw7vfuYDfv72ZRxZtZ8yAXO7+7CSe+LiMhz/axvcuH8v8mafw3NId3PjEMgpye3DDjCK+MH0EL63cxUXjBlCQ2yPqe4dCjpdW7WL2hEGktVmiYNWOasYN6t1q+86qQ+T3yjxqXXevOOmhDzMrAl5QUIvEjz8QwuGizu9esq2SXj3SWb+7hisnD2n1WFllPQfq/QzO60nPjDR69Uhn0Zb9XLvgI4pH9OXmS8dwbYvrfc49YxAvRMLVCzLTfPiDoWNuO6son8Wlldwwo4iZows4bWAuQ/J68vji7dzyzEruuGoC/zJ1CDsOHGJpWRVLSg/wREkZ82eOIhB0fOeyMXz9z0t4a30FsycM5L7rpuEPhGgKhnCEZyEFgu64y/6WVdYzoHcW6T5j1c5qzhiaR+m+Okb0y+60vwC6JKjNbD4wH2D48OHTtm3bdqxdRaQLbKmoZWT/XpgZu6sbCDrHwN5ZpPmMPQcbeGHFLobnZzM8P5vRhTkEQo5nPikn5GDljmouHl/INx9dSr0/CBz7GqG9MtOoi+xzPP85azT3tpmCeaJart0e6/sfds6ofpTur2NX9ZHlDXqk+3j75gvYVX2I1TsPkt8rk5+/so47rprA4LyezLr7bT4/fTg90n2tLvhx86Vj+KfTCpgwpA/1/gBZ6WknvACZetQicsLKD9RTWefnjKF5BEOOqno/v31zU3Ngbf7p5fzslXX0ykznvNH9qG0Mcv0Di/nF1Wdwc2ShrisnD+aeeVN4YcVOXlyxi7lnDGZQXhafbDvAqIJefPmhI3nx7H+cy/PLdza//lkj81m8tZKiftmU7q/v6ubHJKdHOrWNAQC23nn5CfWyFdQi0uleWLGTgpwerU5maivWZW5/8NdV/PHDbTzy1enMOLV/1H2agiHufnUDmWnGvW9s4txT+jFmYC7PLt1BVaSn/6nR/fnyjJF8sv0A72zchxFe4z2a8YN6s72yntkTBrJoayXbKzvnl8Dh1SM7SkEtIp52yB/kbyt2ck1kimSsDl/mrqKmkcx0H/1zjj7w2NAUJCsjjW8/uZyPSyt59aaZLHhnC185byS9eqQ3v86ce99jza4ja9FkpvsYmteTgX2y+GDzfgBevWkmD31QyqOLwouGnVnUlzuumsDu6ga+9ODHzYuInYiTnfXxGHA+0B/YA/zAOXf/8Z6joBaRRFRZ5ycrw0e9P9gq9JduP0Aw5CguygfCBxfTfMbgvJ7N+7y5fi/D+maf8FowOuFFRMTjjhfUWqxXRMTjFNQiIh6noBYR8TgFtYiIxymoRUQ8TkEtIuJxCmoREY9TUIuIeFx6dxfQysu3wO6V3V2FiMiJGTgRZt/V6S+rHrWIiMd5q0cdh99EIiKJTj1qERGPU1CLiHicglpExOMU1CIiHqegFhHxOAW1iIjHKahFRDxOQS0i4nFxuWaimVUA207w6f2BfZ1YTiJQm1OD2pz8Tqa9I5xzBdEeiEtQnwwzKznWBR6TldqcGtTm5Bev9mroQ0TE4xTUIiIe58WgXtDdBXQDtTk1qM3JLy7t9dwYtYiItObFHrWIiLSgoBYR8TjPBLWZXWZm681sk5nd0t31dBYzG2Zmb5rZWjNbbWbfimzPN7PXzGxj5HvfFs+5NfI5rDezS7uv+pNjZmlmttTMXojcT+o2m1memT1lZusi/97npECbb4r8XK8ys8fMLCvZ2mxmD5jZXjNb1WJbh9toZtPMbGXksXvNzGIuwjnX7V9AGrAZGAVkAsuB8d1dVye1bRAwNXI7F9gAjAd+DtwS2X4L8LPI7fGR9vcARkY+l7TubscJtv2/gEeBFyL3k7rNwB+Br0ZuZwJ5ydxmYAiwFegZuf8k8KVkazMwE5gKrGqxrcNtBBYD5wAGvAzMjrUGr/SozwI2Oee2OOf8wOPAld1cU6dwzu1yzn0SuV0DrCX8A34l4f/YRL5fFbl9JfC4c67RObcV2ET480koZjYUmAMsbLE5adtsZr0J/4e+H8A553fOVZHEbY5IB3qaWTqQDewkydrsnHsHqGyzuUNtNLNBQG/n3IcunNp/avGcdnklqIcAZS3ul0e2JRUzKwKmAIuAAc65XRAOc6AwsluyfBa/Ab4DhFpsS+Y2jwIqgAcjwz0LzawXSdxm59wO4JfAdmAXUO2ce5UkbnMLHW3jkMjttttj4pWgjjZWk1TzBs0sB3gauNE5d/B4u0bZllCfhZnNBfY655bE+pQo2xKqzYR7llOB+5xzU4A6wn8SH0vCtzkyLnsl4T/xBwO9zOy64z0lyraEanMMjtXGk2q7V4K6HBjW4v5Qwn9CJQUzyyAc0o84556JbN4T+XOIyPe9ke3J8FnMAD5tZqWEh7EuNLM/k9xtLgfKnXOLIvefIhzcydzmi4CtzrkK51wT8AxwLsnd5sM62sbyyO2222PilaD+GBhtZiPNLBOYBzzfzTV1isiR3fuBtc65X7V46Hng+sjt64G/ttg+z8x6mNlIYDThgxAJwzl3q3NuqHOuiPC/5RvOuetI7jbvBsrMbExk0yxgDUncZsJDHmebWXbk53wW4WMwydzmwzrUxsjwSI2ZnR35rL7Y4jnt6+4jqi2Ool5OeEbEZuD73V1PJ7brPMJ/4qwAlkW+Lgf6Aa8DGyPf81s85/uRz2E9HTgy7MUv4HyOzPpI6jYDk4GSyL/1c0DfFGjzj4B1wCrgYcKzHZKqzcBjhMfgmwj3jL9yIm0EiiOf02bgt0TODI/lS6eQi4h4nFeGPkRE5BgU1CIiHqegFhHxOAW1iIjHKahFRDxOQS0i4nEKahERj/v/B3dOY6inU64AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['dis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature22</th>\n",
       "      <th>feature23</th>\n",
       "      <th>feature24</th>\n",
       "      <th>feature25</th>\n",
       "      <th>feature26</th>\n",
       "      <th>feature27</th>\n",
       "      <th>feature28</th>\n",
       "      <th>feature29</th>\n",
       "      <th>feature30</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065464</td>\n",
       "      <td>0.027737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068303</td>\n",
       "      <td>0.165911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060979</td>\n",
       "      <td>0.008422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097872</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.084868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.186019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070595</td>\n",
       "      <td>0.247596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.249952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.086173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071894</td>\n",
       "      <td>0.246576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.086520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.152395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052850</td>\n",
       "      <td>0.246624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.229947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.065463</td>\n",
       "      <td>0.027741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068333</td>\n",
       "      <td>0.165912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060969</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097872</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0  0.065464  0.027737       0.0       0.0       0.0       0.0  0.185455   \n",
       "1  0.084868  0.000000       0.0       0.0       0.0       0.0  0.186019   \n",
       "2  0.086173  0.000000       0.0       0.0       0.0       0.0  0.185362   \n",
       "3  0.086520  0.000000       0.0       0.0       0.0       0.0  0.152395   \n",
       "4  0.065463  0.027741       0.0       0.0       0.0       0.0  0.185447   \n",
       "\n",
       "   feature8  feature9  feature10  ...  feature22  feature23  feature24  \\\n",
       "0       0.0  0.068303   0.165911  ...   0.060979   0.008422        0.0   \n",
       "1       0.0  0.070595   0.247596  ...   0.105729   0.000000        0.0   \n",
       "2       0.0  0.071894   0.246576  ...   0.106265   0.000000        0.0   \n",
       "3       0.0  0.052850   0.246624  ...   0.090389   0.000000        0.0   \n",
       "4       0.0  0.068333   0.165912  ...   0.060969   0.008425        0.0   \n",
       "\n",
       "   feature25  feature26  feature27  feature28  feature29  feature30  label  \n",
       "0   0.035205        0.0   0.250703        0.0        0.0   0.097872      0  \n",
       "1   0.014328        0.0   0.249952        0.0        0.0   0.000000      0  \n",
       "2   0.014574        0.0   0.250262        0.0        0.0   0.000000      0  \n",
       "3   0.018661        0.0   0.229947        0.0        0.0   0.000000      0  \n",
       "4   0.035205        0.0   0.250699        0.0        0.0   0.097872      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test=df1.label\n",
    "\n",
    "x_test=df1.drop('label',axis=1).values.reshape(-1,30,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.20211266]]\n",
      "\n",
      " [[0.19766198]]\n",
      "\n",
      " [[0.19725189]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.2016375 ]]\n",
      "\n",
      " [[0.20584811]]\n",
      "\n",
      " [[0.20907736]]]\n"
     ]
    }
   ],
   "source": [
    "#y_pred=lstmgan.generator.predict(x_test)\n",
    "y_pred=lstmgan.discriminator.predict(x_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "yhat_probs = lstmgan.discriminator.predict(x_test, verbose=0)\n",
    "yhat_classes=np.argmax(yhat_probs,axis=1)\n",
    "# predict crisp classes for test set\n",
    "#yhat_classes = model.predict_classes(x_val, verbose=0)\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "#yhat_classes = yhat_classes[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-0ee39934b722>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# We set the threshold equal to the training loss of the autoencoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtesting_set_predictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlstmgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "# Helper function that calculates the reconstruction loss of each data sample\n",
    "def calculate_losses(x,preds):\n",
    "    losses=np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        losses[i]=((preds[i] - x[i]) ** 2).mean(axis=None)\n",
    "        \n",
    "    return losses\n",
    "\n",
    "# We set the threshold equal to the training loss of the autoencoder\n",
    "threshold=history.history[\"loss\"][-1]\n",
    "\n",
    "testing_set_predictions=lstmgan.discriminator.predict(x_test)\n",
    "test_losses=calculate_losses(x_test,testing_set_predictions)\n",
    "testing_set_predictions=np.zeros(len(test_losses))\n",
    "testing_set_predictions[np.where(test_losses>threshold)]=1\n",
    "\n",
    "\n",
    "error_df = pd.DataFrame({'Reconstruction_error': testing_set_predictions, 'True_class': y_test})\n",
    "error_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,precision_score,f1_score\n",
    "accuracy=accuracy_score(y_val,testing_set_predictions)\n",
    "recall=recall_score(y_val,testing_set_predictions)\n",
    "precision=precision_score(y_val,testing_set_predictions)\n",
    "f1=f1_score(y_val,testing_set_predictions)\n",
    "print(\"Performance over the testing data set \\n\")\n",
    "print(\"Accuracy : {} , Recall : {} , Precision : {} , F1 : {}\\n\".format(accuracy,recall,precision,f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/#:~:text=Optimal%20Threshold%20for%20Precision%2DRecall%20Curve,-Unlike%20the%20ROC&text=Recall%20is%20calculated%20as%20the,positives%20and%20the%20false%20negatives.\n",
    "#Optimal Threshold Tuning\n",
    "# search thresholds for imbalanced classification\n",
    "from numpy import arange\n",
    "from numpy import argmax\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')\n",
    " \n",
    "\n",
    "# predict probabilities\n",
    "#yhat = model.predict_proba(x_val)\n",
    "# keep probabilities for the positive outcome only\n",
    "#probs = yhat[:, 1]\n",
    "# define thresholds\n",
    "thresholds = arange(0, 1, 0.001)\n",
    "# evaluate each threshold\n",
    "scores = [f1_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each threshold\n",
    "scores = [accuracy_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, Accuracy-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each threshold\n",
    "scores = [precision_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, Precision-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each threshold\n",
    "scores = [recall_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, recall-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# ROC AUC\n",
    "auc = roc_auc_score(y_test, yhat_probs)\n",
    "print('ROC AUC: %f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=8\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(df_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "#x_test = keras.preprocessing.sequence.pad_sequences(x_test , maxlen=maxlen)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "#https://www.kaggle.com/suicaokhoailang/10-fold-lstm-with-attention-0-991-lb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import sklearn.metrics as sk\n",
    "from sklearn.metrics import roc_curve,f1_score\n",
    "from sklearn.metrics import auc\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=42069)\n",
    "preds = []\n",
    "fold = 0\n",
    "aucs = 0\n",
    "for train_idx, val_idx in kf.split(x_train):\n",
    "    x_train_f = x_train[train_idx]\n",
    "    y_train_f = y_test[train_idx]\n",
    "    x_val_f = x_train[val_idx]\n",
    "    y_val_f = y_test[val_idx]\n",
    "    lstmgan.discriminator.compile(optimizer, \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    #model = get_model()\n",
    "    lstmgan.discriminator.fit(x_train_f, y_train_f,\n",
    "              batch_size=256,\n",
    "              epochs=10,\n",
    "              verbose = 1,\n",
    "              validation_data=(x_val_f, y_val_f))\n",
    "\n",
    "    # Get accuracy of model on validation data. It's not AUC but it's something at least!\n",
    "    preds_val = lstmgan.discriminator.predict([x_val_f], batch_size=512)\n",
    "    preds.append(lstmgan.discriminator.predict(x_test))\n",
    "    fold+=1\n",
    "    fpr, tpr, thresholds = roc_curve(y_val_f, preds_val, pos_label=1)\n",
    "    # calculate scores\n",
    "    #lr_f1, lr_auc = f1_score(testy, yhat), auc(lr_recall, lr_precision)\n",
    "    aucs += auc(fpr,tpr)\n",
    "    print('Fold {}, AUC = {}'.format(fold,auc(fpr, tpr)))\n",
    "print(\"Cross Validation AUC = {}\".format(aucs/10))\n",
    "#print(sk.confusion_matrix(y_val_f,preds_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, accuracy_score ,recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#range of output value\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(range(len(y_pred)),y_pred.reshape(-1,1))\n",
    "plt.ylabel('y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for the optimal probability threshold\n",
    "#prob=[0.5,0.55,0.6,0.65,0.75,0.8,0.85,0.9,0.95,1,2,2.5,5]\n",
    "prob=[-0.2,-0.1,-0.05,0,0.005,0.1,0.157,0.2,0.3,0.5,1,1.5,2]\n",
    "for p in prob:\n",
    "    pred_value =[1 if i<p   else 0 for i in y_pred]\n",
    "    f1=f1_score(y_test,pred_value)\n",
    "    acc=accuracy_score(y_test,pred_value)\n",
    "    precision=average_precision_score(y_test,pred_value)\n",
    "    recall= recall_score(y_test,pred_value)\n",
    "    print(f'prob ={p} and f1 score ={f1} : accuracy={acc} : precision={precision} : recall={recall}')\n",
    "    print(pred_value.count(0),pred_value.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=dict()\n",
    "history['gen']=[]\n",
    "history['dis']=[]\n",
    "def train1(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for batch in dataset:\n",
    "\n",
    "           gen_loss,dis_loss= lstmgan.train(batch)\n",
    "        history['gen'].append(gen_loss)\n",
    "        history['dis'].append(dis_loss)\n",
    "        print ('Time for epoch {} is {} sec '.format(epoch + 1, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = lstmgan.discriminator.evaluate(x_test, y_test)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = lstmgan.generator.predict_classes(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average=\"binary\")\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"binary\")\n",
    "#np.savetxt('results/expected1.txt', y_test, fmt='%01d')\n",
    "#np.savetxt('results/predicted1.txt', y_pred, fmt='%01d')\n",
    "\n",
    "print(\"confusion matrix\")\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"accuracy\")\n",
    "print(\"%.6f\" %accuracy)\n",
    "print(\"racall\")\n",
    "print(\"%.6f\" %recall)\n",
    "print(\"precision\")\n",
    "print(\"%.6f\" %precision)\n",
    "print(\"f1score\")\n",
    "print(\"%.6f\" %f1)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"==============================================\")\n",
    "print (cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
