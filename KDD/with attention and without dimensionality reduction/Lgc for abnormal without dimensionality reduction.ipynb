{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading libraries \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "np.set_printoptions(precision=3)\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "colnames = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "            \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\"num_shells\",\"num_access_files\",\n",
    "            \"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\n",
    "            \"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\n",
    "            \"dst_host_count\",\"dst_host_srv_count\",\"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "            \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\n",
    "            \"dst_host_srv_rerror_rate\",\"result\"]\n",
    "print(len(colnames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "train = pd.read_csv(r\"C:\\Users\\Admin\\Desktop\\feature selection\\datasets\\kddcup 1999\\kddcup_data_corrected.csv\",header=None,names=colnames)\n",
    "#print(type(train))\n",
    "#test = pd.read_csv(r\"C:\\Users\\Admin\\Desktop\\feature selection\\datasets\\kddcup 1999\\corrected.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>root_shell</th>\n",
       "      <th>su_attempted</th>\n",
       "      <th>num_root</th>\n",
       "      <th>num_file_creations</th>\n",
       "      <th>num_shells</th>\n",
       "      <th>num_access_files</th>\n",
       "      <th>num_outbound_cmds</th>\n",
       "      <th>is_host_login</th>\n",
       "      <th>is_guest_login</th>\n",
       "      <th>count</th>\n",
       "      <th>srv_count</th>\n",
       "      <th>serror_rate</th>\n",
       "      <th>srv_serror_rate</th>\n",
       "      <th>rerror_rate</th>\n",
       "      <th>srv_rerror_rate</th>\n",
       "      <th>same_srv_rate</th>\n",
       "      <th>diff_srv_rate</th>\n",
       "      <th>srv_diff_host_rate</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>215</td>\n",
       "      <td>45076</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>162</td>\n",
       "      <td>4528</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>236</td>\n",
       "      <td>1228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>233</td>\n",
       "      <td>2032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>239</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration protocol_type service flag  src_bytes  dst_bytes  land  \\\n",
       "0         0           tcp    http   SF        215      45076     0   \n",
       "1         0           tcp    http   SF        162       4528     0   \n",
       "2         0           tcp    http   SF        236       1228     0   \n",
       "3         0           tcp    http   SF        233       2032     0   \n",
       "4         0           tcp    http   SF        239        486     0   \n",
       "\n",
       "   wrong_fragment  urgent  hot  num_failed_logins  logged_in  num_compromised  \\\n",
       "0               0       0    0                  0          1                0   \n",
       "1               0       0    0                  0          1                0   \n",
       "2               0       0    0                  0          1                0   \n",
       "3               0       0    0                  0          1                0   \n",
       "4               0       0    0                  0          1                0   \n",
       "\n",
       "   root_shell  su_attempted  num_root  num_file_creations  num_shells  \\\n",
       "0           0             0         0                   0           0   \n",
       "1           0             0         0                   0           0   \n",
       "2           0             0         0                   0           0   \n",
       "3           0             0         0                   0           0   \n",
       "4           0             0         0                   0           0   \n",
       "\n",
       "   num_access_files  num_outbound_cmds  is_host_login  is_guest_login  count  \\\n",
       "0                 0                  0              0               0      1   \n",
       "1                 0                  0              0               0      2   \n",
       "2                 0                  0              0               0      1   \n",
       "3                 0                  0              0               0      2   \n",
       "4                 0                  0              0               0      3   \n",
       "\n",
       "   srv_count  serror_rate  srv_serror_rate  rerror_rate  srv_rerror_rate  \\\n",
       "0          1          0.0              0.0          0.0              0.0   \n",
       "1          2          0.0              0.0          0.0              0.0   \n",
       "2          1          0.0              0.0          0.0              0.0   \n",
       "3          2          0.0              0.0          0.0              0.0   \n",
       "4          3          0.0              0.0          0.0              0.0   \n",
       "\n",
       "   same_srv_rate  diff_srv_rate  srv_diff_host_rate  dst_host_count  \\\n",
       "0            1.0            0.0                 0.0               0   \n",
       "1            1.0            0.0                 0.0               1   \n",
       "2            1.0            0.0                 0.0               2   \n",
       "3            1.0            0.0                 0.0               3   \n",
       "4            1.0            0.0                 0.0               4   \n",
       "\n",
       "   dst_host_srv_count  dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                   0                     0.0                     0.0   \n",
       "1                   1                     1.0                     0.0   \n",
       "2                   2                     1.0                     0.0   \n",
       "3                   3                     1.0                     0.0   \n",
       "4                   4                     1.0                     0.0   \n",
       "\n",
       "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                         0.00                          0.0   \n",
       "1                         1.00                          0.0   \n",
       "2                         0.50                          0.0   \n",
       "3                         0.33                          0.0   \n",
       "4                         0.25                          0.0   \n",
       "\n",
       "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                   0.0                       0.0                   0.0   \n",
       "1                   0.0                       0.0                   0.0   \n",
       "2                   0.0                       0.0                   0.0   \n",
       "3                   0.0                       0.0                   0.0   \n",
       "4                   0.0                       0.0                   0.0   \n",
       "\n",
       "   dst_host_srv_rerror_rate   result  \n",
       "0                       0.0  normal.  \n",
       "1                       0.0  normal.  \n",
       "2                       0.0  normal.  \n",
       "3                       0.0  normal.  \n",
       "4                       0.0  normal.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data has 1048576 rows and 42 columns\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "print ('The train data has {0} rows and {1} columns'.format(train.shape[0],train.shape[1]))\n",
    "print ('----------------------------')\n",
    "#print ('The test data has {0} rows and {1} columns'.format(test.shape[0],test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal.             595798\n",
       "smurf.              227524\n",
       "neptune.            204815\n",
       "ipsweep.              7579\n",
       "satan.                5393\n",
       "portsweep.            2782\n",
       "nmap.                 2316\n",
       "back.                 2002\n",
       "teardrop.              199\n",
       "guess_passwd.           53\n",
       "pod.                    40\n",
       "warezmaster.            20\n",
       "land.                   17\n",
       "imap.                   12\n",
       "ftp_write.               8\n",
       "multihop.                6\n",
       "buffer_overflow.         5\n",
       "phf.                     3\n",
       "loadmodule.              2\n",
       "perl.                    2\n",
       "Name: result, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.result.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048576 entries, 0 to 1048575\n",
      "Data columns (total 42 columns):\n",
      " #   Column                       Non-Null Count    Dtype  \n",
      "---  ------                       --------------    -----  \n",
      " 0   duration                     1048576 non-null  int64  \n",
      " 1   protocol_type                1048576 non-null  object \n",
      " 2   service                      1048576 non-null  object \n",
      " 3   flag                         1048576 non-null  object \n",
      " 4   src_bytes                    1048576 non-null  int64  \n",
      " 5   dst_bytes                    1048576 non-null  int64  \n",
      " 6   land                         1048576 non-null  int64  \n",
      " 7   wrong_fragment               1048576 non-null  int64  \n",
      " 8   urgent                       1048576 non-null  int64  \n",
      " 9   hot                          1048576 non-null  int64  \n",
      " 10  num_failed_logins            1048576 non-null  int64  \n",
      " 11  logged_in                    1048576 non-null  int64  \n",
      " 12  num_compromised              1048576 non-null  int64  \n",
      " 13  root_shell                   1048576 non-null  int64  \n",
      " 14  su_attempted                 1048576 non-null  int64  \n",
      " 15  num_root                     1048576 non-null  int64  \n",
      " 16  num_file_creations           1048576 non-null  int64  \n",
      " 17  num_shells                   1048576 non-null  int64  \n",
      " 18  num_access_files             1048576 non-null  int64  \n",
      " 19  num_outbound_cmds            1048576 non-null  int64  \n",
      " 20  is_host_login                1048576 non-null  int64  \n",
      " 21  is_guest_login               1048576 non-null  int64  \n",
      " 22  count                        1048576 non-null  int64  \n",
      " 23  srv_count                    1048576 non-null  int64  \n",
      " 24  serror_rate                  1048576 non-null  float64\n",
      " 25  srv_serror_rate              1048576 non-null  float64\n",
      " 26  rerror_rate                  1048576 non-null  float64\n",
      " 27  srv_rerror_rate              1048576 non-null  float64\n",
      " 28  same_srv_rate                1048576 non-null  float64\n",
      " 29  diff_srv_rate                1048576 non-null  float64\n",
      " 30  srv_diff_host_rate           1048576 non-null  float64\n",
      " 31  dst_host_count               1048576 non-null  int64  \n",
      " 32  dst_host_srv_count           1048576 non-null  int64  \n",
      " 33  dst_host_same_srv_rate       1048576 non-null  float64\n",
      " 34  dst_host_diff_srv_rate       1048576 non-null  float64\n",
      " 35  dst_host_same_src_port_rate  1048576 non-null  float64\n",
      " 36  dst_host_srv_diff_host_rate  1048576 non-null  float64\n",
      " 37  dst_host_serror_rate         1048576 non-null  float64\n",
      " 38  dst_host_srv_serror_rate     1048576 non-null  float64\n",
      " 39  dst_host_rerror_rate         1048576 non-null  float64\n",
      " 40  dst_host_srv_rerror_rate     1048576 non-null  float64\n",
      " 41  result                       1048576 non-null  object \n",
      "dtypes: float64(15), int64(23), object(4)\n",
      "memory usage: 336.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration                       False\n",
      "protocol_type                  False\n",
      "service                        False\n",
      "flag                           False\n",
      "src_bytes                      False\n",
      "dst_bytes                      False\n",
      "land                           False\n",
      "wrong_fragment                 False\n",
      "urgent                         False\n",
      "hot                            False\n",
      "num_failed_logins              False\n",
      "logged_in                      False\n",
      "num_compromised                False\n",
      "root_shell                     False\n",
      "su_attempted                   False\n",
      "num_root                       False\n",
      "num_file_creations             False\n",
      "num_shells                     False\n",
      "num_access_files               False\n",
      "num_outbound_cmds              False\n",
      "is_host_login                  False\n",
      "is_guest_login                 False\n",
      "count                          False\n",
      "srv_count                      False\n",
      "serror_rate                    False\n",
      "srv_serror_rate                False\n",
      "rerror_rate                    False\n",
      "srv_rerror_rate                False\n",
      "same_srv_rate                  False\n",
      "diff_srv_rate                  False\n",
      "srv_diff_host_rate             False\n",
      "dst_host_count                 False\n",
      "dst_host_srv_count             False\n",
      "dst_host_same_srv_rate         False\n",
      "dst_host_diff_srv_rate         False\n",
      "dst_host_same_src_port_rate    False\n",
      "dst_host_srv_diff_host_rate    False\n",
      "dst_host_serror_rate           False\n",
      "dst_host_srv_serror_rate       False\n",
      "dst_host_rerror_rate           False\n",
      "dst_host_srv_rerror_rate       False\n",
      "result                         False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "#check missing values\n",
    "train.columns[train.isnull().any()]\n",
    "#print(len(train))\n",
    "print(train.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration                       0.0\n",
      "protocol_type                  0.0\n",
      "service                        0.0\n",
      "flag                           0.0\n",
      "src_bytes                      0.0\n",
      "dst_bytes                      0.0\n",
      "land                           0.0\n",
      "wrong_fragment                 0.0\n",
      "urgent                         0.0\n",
      "hot                            0.0\n",
      "num_failed_logins              0.0\n",
      "logged_in                      0.0\n",
      "num_compromised                0.0\n",
      "root_shell                     0.0\n",
      "su_attempted                   0.0\n",
      "num_root                       0.0\n",
      "num_file_creations             0.0\n",
      "num_shells                     0.0\n",
      "num_access_files               0.0\n",
      "num_outbound_cmds              0.0\n",
      "is_host_login                  0.0\n",
      "is_guest_login                 0.0\n",
      "count                          0.0\n",
      "srv_count                      0.0\n",
      "serror_rate                    0.0\n",
      "srv_serror_rate                0.0\n",
      "rerror_rate                    0.0\n",
      "srv_rerror_rate                0.0\n",
      "same_srv_rate                  0.0\n",
      "diff_srv_rate                  0.0\n",
      "srv_diff_host_rate             0.0\n",
      "dst_host_count                 0.0\n",
      "dst_host_srv_count             0.0\n",
      "dst_host_same_srv_rate         0.0\n",
      "dst_host_diff_srv_rate         0.0\n",
      "dst_host_same_src_port_rate    0.0\n",
      "dst_host_srv_diff_host_rate    0.0\n",
      "dst_host_serror_rate           0.0\n",
      "dst_host_srv_serror_rate       0.0\n",
      "dst_host_rerror_rate           0.0\n",
      "dst_host_srv_rerror_rate       0.0\n",
      "result                         0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#missing value counts in each of these columns\n",
    "miss = train.isnull().sum()/len(train)\n",
    "if(miss.sum()==0.0):\n",
    "    print(miss)\n",
    "#print(train.isnull().sum())\n",
    "else:\n",
    "    miss = miss[miss > 0]\n",
    "    miss.sort_values(inplace=True)\n",
    "    miss\n",
    "    print('missing values are'.format(miss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no missing values\n",
      "duration         0.0\n",
      "protocol_type    0.0\n",
      "service          0.0\n",
      "flag             0.0\n",
      "src_bytes        0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#visualising missing values\n",
    "#df = pd.DataFrame()\n",
    "#df.to_frame()\n",
    "#miss = pd.DataFrame()\n",
    "#miss.drop_duplicates()\n",
    "if(miss.sum()==0.0):\n",
    "    print(\"There is no missing values\")\n",
    "    print(miss.head())\n",
    "else:    \n",
    "    miss.to_frame()\n",
    "    miss.columns = ['count']\n",
    "    miss.index.names = ['Name']\n",
    "    miss['Name'] = miss.index\n",
    "\n",
    "#plot the missing value count\n",
    "#if(miss.columns!=0):\n",
    "    sns.set(style=\"whitegrid\", color_codes=True)\n",
    "    sns.barplot(x = 'Name', y = ['count'], data=miss)\n",
    "#sns.boxplot, 'species', 'value', x='species', y='value')\n",
    "    plt.xticks(rotation = 90)\n",
    "    sns.plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>root_shell</th>\n",
       "      <th>su_attempted</th>\n",
       "      <th>num_root</th>\n",
       "      <th>num_file_creations</th>\n",
       "      <th>num_shells</th>\n",
       "      <th>num_access_files</th>\n",
       "      <th>num_outbound_cmds</th>\n",
       "      <th>is_host_login</th>\n",
       "      <th>is_guest_login</th>\n",
       "      <th>count</th>\n",
       "      <th>srv_count</th>\n",
       "      <th>serror_rate</th>\n",
       "      <th>srv_serror_rate</th>\n",
       "      <th>rerror_rate</th>\n",
       "      <th>srv_rerror_rate</th>\n",
       "      <th>same_srv_rate</th>\n",
       "      <th>diff_srv_rate</th>\n",
       "      <th>srv_diff_host_rate</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1048576.0</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.234529e+00</td>\n",
       "      <td>1.640730e+03</td>\n",
       "      <td>2.355018e+03</td>\n",
       "      <td>1.621246e-05</td>\n",
       "      <td>6.036758e-04</td>\n",
       "      <td>1.907349e-05</td>\n",
       "      <td>3.145885e-02</td>\n",
       "      <td>1.173019e-04</td>\n",
       "      <td>5.073900e-01</td>\n",
       "      <td>1.439762e-02</td>\n",
       "      <td>2.384186e-04</td>\n",
       "      <td>8.296967e-05</td>\n",
       "      <td>2.419853e-02</td>\n",
       "      <td>2.906799e-03</td>\n",
       "      <td>1.611710e-04</td>\n",
       "      <td>2.755165e-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.536743e-07</td>\n",
       "      <td>2.112389e-03</td>\n",
       "      <td>1.546366e+02</td>\n",
       "      <td>1.203271e+02</td>\n",
       "      <td>1.975607e-01</td>\n",
       "      <td>1.977609e-01</td>\n",
       "      <td>2.475420e-02</td>\n",
       "      <td>2.496099e-02</td>\n",
       "      <td>8.094793e-01</td>\n",
       "      <td>2.117072e-02</td>\n",
       "      <td>8.426697e-02</td>\n",
       "      <td>1.839691e+02</td>\n",
       "      <td>1.864760e+02</td>\n",
       "      <td>7.602534e-01</td>\n",
       "      <td>2.824197e-02</td>\n",
       "      <td>2.744349e-01</td>\n",
       "      <td>1.760540e-02</td>\n",
       "      <td>1.975058e-01</td>\n",
       "      <td>1.972018e-01</td>\n",
       "      <td>2.517304e-02</td>\n",
       "      <td>2.455191e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.285165e+02</td>\n",
       "      <td>6.781544e+05</td>\n",
       "      <td>3.174638e+04</td>\n",
       "      <td>4.026440e-03</td>\n",
       "      <td>4.159995e-02</td>\n",
       "      <td>7.042070e-03</td>\n",
       "      <td>7.063307e-01</td>\n",
       "      <td>1.370621e-02</td>\n",
       "      <td>4.999456e-01</td>\n",
       "      <td>2.439982e+00</td>\n",
       "      <td>1.543898e-02</td>\n",
       "      <td>1.215783e-02</td>\n",
       "      <td>2.738431e+00</td>\n",
       "      <td>1.917875e-01</td>\n",
       "      <td>1.299133e-02</td>\n",
       "      <td>5.706872e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.765625e-04</td>\n",
       "      <td>4.591218e-02</td>\n",
       "      <td>2.033122e+02</td>\n",
       "      <td>2.057385e+02</td>\n",
       "      <td>3.972186e-01</td>\n",
       "      <td>3.974836e-01</td>\n",
       "      <td>1.535523e-01</td>\n",
       "      <td>1.535197e-01</td>\n",
       "      <td>3.767900e-01</td>\n",
       "      <td>9.170193e-02</td>\n",
       "      <td>2.235659e-01</td>\n",
       "      <td>9.889551e+01</td>\n",
       "      <td>1.041205e+02</td>\n",
       "      <td>3.981472e-01</td>\n",
       "      <td>9.997035e-02</td>\n",
       "      <td>4.219412e-01</td>\n",
       "      <td>5.819577e-02</td>\n",
       "      <td>3.969704e-01</td>\n",
       "      <td>3.974174e-01</td>\n",
       "      <td>1.509611e-01</td>\n",
       "      <td>1.512144e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.300000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.100000e+01</td>\n",
       "      <td>7.500000e+01</td>\n",
       "      <td>5.600000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.480000e+02</td>\n",
       "      <td>1.210000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.550000e+02</td>\n",
       "      <td>2.550000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.032000e+03</td>\n",
       "      <td>1.398000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.610000e+02</td>\n",
       "      <td>3.700000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.550000e+02</td>\n",
       "      <td>2.550000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.000000e-02</td>\n",
       "      <td>5.200000e-01</td>\n",
       "      <td>2.000000e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.832900e+04</td>\n",
       "      <td>6.933756e+08</td>\n",
       "      <td>1.173059e+07</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>7.700000e+01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.840000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>9.750000e+02</td>\n",
       "      <td>4.000000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.110000e+02</td>\n",
       "      <td>5.110000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.550000e+02</td>\n",
       "      <td>2.550000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           duration     src_bytes     dst_bytes          land  wrong_fragment  \\\n",
       "count  1.048576e+06  1.048576e+06  1.048576e+06  1.048576e+06    1.048576e+06   \n",
       "mean   4.234529e+00  1.640730e+03  2.355018e+03  1.621246e-05    6.036758e-04   \n",
       "std    2.285165e+02  6.781544e+05  3.174638e+04  4.026440e-03    4.159995e-02   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00    0.000000e+00   \n",
       "25%    0.000000e+00  4.300000e+01  0.000000e+00  0.000000e+00    0.000000e+00   \n",
       "50%    0.000000e+00  2.480000e+02  1.210000e+02  0.000000e+00    0.000000e+00   \n",
       "75%    0.000000e+00  1.032000e+03  1.398000e+03  0.000000e+00    0.000000e+00   \n",
       "max    5.832900e+04  6.933756e+08  1.173059e+07  1.000000e+00    3.000000e+00   \n",
       "\n",
       "             urgent           hot  num_failed_logins     logged_in  \\\n",
       "count  1.048576e+06  1.048576e+06       1.048576e+06  1.048576e+06   \n",
       "mean   1.907349e-05  3.145885e-02       1.173019e-04  5.073900e-01   \n",
       "std    7.042070e-03  7.063307e-01       1.370621e-02  4.999456e-01   \n",
       "min    0.000000e+00  0.000000e+00       0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00       0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00       0.000000e+00  1.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00       0.000000e+00  1.000000e+00   \n",
       "max    5.000000e+00  7.700000e+01       5.000000e+00  1.000000e+00   \n",
       "\n",
       "       num_compromised    root_shell  su_attempted      num_root  \\\n",
       "count     1.048576e+06  1.048576e+06  1.048576e+06  1.048576e+06   \n",
       "mean      1.439762e-02  2.384186e-04  8.296967e-05  2.419853e-02   \n",
       "std       2.439982e+00  1.543898e-02  1.215783e-02  2.738431e+00   \n",
       "min       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max       8.840000e+02  1.000000e+00  2.000000e+00  9.750000e+02   \n",
       "\n",
       "       num_file_creations    num_shells  num_access_files  num_outbound_cmds  \\\n",
       "count        1.048576e+06  1.048576e+06      1.048576e+06          1048576.0   \n",
       "mean         2.906799e-03  1.611710e-04      2.755165e-03                0.0   \n",
       "std          1.917875e-01  1.299133e-02      5.706872e-02                0.0   \n",
       "min          0.000000e+00  0.000000e+00      0.000000e+00                0.0   \n",
       "25%          0.000000e+00  0.000000e+00      0.000000e+00                0.0   \n",
       "50%          0.000000e+00  0.000000e+00      0.000000e+00                0.0   \n",
       "75%          0.000000e+00  0.000000e+00      0.000000e+00                0.0   \n",
       "max          4.000000e+01  2.000000e+00      9.000000e+00                0.0   \n",
       "\n",
       "       is_host_login  is_guest_login         count     srv_count  \\\n",
       "count   1.048576e+06    1.048576e+06  1.048576e+06  1.048576e+06   \n",
       "mean    9.536743e-07    2.112389e-03  1.546366e+02  1.203271e+02   \n",
       "std     9.765625e-04    4.591218e-02  2.033122e+02  2.057385e+02   \n",
       "min     0.000000e+00    0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%     0.000000e+00    0.000000e+00  4.000000e+00  4.000000e+00   \n",
       "50%     0.000000e+00    0.000000e+00  1.800000e+01  1.200000e+01   \n",
       "75%     0.000000e+00    0.000000e+00  2.610000e+02  3.700000e+01   \n",
       "max     1.000000e+00    1.000000e+00  5.110000e+02  5.110000e+02   \n",
       "\n",
       "        serror_rate  srv_serror_rate   rerror_rate  srv_rerror_rate  \\\n",
       "count  1.048576e+06     1.048576e+06  1.048576e+06     1.048576e+06   \n",
       "mean   1.975607e-01     1.977609e-01  2.475420e-02     2.496099e-02   \n",
       "std    3.972186e-01     3.974836e-01  1.535523e-01     1.535197e-01   \n",
       "min    0.000000e+00     0.000000e+00  0.000000e+00     0.000000e+00   \n",
       "25%    0.000000e+00     0.000000e+00  0.000000e+00     0.000000e+00   \n",
       "50%    0.000000e+00     0.000000e+00  0.000000e+00     0.000000e+00   \n",
       "75%    0.000000e+00     0.000000e+00  0.000000e+00     0.000000e+00   \n",
       "max    1.000000e+00     1.000000e+00  1.000000e+00     1.000000e+00   \n",
       "\n",
       "       same_srv_rate  diff_srv_rate  srv_diff_host_rate  dst_host_count  \\\n",
       "count   1.048576e+06   1.048576e+06        1.048576e+06    1.048576e+06   \n",
       "mean    8.094793e-01   2.117072e-02        8.426697e-02    1.839691e+02   \n",
       "std     3.767900e-01   9.170193e-02        2.235659e-01    9.889551e+01   \n",
       "min     0.000000e+00   0.000000e+00        0.000000e+00    0.000000e+00   \n",
       "25%     1.000000e+00   0.000000e+00        0.000000e+00    8.100000e+01   \n",
       "50%     1.000000e+00   0.000000e+00        0.000000e+00    2.550000e+02   \n",
       "75%     1.000000e+00   0.000000e+00        0.000000e+00    2.550000e+02   \n",
       "max     1.000000e+00   1.000000e+00        1.000000e+00    2.550000e+02   \n",
       "\n",
       "       dst_host_srv_count  dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "count        1.048576e+06            1.048576e+06            1.048576e+06   \n",
       "mean         1.864760e+02            7.602534e-01            2.824197e-02   \n",
       "std          1.041205e+02            3.981472e-01            9.997035e-02   \n",
       "min          0.000000e+00            0.000000e+00            0.000000e+00   \n",
       "25%          7.500000e+01            5.600000e-01            0.000000e+00   \n",
       "50%          2.550000e+02            1.000000e+00            0.000000e+00   \n",
       "75%          2.550000e+02            1.000000e+00            4.000000e-02   \n",
       "max          2.550000e+02            1.000000e+00            1.000000e+00   \n",
       "\n",
       "       dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "count                 1.048576e+06                 1.048576e+06   \n",
       "mean                  2.744349e-01                 1.760540e-02   \n",
       "std                   4.219412e-01                 5.819577e-02   \n",
       "min                   0.000000e+00                 0.000000e+00   \n",
       "25%                   0.000000e+00                 0.000000e+00   \n",
       "50%                   1.000000e-02                 0.000000e+00   \n",
       "75%                   5.200000e-01                 2.000000e-02   \n",
       "max                   1.000000e+00                 1.000000e+00   \n",
       "\n",
       "       dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "count          1.048576e+06              1.048576e+06          1.048576e+06   \n",
       "mean           1.975058e-01              1.972018e-01          2.517304e-02   \n",
       "std            3.969704e-01              3.974174e-01          1.509611e-01   \n",
       "min            0.000000e+00              0.000000e+00          0.000000e+00   \n",
       "25%            0.000000e+00              0.000000e+00          0.000000e+00   \n",
       "50%            0.000000e+00              0.000000e+00          0.000000e+00   \n",
       "75%            0.000000e+00              0.000000e+00          0.000000e+00   \n",
       "max            1.000000e+00              1.000000e+00          1.000000e+00   \n",
       "\n",
       "       dst_host_srv_rerror_rate  \n",
       "count              1.048576e+06  \n",
       "mean               2.455191e-02  \n",
       "std                1.512144e-01  \n",
       "min                0.000000e+00  \n",
       "25%                0.000000e+00  \n",
       "50%                0.000000e+00  \n",
       "75%                0.000000e+00  \n",
       "max                1.000000e+00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptive statistics\n",
    "train.describe()\n",
    "#If there is any redundant column, remove it from both train & test datasets\n",
    "#Since there is no zeros for all statistical values,it doesn't contain redundant column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will be extracting all the features as a \"priori\" for preprocessing\n",
    "features = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "            \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\"num_shells\",\"num_access_files\",\n",
    "            \"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\n",
    "            \"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\n",
    "            \"dst_host_count\",\"dst_host_srv_count\",\"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "            \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\n",
    "            \"dst_host_srv_rerror_rate\"]\n",
    "target = \"result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048576, 42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = train.loc[:,features]\n",
    "y1 = train.loc[:,target]\n",
    "train.shape\n",
    "#test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['back.' 'buffer_overflow.' 'ftp_write.' 'guess_passwd.' 'imap.'\n",
      " 'ipsweep.' 'land.' 'loadmodule.' 'multihop.' 'neptune.' 'nmap.' 'normal.'\n",
      " 'perl.' 'phf.' 'pod.' 'portsweep.' 'satan.' 'smurf.' 'teardrop.'\n",
      " 'warezmaster.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "normal.             595798\n",
       "smurf.              227524\n",
       "neptune.            204815\n",
       "ipsweep.              7579\n",
       "satan.                5393\n",
       "portsweep.            2782\n",
       "nmap.                 2316\n",
       "back.                 2002\n",
       "teardrop.              199\n",
       "guess_passwd.           53\n",
       "pod.                    40\n",
       "warezmaster.            20\n",
       "land.                   17\n",
       "imap.                   12\n",
       "ftp_write.               8\n",
       "multihop.                6\n",
       "buffer_overflow.         5\n",
       "phf.                     3\n",
       "loadmodule.              2\n",
       "perl.                    2\n",
       "Name: result, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = np.unique(y1)\n",
    "print(classes)\n",
    "# Attack Class Distribution\n",
    "train['result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>root_shell</th>\n",
       "      <th>su_attempted</th>\n",
       "      <th>num_root</th>\n",
       "      <th>num_file_creations</th>\n",
       "      <th>num_shells</th>\n",
       "      <th>num_access_files</th>\n",
       "      <th>num_outbound_cmds</th>\n",
       "      <th>is_host_login</th>\n",
       "      <th>is_guest_login</th>\n",
       "      <th>count</th>\n",
       "      <th>srv_count</th>\n",
       "      <th>serror_rate</th>\n",
       "      <th>srv_serror_rate</th>\n",
       "      <th>rerror_rate</th>\n",
       "      <th>srv_rerror_rate</th>\n",
       "      <th>same_srv_rate</th>\n",
       "      <th>diff_srv_rate</th>\n",
       "      <th>srv_diff_host_rate</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>215</td>\n",
       "      <td>45076</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>162</td>\n",
       "      <td>4528</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>236</td>\n",
       "      <td>1228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>233</td>\n",
       "      <td>2032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>239</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration protocol_type service flag  src_bytes  dst_bytes  land  \\\n",
       "0         0           tcp    http   SF        215      45076     0   \n",
       "1         0           tcp    http   SF        162       4528     0   \n",
       "2         0           tcp    http   SF        236       1228     0   \n",
       "3         0           tcp    http   SF        233       2032     0   \n",
       "4         0           tcp    http   SF        239        486     0   \n",
       "\n",
       "   wrong_fragment  urgent  hot  num_failed_logins  logged_in  num_compromised  \\\n",
       "0               0       0    0                  0          1                0   \n",
       "1               0       0    0                  0          1                0   \n",
       "2               0       0    0                  0          1                0   \n",
       "3               0       0    0                  0          1                0   \n",
       "4               0       0    0                  0          1                0   \n",
       "\n",
       "   root_shell  su_attempted  num_root  num_file_creations  num_shells  \\\n",
       "0           0             0         0                   0           0   \n",
       "1           0             0         0                   0           0   \n",
       "2           0             0         0                   0           0   \n",
       "3           0             0         0                   0           0   \n",
       "4           0             0         0                   0           0   \n",
       "\n",
       "   num_access_files  num_outbound_cmds  is_host_login  is_guest_login  count  \\\n",
       "0                 0                  0              0               0      1   \n",
       "1                 0                  0              0               0      2   \n",
       "2                 0                  0              0               0      1   \n",
       "3                 0                  0              0               0      2   \n",
       "4                 0                  0              0               0      3   \n",
       "\n",
       "   srv_count  serror_rate  srv_serror_rate  rerror_rate  srv_rerror_rate  \\\n",
       "0          1          0.0              0.0          0.0              0.0   \n",
       "1          2          0.0              0.0          0.0              0.0   \n",
       "2          1          0.0              0.0          0.0              0.0   \n",
       "3          2          0.0              0.0          0.0              0.0   \n",
       "4          3          0.0              0.0          0.0              0.0   \n",
       "\n",
       "   same_srv_rate  diff_srv_rate  srv_diff_host_rate  dst_host_count  \\\n",
       "0            1.0            0.0                 0.0               0   \n",
       "1            1.0            0.0                 0.0               1   \n",
       "2            1.0            0.0                 0.0               2   \n",
       "3            1.0            0.0                 0.0               3   \n",
       "4            1.0            0.0                 0.0               4   \n",
       "\n",
       "   dst_host_srv_count  dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                   0                     0.0                     0.0   \n",
       "1                   1                     1.0                     0.0   \n",
       "2                   2                     1.0                     0.0   \n",
       "3                   3                     1.0                     0.0   \n",
       "4                   4                     1.0                     0.0   \n",
       "\n",
       "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                         0.00                          0.0   \n",
       "1                         1.00                          0.0   \n",
       "2                         0.50                          0.0   \n",
       "3                         0.33                          0.0   \n",
       "4                         0.25                          0.0   \n",
       "\n",
       "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                   0.0                       0.0                   0.0   \n",
       "1                   0.0                       0.0                   0.0   \n",
       "2                   0.0                       0.0                   0.0   \n",
       "3                   0.0                       0.0                   0.0   \n",
       "4                   0.0                       0.0                   0.0   \n",
       "\n",
       "   dst_host_srv_rerror_rate  result  \n",
       "0                       0.0  normal  \n",
       "1                       0.0  normal  \n",
       "2                       0.0  normal  \n",
       "3                       0.0  normal  \n",
       "4                       0.0  normal  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replacing all classes of attack with 1 and normal result with 0 in our icmp_df\n",
    "for i in range(len(classes)):\n",
    "    if classes[i] == \"normal.\":\n",
    "        train = train.replace(classes[i], 0)\n",
    "  #      data[\"Team\"]= data[\"Team\"].str.replace(\"boston\", \"New Boston\", case = False) \n",
    "    else:\n",
    "        train = train.replace(classes[i], 1)\n",
    "\n",
    "#train\n",
    "\n",
    "train[\"result\"] = train[\"result\"].replace(0, \"normal\")\n",
    "train[\"result\"] = train[\"result\"].replace(1, \"abnormal\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal      595798\n",
       "abnormal    452778\n",
       "Name: result, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attack Class Distribution\n",
    "#print('0-normal \\t 1-abnorml')\n",
    "train['result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 for normal data and 1 for abnormalities\n",
    "train.result=train.result.apply(lambda x: 0 if x == 'normal' else 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#randomly sample 500 data point for training\n",
    "train=train[train.result==1].sample(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Helper function for scaling continous values\n",
    "def minmax_scale_values(train, col_name):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler = scaler.fit(train[col_name].values.reshape(-1, 1))\n",
    "    train_values_standardized = scaler.transform(train[col_name].values.reshape(-1, 1))\n",
    "    train[col_name] = train_values_standardized\n",
    "    #test_values_standardized = scaler.transform(test[col_name].values.reshape(-1, 1))\n",
    "    #test[col_name] = test_values_standardized\n",
    "    \n",
    "    \n",
    "#Helper function for one hot encoding\n",
    "def encode_text(train, name):\n",
    "    training_set_dummies = pd.get_dummies(train[name])\n",
    "    #testing_set_dummies = pd.get_dummies(test[name])\n",
    "    for x in training_set_dummies.columns:\n",
    "        dummy_name = \"{}_{}\".format(name, x)\n",
    "        train[dummy_name] = training_set_dummies[x]\n",
    "        #if x in testing_set_dummies.columns :\n",
    "        #    test[dummy_name]=testing_set_dummies[x]\n",
    "        #else :\n",
    "        #    test[dummy_name]=np.zeros(len(test))\n",
    "    train.drop(name, axis=1, inplace=True)\n",
    "    #test.drop(name, axis=1, inplace=True)\n",
    "             \n",
    "sympolic_columns=[\"protocol_type\",\"service\",\"flag\"]\n",
    "label_column=\"Class\"\n",
    "for column in train.columns :\n",
    "    if column in sympolic_columns:\n",
    "        encode_text(train,column)\n",
    "    elif not column == label_column:\n",
    "        minmax_scale_values(train, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly sample 500 data point for training\n",
    "train=train[train.result==1].sample(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>root_shell</th>\n",
       "      <th>su_attempted</th>\n",
       "      <th>num_root</th>\n",
       "      <th>num_file_creations</th>\n",
       "      <th>num_shells</th>\n",
       "      <th>num_access_files</th>\n",
       "      <th>num_outbound_cmds</th>\n",
       "      <th>is_host_login</th>\n",
       "      <th>is_guest_login</th>\n",
       "      <th>count</th>\n",
       "      <th>srv_count</th>\n",
       "      <th>serror_rate</th>\n",
       "      <th>srv_serror_rate</th>\n",
       "      <th>rerror_rate</th>\n",
       "      <th>srv_rerror_rate</th>\n",
       "      <th>same_srv_rate</th>\n",
       "      <th>diff_srv_rate</th>\n",
       "      <th>srv_diff_host_rate</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>result</th>\n",
       "      <th>protocol_type_icmp</th>\n",
       "      <th>protocol_type_tcp</th>\n",
       "      <th>protocol_type_udp</th>\n",
       "      <th>service_IRC</th>\n",
       "      <th>service_X11</th>\n",
       "      <th>service_Z39_50</th>\n",
       "      <th>service_aol</th>\n",
       "      <th>service_auth</th>\n",
       "      <th>service_bgp</th>\n",
       "      <th>service_courier</th>\n",
       "      <th>service_csnet_ns</th>\n",
       "      <th>service_ctf</th>\n",
       "      <th>service_daytime</th>\n",
       "      <th>service_discard</th>\n",
       "      <th>service_domain</th>\n",
       "      <th>service_domain_u</th>\n",
       "      <th>service_echo</th>\n",
       "      <th>service_eco_i</th>\n",
       "      <th>service_ecr_i</th>\n",
       "      <th>service_efs</th>\n",
       "      <th>service_exec</th>\n",
       "      <th>service_finger</th>\n",
       "      <th>service_ftp</th>\n",
       "      <th>service_ftp_data</th>\n",
       "      <th>service_gopher</th>\n",
       "      <th>service_harvest</th>\n",
       "      <th>service_hostnames</th>\n",
       "      <th>service_http</th>\n",
       "      <th>service_http_2784</th>\n",
       "      <th>service_http_443</th>\n",
       "      <th>service_imap4</th>\n",
       "      <th>service_iso_tsap</th>\n",
       "      <th>service_klogin</th>\n",
       "      <th>service_kshell</th>\n",
       "      <th>service_ldap</th>\n",
       "      <th>service_link</th>\n",
       "      <th>service_login</th>\n",
       "      <th>service_mtp</th>\n",
       "      <th>service_name</th>\n",
       "      <th>service_netbios_dgm</th>\n",
       "      <th>service_netbios_ns</th>\n",
       "      <th>service_netbios_ssn</th>\n",
       "      <th>service_netstat</th>\n",
       "      <th>service_nnsp</th>\n",
       "      <th>service_nntp</th>\n",
       "      <th>service_ntp_u</th>\n",
       "      <th>service_other</th>\n",
       "      <th>service_pm_dump</th>\n",
       "      <th>service_pop_2</th>\n",
       "      <th>service_pop_3</th>\n",
       "      <th>service_printer</th>\n",
       "      <th>service_private</th>\n",
       "      <th>service_remote_job</th>\n",
       "      <th>service_rje</th>\n",
       "      <th>service_shell</th>\n",
       "      <th>service_smtp</th>\n",
       "      <th>service_sql_net</th>\n",
       "      <th>service_ssh</th>\n",
       "      <th>service_sunrpc</th>\n",
       "      <th>service_supdup</th>\n",
       "      <th>service_systat</th>\n",
       "      <th>service_telnet</th>\n",
       "      <th>service_time</th>\n",
       "      <th>service_urh_i</th>\n",
       "      <th>service_urp_i</th>\n",
       "      <th>service_uucp</th>\n",
       "      <th>service_uucp_path</th>\n",
       "      <th>service_vmnet</th>\n",
       "      <th>service_whois</th>\n",
       "      <th>flag_OTH</th>\n",
       "      <th>flag_REJ</th>\n",
       "      <th>flag_RSTO</th>\n",
       "      <th>flag_RSTOS0</th>\n",
       "      <th>flag_RSTR</th>\n",
       "      <th>flag_S0</th>\n",
       "      <th>flag_S1</th>\n",
       "      <th>flag_S2</th>\n",
       "      <th>flag_S3</th>\n",
       "      <th>flag_SF</th>\n",
       "      <th>flag_SH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>539463</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272016</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617692</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573386</td>\n",
       "      <td>0.035225</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977073</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445353</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957178</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        duration  src_bytes  dst_bytes  land  wrong_fragment  urgent  hot  \\\n",
       "539463       0.0   0.000000        0.0   0.0             0.0     0.0  0.0   \n",
       "617692       0.0   0.000000        0.0   0.0             0.0     0.0  0.0   \n",
       "977073       0.0   0.000001        0.0   0.0             0.0     0.0  0.0   \n",
       "445353       0.0   0.000001        0.0   0.0             0.0     0.0  0.0   \n",
       "957178       0.0   0.000001        0.0   0.0             0.0     0.0  0.0   \n",
       "\n",
       "        num_failed_logins  logged_in  num_compromised  root_shell  \\\n",
       "539463                0.0        0.0              0.0         0.0   \n",
       "617692                0.0        0.0              0.0         0.0   \n",
       "977073                0.0        0.0              0.0         0.0   \n",
       "445353                0.0        0.0              0.0         0.0   \n",
       "957178                0.0        0.0              0.0         0.0   \n",
       "\n",
       "        su_attempted  num_root  num_file_creations  num_shells  \\\n",
       "539463           0.0       0.0                 0.0         0.0   \n",
       "617692           0.0       0.0                 0.0         0.0   \n",
       "977073           0.0       0.0                 0.0         0.0   \n",
       "445353           0.0       0.0                 0.0         0.0   \n",
       "957178           0.0       0.0                 0.0         0.0   \n",
       "\n",
       "        num_access_files  num_outbound_cmds  is_host_login  is_guest_login  \\\n",
       "539463               0.0                0.0            0.0             0.0   \n",
       "617692               0.0                0.0            0.0             0.0   \n",
       "977073               0.0                0.0            0.0             0.0   \n",
       "445353               0.0                0.0            0.0             0.0   \n",
       "957178               0.0                0.0            0.0             0.0   \n",
       "\n",
       "           count  srv_count  serror_rate  srv_serror_rate  rerror_rate  \\\n",
       "539463  0.272016   0.027397          1.0              1.0          0.0   \n",
       "617692  0.573386   0.035225          1.0              1.0          0.0   \n",
       "977073  1.000000   1.000000          0.0              0.0          0.0   \n",
       "445353  1.000000   1.000000          0.0              0.0          0.0   \n",
       "957178  1.000000   1.000000          0.0              0.0          0.0   \n",
       "\n",
       "        srv_rerror_rate  same_srv_rate  diff_srv_rate  srv_diff_host_rate  \\\n",
       "539463              0.0           0.10           0.06                 0.0   \n",
       "617692              0.0           0.06           0.06                 0.0   \n",
       "977073              0.0           1.00           0.00                 0.0   \n",
       "445353              0.0           1.00           0.00                 0.0   \n",
       "957178              0.0           1.00           0.00                 0.0   \n",
       "\n",
       "        dst_host_count  dst_host_srv_count  dst_host_same_srv_rate  \\\n",
       "539463             1.0            0.054902                    0.05   \n",
       "617692             1.0            0.070588                    0.07   \n",
       "977073             1.0            1.000000                    1.00   \n",
       "445353             1.0            1.000000                    1.00   \n",
       "957178             1.0            1.000000                    1.00   \n",
       "\n",
       "        dst_host_diff_srv_rate  dst_host_same_src_port_rate  \\\n",
       "539463                    0.06                          0.0   \n",
       "617692                    0.07                          0.0   \n",
       "977073                    0.00                          1.0   \n",
       "445353                    0.00                          1.0   \n",
       "957178                    0.00                          1.0   \n",
       "\n",
       "        dst_host_srv_diff_host_rate  dst_host_serror_rate  \\\n",
       "539463                          0.0                   1.0   \n",
       "617692                          0.0                   1.0   \n",
       "977073                          0.0                   0.0   \n",
       "445353                          0.0                   0.0   \n",
       "957178                          0.0                   0.0   \n",
       "\n",
       "        dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "539463                       1.0                   0.0   \n",
       "617692                       1.0                   0.0   \n",
       "977073                       0.0                   0.0   \n",
       "445353                       0.0                   0.0   \n",
       "957178                       0.0                   0.0   \n",
       "\n",
       "        dst_host_srv_rerror_rate  result  protocol_type_icmp  \\\n",
       "539463                       0.0     1.0                   0   \n",
       "617692                       0.0     1.0                   0   \n",
       "977073                       0.0     1.0                   1   \n",
       "445353                       0.0     1.0                   1   \n",
       "957178                       0.0     1.0                   1   \n",
       "\n",
       "        protocol_type_tcp  protocol_type_udp  service_IRC  service_X11  \\\n",
       "539463                  1                  0            0            0   \n",
       "617692                  1                  0            0            0   \n",
       "977073                  0                  0            0            0   \n",
       "445353                  0                  0            0            0   \n",
       "957178                  0                  0            0            0   \n",
       "\n",
       "        service_Z39_50  service_aol  service_auth  service_bgp  \\\n",
       "539463               0            0             0            0   \n",
       "617692               0            0             0            0   \n",
       "977073               0            0             0            0   \n",
       "445353               0            0             0            0   \n",
       "957178               0            0             0            0   \n",
       "\n",
       "        service_courier  service_csnet_ns  service_ctf  service_daytime  \\\n",
       "539463                0                 0            0                0   \n",
       "617692                0                 0            0                0   \n",
       "977073                0                 0            0                0   \n",
       "445353                0                 0            0                0   \n",
       "957178                0                 0            0                0   \n",
       "\n",
       "        service_discard  service_domain  service_domain_u  service_echo  \\\n",
       "539463                0               0                 0             0   \n",
       "617692                0               0                 0             0   \n",
       "977073                0               0                 0             0   \n",
       "445353                0               0                 0             0   \n",
       "957178                0               0                 0             0   \n",
       "\n",
       "        service_eco_i  service_ecr_i  service_efs  service_exec  \\\n",
       "539463              0              0            0             0   \n",
       "617692              0              0            0             0   \n",
       "977073              0              1            0             0   \n",
       "445353              0              1            0             0   \n",
       "957178              0              1            0             0   \n",
       "\n",
       "        service_finger  service_ftp  service_ftp_data  service_gopher  \\\n",
       "539463               0            0                 0               0   \n",
       "617692               0            0                 0               0   \n",
       "977073               0            0                 0               0   \n",
       "445353               0            0                 0               0   \n",
       "957178               0            0                 0               0   \n",
       "\n",
       "        service_harvest  service_hostnames  service_http  service_http_2784  \\\n",
       "539463                0                  0             0                  0   \n",
       "617692                0                  0             0                  0   \n",
       "977073                0                  0             0                  0   \n",
       "445353                0                  0             0                  0   \n",
       "957178                0                  0             0                  0   \n",
       "\n",
       "        service_http_443  service_imap4  service_iso_tsap  service_klogin  \\\n",
       "539463                 0              0                 0               0   \n",
       "617692                 0              0                 0               0   \n",
       "977073                 0              0                 0               0   \n",
       "445353                 0              0                 0               0   \n",
       "957178                 0              0                 0               0   \n",
       "\n",
       "        service_kshell  service_ldap  service_link  service_login  \\\n",
       "539463               0             0             0              0   \n",
       "617692               0             0             0              0   \n",
       "977073               0             0             0              0   \n",
       "445353               0             0             0              0   \n",
       "957178               0             0             0              0   \n",
       "\n",
       "        service_mtp  service_name  service_netbios_dgm  service_netbios_ns  \\\n",
       "539463            0             0                    0                   0   \n",
       "617692            0             0                    0                   0   \n",
       "977073            0             0                    0                   0   \n",
       "445353            0             0                    0                   0   \n",
       "957178            0             0                    0                   0   \n",
       "\n",
       "        service_netbios_ssn  service_netstat  service_nnsp  service_nntp  \\\n",
       "539463                    0                0             0             0   \n",
       "617692                    0                0             0             0   \n",
       "977073                    0                0             0             0   \n",
       "445353                    0                0             0             0   \n",
       "957178                    0                0             0             0   \n",
       "\n",
       "        service_ntp_u  service_other  service_pm_dump  service_pop_2  \\\n",
       "539463              0              0                0              0   \n",
       "617692              0              0                0              0   \n",
       "977073              0              0                0              0   \n",
       "445353              0              0                0              0   \n",
       "957178              0              0                0              0   \n",
       "\n",
       "        service_pop_3  service_printer  service_private  service_remote_job  \\\n",
       "539463              0                0                1                   0   \n",
       "617692              0                0                1                   0   \n",
       "977073              0                0                0                   0   \n",
       "445353              0                0                0                   0   \n",
       "957178              0                0                0                   0   \n",
       "\n",
       "        service_rje  service_shell  service_smtp  service_sql_net  \\\n",
       "539463            0              0             0                0   \n",
       "617692            0              0             0                0   \n",
       "977073            0              0             0                0   \n",
       "445353            0              0             0                0   \n",
       "957178            0              0             0                0   \n",
       "\n",
       "        service_ssh  service_sunrpc  service_supdup  service_systat  \\\n",
       "539463            0               0               0               0   \n",
       "617692            0               0               0               0   \n",
       "977073            0               0               0               0   \n",
       "445353            0               0               0               0   \n",
       "957178            0               0               0               0   \n",
       "\n",
       "        service_telnet  service_time  service_urh_i  service_urp_i  \\\n",
       "539463               0             0              0              0   \n",
       "617692               0             0              0              0   \n",
       "977073               0             0              0              0   \n",
       "445353               0             0              0              0   \n",
       "957178               0             0              0              0   \n",
       "\n",
       "        service_uucp  service_uucp_path  service_vmnet  service_whois  \\\n",
       "539463             0                  0              0              0   \n",
       "617692             0                  0              0              0   \n",
       "977073             0                  0              0              0   \n",
       "445353             0                  0              0              0   \n",
       "957178             0                  0              0              0   \n",
       "\n",
       "        flag_OTH  flag_REJ  flag_RSTO  flag_RSTOS0  flag_RSTR  flag_S0  \\\n",
       "539463         0         0          0            0          0        1   \n",
       "617692         0         0          0            0          0        1   \n",
       "977073         0         0          0            0          0        0   \n",
       "445353         0         0          0            0          0        0   \n",
       "957178         0         0          0            0          0        0   \n",
       "\n",
       "        flag_S1  flag_S2  flag_S3  flag_SF  flag_SH  \n",
       "539463        0        0        0        0        0  \n",
       "617692        0        0        0        0        0  \n",
       "977073        0        0        0        1        0  \n",
       "445353        0        0        0        1        0  \n",
       "957178        0        0        0        1        0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 119)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly sample 500 data point for training\n",
    "df_train=train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the item sampled from our dataset\n",
    "index_list=df_train.index\n",
    "train=train.drop(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the label columns\n",
    "df_label=df_train.result\n",
    "df_train=df_train.drop('result',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label\n",
    "df_label.to_csv('abnormallabel1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 118)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.values.reshape((df_train.shape[0], 1, df_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1, 118)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "timesteps =  df_train.shape[1] # equal to the lookback\n",
    "n_features =  df_train.shape[2] # 59\n",
    "\n",
    "epochs = 15\n",
    "batch = 64\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=dict()\n",
    "history['gen']=[]\n",
    "history['dis']=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error\n",
    "                             ,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Bidirectional, LSTM, Reshape, RepeatVector, TimeDistributed\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "#https://www.kaggle.com/function9/bidirectional-lstm-gan-music-generation\n",
    "#https://www.kaggle.com/abhisheksinha28/bidirectional-lstm/data\n",
    "#https://www.kaggle.com/sekfook97/gan-for-anomaly-detection\n",
    "# importing libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Bidirectional, LSTM, Reshape, RepeatVector, TimeDistributed\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 1\n",
    "        self.img_cols = df_train.shape[2]\n",
    "        self.img_shape = (self.img_rows, self.img_cols)\n",
    "        self.latent_dim = df_train.shape[2]\n",
    "        r=df_train.shape[1]\n",
    "\n",
    "        optimizer = Adam(0.0001, 0.4)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates song\n",
    "        z = Input(shape=(1,df_train.shape[2]))\n",
    "    \n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        r=df_train.shape[2]\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(1, df_train.shape[2])))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #specifying output to have 40 timesteps\n",
    "        model.add(RepeatVector(r))\n",
    "        #specifying 1 feature as the output\n",
    "        \n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.3))   \n",
    "        model.add(TimeDistributed(Dense(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(1,df_train.shape[2]))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "        #return Model(img)\n",
    "    \n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(64, use_bias=False, input_shape=(df_train.shape[2], 1)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Convolution1D(32, 3, strides=11, padding='same', use_bias=False))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Convolution1D(1, 3, strides=11, padding='same', use_bias=False))\n",
    "        model.add(Flatten())        \n",
    "        model.add(RepeatVector(1))        \n",
    "        model.add(TimeDistributed(Dense(32, activation = 'relu')))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(1, activation = 'relu')))\n",
    "        #model.add(TimeDistributed(Dense(1)))\n",
    "        #model.add(Dense(1, activation='tanh'))\n",
    "        #model.add(Dense(1))\n",
    "        #model.add(Activation('sigmoid'))\n",
    "        model.summary()\n",
    "        #model.add(layers.BatchNormalization())\n",
    "        #model.add(sigmoid())\n",
    "       \n",
    "        img = Input(shape=(df_train.shape[2],1))\n",
    "        validity = model(img)\n",
    "        print(\"exit\")\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs=15, batch_size=64, save_interval=50):\n",
    "        print(\"enter\")  \n",
    "        # Load the dataset\n",
    "        X_train = df_train\n",
    "       \n",
    "        # Rescale 0 to 1\n",
    "      #  X_train = X_train / 128\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size,1,1))\n",
    "        fake = np.zeros((batch_size,1,1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of songs\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            imgs = np.array(imgs)\n",
    "            imgs = imgs.reshape(len(imgs),df_train.shape[2],1)\n",
    "\n",
    "            # Sample noise and generate a batch of new songs\n",
    "            noise = np.random.normal(0, 1, (batch_size,1,df_train.shape[2]))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            X_train = df_train\n",
    "            valid = np.ones((batch_size,1,1))\n",
    "            fake = np.zeros((batch_size,1,1))\n",
    "            \n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake songs as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "            history['gen'].append(g_loss)\n",
    "            \n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            history['dis'].append(d_loss) \n",
    "            #return (g_loss,d_loss)\n",
    "            # If at save interval => save model\n",
    "#            if epoch % save_interval == 0:\n",
    " #               self.generator.save(\"LSTM_generator.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 118, 64)           64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 118, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 11, 32)            6144      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 11, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 11, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 1)              96        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 1, 1)              0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 32)             64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 1, 1)              33        \n",
      "=================================================================\n",
      "Total params: 6,401\n",
      "Trainable params: 6,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "exit\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 1, 256)            252928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 1, 256)            394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 118, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 118, 256)          394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 118, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 118, 256)          394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 118, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 118, 256)          394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 118, 256)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 118, 256)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 118, 128)          32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 118, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 118, 128)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 118, 128)          16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 118, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 118, 128)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 118, 1)            129       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 118, 1)            0         \n",
      "=================================================================\n",
      "Total params: 2,273,665\n",
      "Trainable params: 2,273,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "enter\n",
      "0 [D loss: 5.602957, acc.: 50.00%] [G loss: 12.211890]\n",
      "1 [D loss: 5.140296, acc.: 50.00%] [G loss: 7.339921]\n",
      "2 [D loss: 4.342687, acc.: 50.00%] [G loss: 7.017649]\n",
      "3 [D loss: 3.721411, acc.: 50.00%] [G loss: 6.748005]\n",
      "4 [D loss: 3.625534, acc.: 50.00%] [G loss: 6.529817]\n",
      "5 [D loss: 3.084248, acc.: 50.00%] [G loss: 6.252421]\n",
      "6 [D loss: 3.992304, acc.: 50.00%] [G loss: 5.938709]\n",
      "7 [D loss: 2.906528, acc.: 50.00%] [G loss: 5.840056]\n",
      "8 [D loss: 2.763152, acc.: 50.00%] [G loss: 5.554802]\n",
      "9 [D loss: 2.885808, acc.: 50.00%] [G loss: 5.602829]\n",
      "10 [D loss: 4.440485, acc.: 50.00%] [G loss: 5.432782]\n",
      "11 [D loss: 3.266516, acc.: 50.00%] [G loss: 5.382812]\n",
      "12 [D loss: 3.069582, acc.: 50.00%] [G loss: 5.279189]\n",
      "13 [D loss: 3.331999, acc.: 50.00%] [G loss: 5.166945]\n",
      "14 [D loss: 2.801303, acc.: 50.00%] [G loss: 5.169080]\n",
      "15 [D loss: 3.634427, acc.: 50.00%] [G loss: 5.118078]\n",
      "16 [D loss: 3.546631, acc.: 50.00%] [G loss: 5.092231]\n",
      "17 [D loss: 2.857249, acc.: 50.00%] [G loss: 5.035852]\n",
      "18 [D loss: 2.766629, acc.: 50.00%] [G loss: 5.029752]\n",
      "19 [D loss: 3.240142, acc.: 50.00%] [G loss: 4.961864]\n",
      "20 [D loss: 2.774019, acc.: 50.00%] [G loss: 4.888559]\n",
      "21 [D loss: 2.385378, acc.: 50.00%] [G loss: 5.072756]\n",
      "22 [D loss: 2.743231, acc.: 50.00%] [G loss: 4.903086]\n",
      "23 [D loss: 2.388802, acc.: 50.00%] [G loss: 4.845304]\n",
      "24 [D loss: 3.242301, acc.: 50.00%] [G loss: 4.792325]\n",
      "25 [D loss: 2.458334, acc.: 50.00%] [G loss: 4.722900]\n",
      "26 [D loss: 2.621827, acc.: 50.00%] [G loss: 4.705795]\n",
      "27 [D loss: 3.241979, acc.: 50.00%] [G loss: 4.716293]\n",
      "28 [D loss: 3.055701, acc.: 50.00%] [G loss: 4.639181]\n",
      "29 [D loss: 2.629574, acc.: 50.00%] [G loss: 4.621036]\n",
      "30 [D loss: 2.910769, acc.: 50.00%] [G loss: 4.641296]\n",
      "31 [D loss: 2.998551, acc.: 50.00%] [G loss: 4.633460]\n",
      "32 [D loss: 3.190086, acc.: 50.00%] [G loss: 4.661957]\n",
      "33 [D loss: 2.535069, acc.: 50.00%] [G loss: 4.659246]\n",
      "34 [D loss: 2.860146, acc.: 50.00%] [G loss: 4.459136]\n",
      "35 [D loss: 2.225960, acc.: 50.00%] [G loss: 4.603024]\n",
      "36 [D loss: 2.493066, acc.: 50.00%] [G loss: 4.512372]\n",
      "37 [D loss: 2.538013, acc.: 50.00%] [G loss: 4.459503]\n",
      "38 [D loss: 2.659245, acc.: 50.00%] [G loss: 4.444161]\n",
      "39 [D loss: 2.561160, acc.: 50.00%] [G loss: 4.394105]\n",
      "40 [D loss: 2.776133, acc.: 50.00%] [G loss: 4.437463]\n",
      "41 [D loss: 2.637464, acc.: 50.00%] [G loss: 4.361022]\n",
      "42 [D loss: 2.456181, acc.: 50.00%] [G loss: 4.409472]\n",
      "43 [D loss: 3.059446, acc.: 50.00%] [G loss: 4.306004]\n",
      "44 [D loss: 2.819777, acc.: 50.00%] [G loss: 4.427184]\n",
      "45 [D loss: 2.621750, acc.: 50.00%] [G loss: 4.405679]\n",
      "46 [D loss: 2.458551, acc.: 50.00%] [G loss: 4.324729]\n",
      "47 [D loss: 2.621356, acc.: 50.00%] [G loss: 4.360802]\n",
      "48 [D loss: 2.604414, acc.: 50.00%] [G loss: 4.354012]\n",
      "49 [D loss: 2.402972, acc.: 50.00%] [G loss: 4.253131]\n",
      "50 [D loss: 2.551750, acc.: 50.00%] [G loss: 4.341276]\n",
      "51 [D loss: 2.285300, acc.: 50.00%] [G loss: 4.131194]\n",
      "52 [D loss: 2.486104, acc.: 50.00%] [G loss: 4.148251]\n",
      "53 [D loss: 2.374190, acc.: 50.00%] [G loss: 4.173440]\n",
      "54 [D loss: 2.846886, acc.: 50.00%] [G loss: 4.146145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 [D loss: 2.210382, acc.: 50.00%] [G loss: 4.194010]\n",
      "56 [D loss: 2.175672, acc.: 50.00%] [G loss: 4.181369]\n",
      "57 [D loss: 2.459784, acc.: 50.00%] [G loss: 4.204001]\n",
      "58 [D loss: 2.337347, acc.: 50.00%] [G loss: 4.206207]\n",
      "59 [D loss: 2.372390, acc.: 50.00%] [G loss: 4.116137]\n",
      "60 [D loss: 2.104671, acc.: 50.00%] [G loss: 4.071361]\n",
      "61 [D loss: 2.407036, acc.: 50.00%] [G loss: 4.056628]\n",
      "62 [D loss: 2.312316, acc.: 50.00%] [G loss: 4.007140]\n",
      "63 [D loss: 2.389681, acc.: 50.00%] [G loss: 3.996643]\n",
      "64 [D loss: 2.916974, acc.: 50.00%] [G loss: 3.975606]\n",
      "65 [D loss: 2.083185, acc.: 50.00%] [G loss: 4.060205]\n",
      "66 [D loss: 2.073680, acc.: 50.00%] [G loss: 4.061057]\n",
      "67 [D loss: 2.060565, acc.: 50.00%] [G loss: 3.994590]\n",
      "68 [D loss: 2.032228, acc.: 50.00%] [G loss: 3.964619]\n",
      "69 [D loss: 2.071178, acc.: 50.00%] [G loss: 4.023966]\n",
      "70 [D loss: 2.385826, acc.: 50.00%] [G loss: 3.961540]\n",
      "71 [D loss: 2.101161, acc.: 50.00%] [G loss: 4.017519]\n",
      "72 [D loss: 2.055270, acc.: 50.00%] [G loss: 3.928680]\n",
      "73 [D loss: 2.071340, acc.: 50.00%] [G loss: 3.939250]\n",
      "74 [D loss: 2.015357, acc.: 50.00%] [G loss: 3.928464]\n",
      "75 [D loss: 2.056250, acc.: 50.00%] [G loss: 3.919636]\n",
      "76 [D loss: 2.096541, acc.: 50.00%] [G loss: 3.917952]\n",
      "77 [D loss: 1.997707, acc.: 50.00%] [G loss: 3.835257]\n",
      "78 [D loss: 2.009040, acc.: 50.00%] [G loss: 3.748472]\n",
      "79 [D loss: 2.042380, acc.: 50.00%] [G loss: 3.746482]\n",
      "80 [D loss: 2.207616, acc.: 50.00%] [G loss: 3.857718]\n",
      "81 [D loss: 1.944453, acc.: 50.00%] [G loss: 3.821606]\n",
      "82 [D loss: 2.124038, acc.: 50.00%] [G loss: 3.780613]\n",
      "83 [D loss: 1.995127, acc.: 50.00%] [G loss: 3.868508]\n",
      "84 [D loss: 1.955599, acc.: 50.00%] [G loss: 3.767338]\n",
      "85 [D loss: 2.002674, acc.: 50.00%] [G loss: 3.759919]\n",
      "86 [D loss: 2.032563, acc.: 50.00%] [G loss: 3.690955]\n",
      "87 [D loss: 1.912072, acc.: 50.00%] [G loss: 3.698742]\n",
      "88 [D loss: 1.916005, acc.: 50.00%] [G loss: 3.768647]\n",
      "89 [D loss: 1.874031, acc.: 50.00%] [G loss: 3.729038]\n",
      "90 [D loss: 1.920169, acc.: 50.00%] [G loss: 3.845314]\n",
      "91 [D loss: 1.915577, acc.: 50.00%] [G loss: 3.699582]\n",
      "92 [D loss: 1.967278, acc.: 50.00%] [G loss: 3.797377]\n",
      "93 [D loss: 1.972975, acc.: 50.00%] [G loss: 3.671070]\n",
      "94 [D loss: 1.819741, acc.: 50.00%] [G loss: 3.639708]\n",
      "95 [D loss: 1.874999, acc.: 50.00%] [G loss: 3.574084]\n",
      "96 [D loss: 1.984090, acc.: 50.00%] [G loss: 3.649813]\n",
      "97 [D loss: 1.835877, acc.: 50.00%] [G loss: 3.680916]\n",
      "98 [D loss: 1.838109, acc.: 50.00%] [G loss: 3.566051]\n",
      "99 [D loss: 1.940747, acc.: 50.00%] [G loss: 3.566074]\n",
      "100 [D loss: 1.830040, acc.: 50.00%] [G loss: 3.555428]\n",
      "101 [D loss: 1.809512, acc.: 50.00%] [G loss: 3.605788]\n",
      "102 [D loss: 1.838311, acc.: 50.00%] [G loss: 3.538210]\n",
      "103 [D loss: 1.834714, acc.: 50.00%] [G loss: 3.653207]\n",
      "104 [D loss: 1.863181, acc.: 50.00%] [G loss: 3.658687]\n",
      "105 [D loss: 1.799319, acc.: 50.00%] [G loss: 3.472118]\n",
      "106 [D loss: 1.838982, acc.: 50.00%] [G loss: 3.571737]\n",
      "107 [D loss: 1.790181, acc.: 50.00%] [G loss: 3.616795]\n",
      "108 [D loss: 1.768831, acc.: 50.00%] [G loss: 3.785390]\n",
      "109 [D loss: 1.892087, acc.: 50.00%] [G loss: 3.561584]\n",
      "110 [D loss: 1.772699, acc.: 50.00%] [G loss: 3.537928]\n",
      "111 [D loss: 1.814162, acc.: 50.00%] [G loss: 3.543717]\n",
      "112 [D loss: 1.754878, acc.: 50.00%] [G loss: 3.550001]\n",
      "113 [D loss: 1.761191, acc.: 50.00%] [G loss: 3.523723]\n",
      "114 [D loss: 1.793515, acc.: 50.00%] [G loss: 3.561224]\n",
      "115 [D loss: 1.849794, acc.: 50.00%] [G loss: 3.554393]\n",
      "116 [D loss: 1.770169, acc.: 50.00%] [G loss: 3.577585]\n",
      "117 [D loss: 1.732958, acc.: 50.00%] [G loss: 3.611119]\n",
      "118 [D loss: 1.712172, acc.: 50.00%] [G loss: 3.462723]\n",
      "119 [D loss: 1.778370, acc.: 50.00%] [G loss: 3.519255]\n",
      "120 [D loss: 1.772994, acc.: 50.00%] [G loss: 3.568275]\n",
      "121 [D loss: 1.811857, acc.: 50.00%] [G loss: 3.438233]\n",
      "122 [D loss: 1.738914, acc.: 50.00%] [G loss: 3.650401]\n",
      "123 [D loss: 1.800688, acc.: 50.00%] [G loss: 3.482811]\n",
      "124 [D loss: 1.747996, acc.: 50.00%] [G loss: 3.371565]\n",
      "125 [D loss: 1.738674, acc.: 50.00%] [G loss: 3.425772]\n",
      "126 [D loss: 1.860051, acc.: 50.00%] [G loss: 3.404092]\n",
      "127 [D loss: 1.770271, acc.: 50.00%] [G loss: 3.480054]\n",
      "128 [D loss: 1.753323, acc.: 50.00%] [G loss: 3.471521]\n",
      "129 [D loss: 1.742325, acc.: 50.00%] [G loss: 3.337559]\n",
      "130 [D loss: 1.737469, acc.: 50.00%] [G loss: 3.494616]\n",
      "131 [D loss: 1.693163, acc.: 50.00%] [G loss: 3.428200]\n",
      "132 [D loss: 1.762821, acc.: 50.00%] [G loss: 3.401235]\n",
      "133 [D loss: 1.688549, acc.: 50.00%] [G loss: 3.369035]\n",
      "134 [D loss: 1.683673, acc.: 50.00%] [G loss: 3.284890]\n",
      "135 [D loss: 1.703712, acc.: 50.00%] [G loss: 3.379906]\n",
      "136 [D loss: 1.730423, acc.: 50.00%] [G loss: 3.418641]\n",
      "137 [D loss: 1.735640, acc.: 50.00%] [G loss: 3.313745]\n",
      "138 [D loss: 1.737929, acc.: 50.00%] [G loss: 3.354188]\n",
      "139 [D loss: 1.769517, acc.: 50.00%] [G loss: 3.458579]\n",
      "140 [D loss: 1.690301, acc.: 50.00%] [G loss: 3.318693]\n",
      "141 [D loss: 1.676535, acc.: 50.00%] [G loss: 3.443405]\n",
      "142 [D loss: 1.661179, acc.: 50.00%] [G loss: 3.315618]\n",
      "143 [D loss: 1.699853, acc.: 50.00%] [G loss: 3.292252]\n",
      "144 [D loss: 1.770070, acc.: 50.00%] [G loss: 3.335105]\n",
      "145 [D loss: 1.754398, acc.: 50.00%] [G loss: 3.383929]\n",
      "146 [D loss: 1.727871, acc.: 50.00%] [G loss: 3.306155]\n",
      "147 [D loss: 1.664122, acc.: 50.00%] [G loss: 3.239233]\n",
      "148 [D loss: 1.989243, acc.: 50.00%] [G loss: 3.329028]\n",
      "149 [D loss: 1.664974, acc.: 50.00%] [G loss: 3.317335]\n",
      "150 [D loss: 1.635248, acc.: 50.00%] [G loss: 3.374735]\n",
      "151 [D loss: 1.694970, acc.: 50.00%] [G loss: 3.333502]\n",
      "152 [D loss: 1.640920, acc.: 50.00%] [G loss: 3.231822]\n",
      "153 [D loss: 1.629268, acc.: 50.00%] [G loss: 3.246333]\n",
      "154 [D loss: 1.652254, acc.: 50.00%] [G loss: 3.261170]\n",
      "155 [D loss: 1.680284, acc.: 50.00%] [G loss: 3.307698]\n",
      "156 [D loss: 1.681828, acc.: 50.00%] [G loss: 3.283596]\n",
      "157 [D loss: 1.678796, acc.: 50.00%] [G loss: 3.313575]\n",
      "158 [D loss: 1.617284, acc.: 50.00%] [G loss: 3.286422]\n",
      "159 [D loss: 1.591099, acc.: 50.00%] [G loss: 3.199314]\n",
      "160 [D loss: 1.638731, acc.: 50.00%] [G loss: 3.136395]\n",
      "161 [D loss: 1.638232, acc.: 50.00%] [G loss: 3.233098]\n",
      "162 [D loss: 1.659301, acc.: 50.00%] [G loss: 3.251813]\n",
      "163 [D loss: 1.633612, acc.: 50.00%] [G loss: 3.344273]\n",
      "164 [D loss: 1.646933, acc.: 50.00%] [G loss: 3.156402]\n",
      "165 [D loss: 1.706167, acc.: 50.00%] [G loss: 3.098886]\n",
      "166 [D loss: 1.606043, acc.: 50.00%] [G loss: 3.225425]\n",
      "167 [D loss: 1.583836, acc.: 50.00%] [G loss: 3.233810]\n",
      "168 [D loss: 1.636094, acc.: 50.00%] [G loss: 3.250385]\n",
      "169 [D loss: 1.693513, acc.: 50.00%] [G loss: 3.253307]\n",
      "170 [D loss: 1.642466, acc.: 50.00%] [G loss: 3.128380]\n",
      "171 [D loss: 1.553159, acc.: 50.00%] [G loss: 3.154735]\n",
      "172 [D loss: 1.711454, acc.: 50.00%] [G loss: 3.198470]\n",
      "173 [D loss: 1.621738, acc.: 50.00%] [G loss: 3.179024]\n",
      "174 [D loss: 1.652137, acc.: 50.00%] [G loss: 3.175776]\n",
      "175 [D loss: 1.605427, acc.: 50.00%] [G loss: 3.148630]\n",
      "176 [D loss: 1.626443, acc.: 50.00%] [G loss: 3.195978]\n",
      "177 [D loss: 1.592759, acc.: 50.00%] [G loss: 3.150281]\n",
      "178 [D loss: 1.642254, acc.: 50.00%] [G loss: 3.061593]\n",
      "179 [D loss: 1.532026, acc.: 50.00%] [G loss: 3.155076]\n",
      "180 [D loss: 1.631653, acc.: 50.00%] [G loss: 3.186310]\n",
      "181 [D loss: 1.655340, acc.: 50.00%] [G loss: 3.144027]\n",
      "182 [D loss: 1.625353, acc.: 50.00%] [G loss: 3.164059]\n",
      "183 [D loss: 1.628125, acc.: 50.00%] [G loss: 3.162163]\n",
      "184 [D loss: 1.500201, acc.: 50.00%] [G loss: 3.292971]\n",
      "185 [D loss: 1.599939, acc.: 50.00%] [G loss: 3.244492]\n",
      "186 [D loss: 1.583897, acc.: 50.00%] [G loss: 3.125491]\n",
      "187 [D loss: 1.596372, acc.: 50.00%] [G loss: 3.120064]\n",
      "188 [D loss: 1.562979, acc.: 50.00%] [G loss: 3.155354]\n",
      "189 [D loss: 1.597072, acc.: 50.00%] [G loss: 3.150485]\n",
      "190 [D loss: 1.616558, acc.: 50.00%] [G loss: 3.093850]\n",
      "191 [D loss: 1.473981, acc.: 50.00%] [G loss: 3.078970]\n",
      "192 [D loss: 1.614331, acc.: 50.00%] [G loss: 3.089119]\n",
      "193 [D loss: 1.558809, acc.: 50.00%] [G loss: 3.102350]\n",
      "194 [D loss: 1.584675, acc.: 50.00%] [G loss: 3.122842]\n",
      "195 [D loss: 1.579451, acc.: 50.00%] [G loss: 3.086718]\n",
      "196 [D loss: 1.607025, acc.: 50.00%] [G loss: 3.153750]\n",
      "197 [D loss: 1.570894, acc.: 50.00%] [G loss: 3.063335]\n",
      "198 [D loss: 1.572810, acc.: 50.00%] [G loss: 3.039172]\n",
      "199 [D loss: 1.558390, acc.: 50.00%] [G loss: 3.053867]\n",
      "200 [D loss: 1.540260, acc.: 50.00%] [G loss: 2.956847]\n",
      "201 [D loss: 1.566960, acc.: 50.00%] [G loss: 3.094490]\n",
      "202 [D loss: 1.585472, acc.: 50.00%] [G loss: 2.978679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203 [D loss: 1.555341, acc.: 50.00%] [G loss: 3.049551]\n",
      "204 [D loss: 1.499780, acc.: 50.00%] [G loss: 3.057087]\n",
      "205 [D loss: 1.552486, acc.: 50.00%] [G loss: 3.132843]\n",
      "206 [D loss: 1.511579, acc.: 50.00%] [G loss: 3.123629]\n",
      "207 [D loss: 1.556598, acc.: 50.00%] [G loss: 3.116178]\n",
      "208 [D loss: 1.562484, acc.: 50.00%] [G loss: 3.126356]\n",
      "209 [D loss: 1.485398, acc.: 50.00%] [G loss: 3.122161]\n",
      "210 [D loss: 1.544786, acc.: 50.00%] [G loss: 2.929680]\n",
      "211 [D loss: 1.536952, acc.: 50.00%] [G loss: 3.021891]\n",
      "212 [D loss: 1.474403, acc.: 50.00%] [G loss: 3.107883]\n",
      "213 [D loss: 1.531035, acc.: 50.00%] [G loss: 3.037591]\n",
      "214 [D loss: 1.545677, acc.: 50.00%] [G loss: 3.071100]\n",
      "215 [D loss: 1.579341, acc.: 50.00%] [G loss: 2.971292]\n",
      "216 [D loss: 1.453918, acc.: 50.00%] [G loss: 3.101803]\n",
      "217 [D loss: 1.506394, acc.: 50.00%] [G loss: 3.009820]\n",
      "218 [D loss: 1.509866, acc.: 50.00%] [G loss: 3.021054]\n",
      "219 [D loss: 1.874200, acc.: 50.00%] [G loss: 3.027620]\n",
      "220 [D loss: 1.479151, acc.: 50.00%] [G loss: 2.874731]\n",
      "221 [D loss: 1.574601, acc.: 50.00%] [G loss: 3.048767]\n",
      "222 [D loss: 1.506859, acc.: 50.00%] [G loss: 2.978765]\n",
      "223 [D loss: 1.552486, acc.: 50.00%] [G loss: 3.156102]\n",
      "224 [D loss: 1.433907, acc.: 50.00%] [G loss: 2.976313]\n",
      "225 [D loss: 1.583951, acc.: 50.00%] [G loss: 3.010882]\n",
      "226 [D loss: 1.547624, acc.: 50.00%] [G loss: 2.917989]\n",
      "227 [D loss: 1.473858, acc.: 50.00%] [G loss: 2.903805]\n",
      "228 [D loss: 1.464052, acc.: 50.00%] [G loss: 2.986936]\n",
      "229 [D loss: 1.450806, acc.: 50.00%] [G loss: 3.042363]\n",
      "230 [D loss: 1.518051, acc.: 50.00%] [G loss: 3.151960]\n",
      "231 [D loss: 1.574034, acc.: 50.00%] [G loss: 2.867286]\n",
      "232 [D loss: 1.485215, acc.: 50.00%] [G loss: 2.875307]\n",
      "233 [D loss: 1.429758, acc.: 50.00%] [G loss: 3.011906]\n",
      "234 [D loss: 1.506380, acc.: 50.00%] [G loss: 2.871554]\n",
      "235 [D loss: 1.489942, acc.: 50.00%] [G loss: 2.937469]\n",
      "236 [D loss: 1.490294, acc.: 50.00%] [G loss: 2.938633]\n",
      "237 [D loss: 1.445049, acc.: 50.00%] [G loss: 2.809013]\n",
      "238 [D loss: 1.423053, acc.: 50.00%] [G loss: 2.847539]\n",
      "239 [D loss: 1.427400, acc.: 50.00%] [G loss: 2.981155]\n",
      "240 [D loss: 1.511268, acc.: 50.00%] [G loss: 2.862142]\n",
      "241 [D loss: 1.449371, acc.: 50.00%] [G loss: 2.893821]\n",
      "242 [D loss: 1.405598, acc.: 50.00%] [G loss: 2.936697]\n",
      "243 [D loss: 1.462633, acc.: 50.00%] [G loss: 2.900590]\n",
      "244 [D loss: 1.483245, acc.: 50.00%] [G loss: 2.915972]\n",
      "245 [D loss: 1.439349, acc.: 50.00%] [G loss: 2.770672]\n",
      "246 [D loss: 1.474762, acc.: 50.00%] [G loss: 2.913507]\n",
      "247 [D loss: 1.402177, acc.: 50.00%] [G loss: 2.918893]\n",
      "248 [D loss: 1.407689, acc.: 50.00%] [G loss: 2.866937]\n",
      "249 [D loss: 1.812682, acc.: 50.00%] [G loss: 2.953183]\n",
      "250 [D loss: 1.433288, acc.: 50.00%] [G loss: 2.943752]\n",
      "251 [D loss: 1.394354, acc.: 50.00%] [G loss: 2.846240]\n",
      "252 [D loss: 1.473848, acc.: 50.00%] [G loss: 2.871828]\n",
      "253 [D loss: 1.541688, acc.: 50.00%] [G loss: 2.887985]\n",
      "254 [D loss: 1.444552, acc.: 50.00%] [G loss: 2.755744]\n",
      "255 [D loss: 1.511214, acc.: 50.00%] [G loss: 2.799157]\n",
      "256 [D loss: 1.424477, acc.: 50.00%] [G loss: 2.892509]\n",
      "257 [D loss: 1.515856, acc.: 50.00%] [G loss: 2.875346]\n",
      "258 [D loss: 1.414417, acc.: 50.00%] [G loss: 2.955752]\n",
      "259 [D loss: 1.491485, acc.: 50.00%] [G loss: 2.842113]\n",
      "260 [D loss: 1.451262, acc.: 50.00%] [G loss: 2.965628]\n",
      "261 [D loss: 1.377634, acc.: 50.00%] [G loss: 2.786827]\n",
      "262 [D loss: 1.392034, acc.: 50.00%] [G loss: 2.848327]\n",
      "263 [D loss: 1.378988, acc.: 50.00%] [G loss: 2.789512]\n",
      "264 [D loss: 1.436443, acc.: 50.00%] [G loss: 2.905310]\n",
      "265 [D loss: 1.416272, acc.: 50.00%] [G loss: 2.983581]\n",
      "266 [D loss: 1.395361, acc.: 50.00%] [G loss: 2.759670]\n",
      "267 [D loss: 1.358356, acc.: 50.00%] [G loss: 2.871395]\n",
      "268 [D loss: 1.489354, acc.: 50.00%] [G loss: 2.860418]\n",
      "269 [D loss: 1.386556, acc.: 50.00%] [G loss: 2.909827]\n",
      "270 [D loss: 1.482314, acc.: 50.00%] [G loss: 2.835664]\n",
      "271 [D loss: 1.378155, acc.: 50.00%] [G loss: 2.757608]\n",
      "272 [D loss: 1.399532, acc.: 50.00%] [G loss: 2.838900]\n",
      "273 [D loss: 1.428128, acc.: 50.00%] [G loss: 2.814723]\n",
      "274 [D loss: 1.398266, acc.: 50.00%] [G loss: 2.848881]\n",
      "275 [D loss: 1.332812, acc.: 50.00%] [G loss: 2.685373]\n",
      "276 [D loss: 1.403999, acc.: 50.00%] [G loss: 2.812436]\n",
      "277 [D loss: 1.397698, acc.: 50.00%] [G loss: 2.712658]\n",
      "278 [D loss: 1.393610, acc.: 50.00%] [G loss: 2.770854]\n",
      "279 [D loss: 1.388783, acc.: 50.00%] [G loss: 2.742485]\n",
      "280 [D loss: 1.469241, acc.: 50.00%] [G loss: 2.781082]\n",
      "281 [D loss: 1.391406, acc.: 50.00%] [G loss: 2.769913]\n",
      "282 [D loss: 1.365092, acc.: 50.00%] [G loss: 2.851369]\n",
      "283 [D loss: 1.381223, acc.: 50.00%] [G loss: 2.810659]\n",
      "284 [D loss: 1.388736, acc.: 50.00%] [G loss: 2.742818]\n",
      "285 [D loss: 1.372061, acc.: 50.00%] [G loss: 2.724873]\n",
      "286 [D loss: 1.410115, acc.: 50.00%] [G loss: 2.729579]\n",
      "287 [D loss: 1.428252, acc.: 50.00%] [G loss: 2.724793]\n",
      "288 [D loss: 1.411702, acc.: 50.00%] [G loss: 2.707192]\n",
      "289 [D loss: 1.376834, acc.: 50.00%] [G loss: 2.815787]\n",
      "290 [D loss: 1.429879, acc.: 50.00%] [G loss: 2.792812]\n",
      "291 [D loss: 1.373562, acc.: 50.00%] [G loss: 2.782590]\n",
      "292 [D loss: 1.362774, acc.: 50.00%] [G loss: 2.657239]\n",
      "293 [D loss: 1.347250, acc.: 50.00%] [G loss: 2.745603]\n",
      "294 [D loss: 1.388244, acc.: 50.00%] [G loss: 2.756710]\n",
      "295 [D loss: 1.401122, acc.: 50.00%] [G loss: 2.744620]\n",
      "296 [D loss: 1.353699, acc.: 50.00%] [G loss: 2.692383]\n",
      "297 [D loss: 1.611927, acc.: 50.00%] [G loss: 2.816977]\n",
      "298 [D loss: 1.334421, acc.: 50.00%] [G loss: 2.697812]\n",
      "299 [D loss: 1.460375, acc.: 50.00%] [G loss: 2.687099]\n",
      "300 [D loss: 1.374206, acc.: 50.00%] [G loss: 2.768103]\n",
      "301 [D loss: 1.324306, acc.: 50.00%] [G loss: 2.732680]\n",
      "302 [D loss: 1.392008, acc.: 50.00%] [G loss: 2.694922]\n",
      "303 [D loss: 1.375642, acc.: 50.00%] [G loss: 2.796425]\n",
      "304 [D loss: 1.394528, acc.: 50.00%] [G loss: 2.657307]\n",
      "305 [D loss: 1.379733, acc.: 50.00%] [G loss: 2.792971]\n",
      "306 [D loss: 1.363576, acc.: 50.00%] [G loss: 2.730526]\n",
      "307 [D loss: 1.370136, acc.: 50.00%] [G loss: 2.699392]\n",
      "308 [D loss: 1.335420, acc.: 50.00%] [G loss: 2.768810]\n",
      "309 [D loss: 1.327540, acc.: 50.00%] [G loss: 2.664064]\n",
      "310 [D loss: 1.390747, acc.: 50.00%] [G loss: 2.694523]\n",
      "311 [D loss: 1.378796, acc.: 50.00%] [G loss: 2.783969]\n",
      "312 [D loss: 1.246586, acc.: 50.00%] [G loss: 2.794207]\n",
      "313 [D loss: 1.299904, acc.: 50.00%] [G loss: 2.724946]\n",
      "314 [D loss: 1.362766, acc.: 50.00%] [G loss: 2.801089]\n",
      "315 [D loss: 1.296075, acc.: 50.00%] [G loss: 2.708994]\n",
      "316 [D loss: 1.321682, acc.: 50.00%] [G loss: 2.755331]\n",
      "317 [D loss: 1.316368, acc.: 50.00%] [G loss: 2.712540]\n",
      "318 [D loss: 1.290895, acc.: 50.00%] [G loss: 2.679326]\n",
      "319 [D loss: 1.320770, acc.: 50.00%] [G loss: 2.719966]\n",
      "320 [D loss: 1.330088, acc.: 50.00%] [G loss: 2.777461]\n",
      "321 [D loss: 1.292983, acc.: 50.00%] [G loss: 2.747811]\n",
      "322 [D loss: 1.287484, acc.: 50.00%] [G loss: 2.794736]\n",
      "323 [D loss: 1.288494, acc.: 50.00%] [G loss: 2.717718]\n",
      "324 [D loss: 1.230036, acc.: 50.00%] [G loss: 2.713963]\n",
      "325 [D loss: 1.309443, acc.: 50.00%] [G loss: 2.737370]\n",
      "326 [D loss: 1.336086, acc.: 50.00%] [G loss: 2.677420]\n",
      "327 [D loss: 1.254092, acc.: 50.00%] [G loss: 2.752761]\n",
      "328 [D loss: 1.344148, acc.: 50.00%] [G loss: 2.681428]\n",
      "329 [D loss: 1.260318, acc.: 50.00%] [G loss: 2.738343]\n",
      "330 [D loss: 1.246866, acc.: 50.00%] [G loss: 2.650682]\n",
      "331 [D loss: 1.239715, acc.: 50.00%] [G loss: 2.675011]\n",
      "332 [D loss: 1.253709, acc.: 50.00%] [G loss: 2.700786]\n",
      "333 [D loss: 1.300389, acc.: 50.00%] [G loss: 2.756288]\n",
      "334 [D loss: 1.236047, acc.: 50.00%] [G loss: 2.612514]\n",
      "335 [D loss: 1.281341, acc.: 50.00%] [G loss: 2.750947]\n",
      "336 [D loss: 1.315833, acc.: 50.00%] [G loss: 2.628593]\n",
      "337 [D loss: 1.223699, acc.: 50.00%] [G loss: 2.705186]\n",
      "338 [D loss: 1.288447, acc.: 50.00%] [G loss: 2.695453]\n",
      "339 [D loss: 1.341703, acc.: 50.00%] [G loss: 2.611153]\n",
      "340 [D loss: 1.264675, acc.: 50.00%] [G loss: 2.634125]\n",
      "341 [D loss: 1.347122, acc.: 50.00%] [G loss: 2.716876]\n",
      "342 [D loss: 1.311360, acc.: 50.00%] [G loss: 2.679929]\n",
      "343 [D loss: 1.231878, acc.: 50.00%] [G loss: 2.653375]\n",
      "344 [D loss: 1.317597, acc.: 50.00%] [G loss: 2.553123]\n",
      "345 [D loss: 1.254498, acc.: 50.00%] [G loss: 2.645924]\n",
      "346 [D loss: 1.230202, acc.: 50.00%] [G loss: 2.584423]\n",
      "347 [D loss: 1.242077, acc.: 50.00%] [G loss: 2.687152]\n",
      "348 [D loss: 1.318820, acc.: 50.00%] [G loss: 2.655710]\n",
      "349 [D loss: 1.384612, acc.: 50.00%] [G loss: 2.678546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 [D loss: 1.225906, acc.: 50.00%] [G loss: 2.666893]\n",
      "351 [D loss: 1.317489, acc.: 50.00%] [G loss: 2.604868]\n",
      "352 [D loss: 1.254243, acc.: 50.00%] [G loss: 2.710334]\n",
      "353 [D loss: 1.207209, acc.: 50.00%] [G loss: 2.514811]\n",
      "354 [D loss: 1.536451, acc.: 50.00%] [G loss: 2.499652]\n",
      "355 [D loss: 1.245948, acc.: 50.00%] [G loss: 2.613284]\n",
      "356 [D loss: 1.253982, acc.: 50.00%] [G loss: 2.640016]\n",
      "357 [D loss: 1.263859, acc.: 50.00%] [G loss: 2.480069]\n",
      "358 [D loss: 1.342254, acc.: 50.00%] [G loss: 2.524191]\n",
      "359 [D loss: 1.281195, acc.: 50.00%] [G loss: 2.546479]\n",
      "360 [D loss: 1.240292, acc.: 50.00%] [G loss: 2.621329]\n",
      "361 [D loss: 1.333709, acc.: 50.00%] [G loss: 2.534780]\n",
      "362 [D loss: 1.325659, acc.: 50.00%] [G loss: 2.648654]\n",
      "363 [D loss: 1.273699, acc.: 50.00%] [G loss: 2.483217]\n",
      "364 [D loss: 1.228012, acc.: 50.00%] [G loss: 2.566459]\n",
      "365 [D loss: 1.266438, acc.: 50.00%] [G loss: 2.573307]\n",
      "366 [D loss: 1.188753, acc.: 50.00%] [G loss: 2.487729]\n",
      "367 [D loss: 1.307042, acc.: 50.00%] [G loss: 2.563797]\n",
      "368 [D loss: 1.295877, acc.: 50.00%] [G loss: 2.618245]\n",
      "369 [D loss: 1.559955, acc.: 50.00%] [G loss: 2.570494]\n",
      "370 [D loss: 1.244808, acc.: 50.00%] [G loss: 2.674100]\n",
      "371 [D loss: 1.184378, acc.: 50.00%] [G loss: 2.586639]\n",
      "372 [D loss: 1.192031, acc.: 50.00%] [G loss: 2.629633]\n",
      "373 [D loss: 1.151321, acc.: 50.00%] [G loss: 2.506809]\n",
      "374 [D loss: 1.227300, acc.: 50.00%] [G loss: 2.718970]\n",
      "375 [D loss: 1.139852, acc.: 50.00%] [G loss: 2.489236]\n",
      "376 [D loss: 1.282404, acc.: 50.00%] [G loss: 2.670261]\n",
      "377 [D loss: 1.168573, acc.: 50.00%] [G loss: 2.653048]\n",
      "378 [D loss: 1.286515, acc.: 50.00%] [G loss: 2.539233]\n",
      "379 [D loss: 1.195027, acc.: 50.00%] [G loss: 2.646150]\n",
      "380 [D loss: 1.226921, acc.: 50.00%] [G loss: 2.568365]\n",
      "381 [D loss: 1.173058, acc.: 50.00%] [G loss: 2.535782]\n",
      "382 [D loss: 1.258326, acc.: 50.00%] [G loss: 2.602517]\n",
      "383 [D loss: 1.240179, acc.: 50.00%] [G loss: 2.570428]\n",
      "384 [D loss: 1.216125, acc.: 50.00%] [G loss: 2.590560]\n",
      "385 [D loss: 1.140426, acc.: 50.00%] [G loss: 2.486291]\n",
      "386 [D loss: 1.327282, acc.: 50.00%] [G loss: 2.530157]\n",
      "387 [D loss: 1.199561, acc.: 50.00%] [G loss: 2.662135]\n",
      "388 [D loss: 1.104890, acc.: 50.00%] [G loss: 2.478204]\n",
      "389 [D loss: 1.131831, acc.: 50.00%] [G loss: 2.533389]\n",
      "390 [D loss: 1.170503, acc.: 50.00%] [G loss: 2.625997]\n",
      "391 [D loss: 1.193844, acc.: 50.00%] [G loss: 2.512316]\n",
      "392 [D loss: 1.146885, acc.: 50.00%] [G loss: 2.517428]\n",
      "393 [D loss: 1.216002, acc.: 50.00%] [G loss: 2.511420]\n",
      "394 [D loss: 1.142335, acc.: 50.00%] [G loss: 2.593291]\n",
      "395 [D loss: 1.116017, acc.: 50.00%] [G loss: 2.528552]\n",
      "396 [D loss: 1.259498, acc.: 50.00%] [G loss: 2.550193]\n",
      "397 [D loss: 1.064977, acc.: 50.00%] [G loss: 2.465682]\n",
      "398 [D loss: 1.169615, acc.: 50.00%] [G loss: 2.573677]\n",
      "399 [D loss: 1.325061, acc.: 50.00%] [G loss: 2.476792]\n",
      "400 [D loss: 1.181098, acc.: 50.00%] [G loss: 2.591342]\n",
      "401 [D loss: 1.159268, acc.: 50.00%] [G loss: 2.499048]\n",
      "402 [D loss: 1.075791, acc.: 50.00%] [G loss: 2.601423]\n",
      "403 [D loss: 1.235577, acc.: 50.00%] [G loss: 2.313459]\n",
      "404 [D loss: 1.211469, acc.: 50.00%] [G loss: 2.591367]\n",
      "405 [D loss: 1.225904, acc.: 50.00%] [G loss: 2.515633]\n",
      "406 [D loss: 1.254191, acc.: 50.00%] [G loss: 2.686251]\n",
      "407 [D loss: 1.127374, acc.: 50.00%] [G loss: 2.459515]\n",
      "408 [D loss: 1.193001, acc.: 50.00%] [G loss: 2.557904]\n",
      "409 [D loss: 1.168977, acc.: 50.00%] [G loss: 2.486558]\n",
      "410 [D loss: 1.172942, acc.: 50.00%] [G loss: 2.550479]\n",
      "411 [D loss: 1.190004, acc.: 50.00%] [G loss: 2.377753]\n",
      "412 [D loss: 1.063161, acc.: 50.00%] [G loss: 2.435310]\n",
      "413 [D loss: 1.234397, acc.: 50.00%] [G loss: 2.360795]\n",
      "414 [D loss: 1.192717, acc.: 50.00%] [G loss: 2.539106]\n",
      "415 [D loss: 1.121399, acc.: 50.00%] [G loss: 2.469573]\n",
      "416 [D loss: 1.103999, acc.: 50.00%] [G loss: 2.340729]\n",
      "417 [D loss: 1.084921, acc.: 50.00%] [G loss: 2.485813]\n",
      "418 [D loss: 1.234654, acc.: 50.00%] [G loss: 2.410937]\n",
      "419 [D loss: 1.138497, acc.: 50.00%] [G loss: 2.483873]\n",
      "420 [D loss: 1.192344, acc.: 50.00%] [G loss: 2.562401]\n",
      "421 [D loss: 1.198266, acc.: 50.00%] [G loss: 2.403275]\n",
      "422 [D loss: 1.199031, acc.: 50.00%] [G loss: 2.447901]\n",
      "423 [D loss: 1.207489, acc.: 50.00%] [G loss: 2.496801]\n",
      "424 [D loss: 1.105112, acc.: 50.00%] [G loss: 2.549511]\n",
      "425 [D loss: 1.110356, acc.: 50.00%] [G loss: 2.462215]\n",
      "426 [D loss: 1.190630, acc.: 50.00%] [G loss: 2.594260]\n",
      "427 [D loss: 1.124861, acc.: 50.00%] [G loss: 2.363878]\n",
      "428 [D loss: 1.101640, acc.: 50.00%] [G loss: 2.551262]\n",
      "429 [D loss: 1.189128, acc.: 50.00%] [G loss: 2.405318]\n",
      "430 [D loss: 1.077656, acc.: 50.00%] [G loss: 2.559680]\n",
      "431 [D loss: 1.279056, acc.: 50.00%] [G loss: 2.544694]\n",
      "432 [D loss: 1.099265, acc.: 50.00%] [G loss: 2.650718]\n",
      "433 [D loss: 1.134987, acc.: 50.00%] [G loss: 2.454332]\n",
      "434 [D loss: 1.213068, acc.: 50.00%] [G loss: 2.430467]\n",
      "435 [D loss: 1.192290, acc.: 50.00%] [G loss: 2.598413]\n",
      "436 [D loss: 1.161348, acc.: 50.00%] [G loss: 2.460305]\n",
      "437 [D loss: 1.174088, acc.: 50.00%] [G loss: 2.404978]\n",
      "438 [D loss: 1.178348, acc.: 50.00%] [G loss: 2.441234]\n",
      "439 [D loss: 1.125967, acc.: 50.00%] [G loss: 2.509099]\n",
      "440 [D loss: 1.124435, acc.: 50.00%] [G loss: 2.509511]\n",
      "441 [D loss: 1.152455, acc.: 50.00%] [G loss: 2.416735]\n",
      "442 [D loss: 1.210891, acc.: 50.00%] [G loss: 2.407787]\n",
      "443 [D loss: 1.099833, acc.: 50.00%] [G loss: 2.393522]\n",
      "444 [D loss: 1.125366, acc.: 50.00%] [G loss: 2.403082]\n",
      "445 [D loss: 1.024054, acc.: 50.00%] [G loss: 2.356772]\n",
      "446 [D loss: 1.145491, acc.: 50.00%] [G loss: 2.089266]\n",
      "447 [D loss: 1.188506, acc.: 47.50%] [G loss: 1.584494]\n",
      "448 [D loss: 1.263331, acc.: 30.00%] [G loss: 1.651240]\n",
      "449 [D loss: 2.934196, acc.: 27.50%] [G loss: 1.205636]\n",
      "450 [D loss: 2.440425, acc.: 30.00%] [G loss: 1.072289]\n",
      "451 [D loss: 2.000452, acc.: 35.00%] [G loss: 1.011235]\n",
      "452 [D loss: 2.176589, acc.: 30.00%] [G loss: 2.081737]\n",
      "453 [D loss: 1.278661, acc.: 32.50%] [G loss: 2.056576]\n",
      "454 [D loss: 1.708020, acc.: 40.00%] [G loss: 2.855462]\n",
      "455 [D loss: 1.154625, acc.: 47.50%] [G loss: 1.674852]\n",
      "456 [D loss: 1.245281, acc.: 45.00%] [G loss: 1.783424]\n",
      "457 [D loss: 1.303415, acc.: 45.00%] [G loss: 3.430287]\n",
      "458 [D loss: 1.293513, acc.: 42.50%] [G loss: 3.529675]\n",
      "459 [D loss: 1.125671, acc.: 50.00%] [G loss: 5.148999]\n",
      "460 [D loss: 1.117726, acc.: 47.50%] [G loss: 3.350571]\n",
      "461 [D loss: 1.459004, acc.: 50.00%] [G loss: 4.158169]\n",
      "462 [D loss: 1.140247, acc.: 50.00%] [G loss: 4.658538]\n",
      "463 [D loss: 1.166823, acc.: 50.00%] [G loss: 4.517473]\n",
      "464 [D loss: 1.199344, acc.: 50.00%] [G loss: 4.653893]\n",
      "465 [D loss: 1.095941, acc.: 47.50%] [G loss: 4.501169]\n",
      "466 [D loss: 1.204956, acc.: 50.00%] [G loss: 4.727685]\n",
      "467 [D loss: 1.133892, acc.: 47.50%] [G loss: 4.712738]\n",
      "468 [D loss: 1.097932, acc.: 50.00%] [G loss: 4.259697]\n",
      "469 [D loss: 1.490466, acc.: 50.00%] [G loss: 3.150868]\n",
      "470 [D loss: 1.128696, acc.: 50.00%] [G loss: 5.563410]\n",
      "471 [D loss: 1.204522, acc.: 50.00%] [G loss: 3.494244]\n",
      "472 [D loss: 1.455565, acc.: 50.00%] [G loss: 6.479617]\n",
      "473 [D loss: 1.243656, acc.: 47.50%] [G loss: 3.704599]\n",
      "474 [D loss: 1.190821, acc.: 47.50%] [G loss: 4.429540]\n",
      "475 [D loss: 1.070758, acc.: 50.00%] [G loss: 5.919700]\n",
      "476 [D loss: 1.154341, acc.: 47.50%] [G loss: 4.217896]\n",
      "477 [D loss: 1.051767, acc.: 50.00%] [G loss: 4.020139]\n",
      "478 [D loss: 1.034116, acc.: 50.00%] [G loss: 5.033923]\n",
      "479 [D loss: 1.179366, acc.: 50.00%] [G loss: 3.775419]\n",
      "480 [D loss: 1.103759, acc.: 50.00%] [G loss: 4.219033]\n",
      "481 [D loss: 1.088040, acc.: 50.00%] [G loss: 3.099991]\n",
      "482 [D loss: 1.118603, acc.: 50.00%] [G loss: 7.401662]\n",
      "483 [D loss: 1.087493, acc.: 50.00%] [G loss: 4.745517]\n",
      "484 [D loss: 1.124589, acc.: 50.00%] [G loss: 4.453822]\n",
      "485 [D loss: 1.101047, acc.: 50.00%] [G loss: 4.665401]\n",
      "486 [D loss: 1.045359, acc.: 50.00%] [G loss: 2.361784]\n",
      "487 [D loss: 1.065517, acc.: 50.00%] [G loss: 3.692448]\n",
      "488 [D loss: 1.107285, acc.: 50.00%] [G loss: 6.223751]\n",
      "489 [D loss: 0.991640, acc.: 50.00%] [G loss: 4.316291]\n",
      "490 [D loss: 1.193432, acc.: 50.00%] [G loss: 3.026892]\n",
      "491 [D loss: 1.089054, acc.: 50.00%] [G loss: 2.962553]\n",
      "492 [D loss: 0.993929, acc.: 50.00%] [G loss: 5.028772]\n",
      "493 [D loss: 1.019587, acc.: 50.00%] [G loss: 5.435249]\n",
      "494 [D loss: 1.016901, acc.: 50.00%] [G loss: 3.892713]\n",
      "495 [D loss: 1.192930, acc.: 50.00%] [G loss: 3.558959]\n",
      "496 [D loss: 1.067222, acc.: 50.00%] [G loss: 2.885636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497 [D loss: 0.971090, acc.: 50.00%] [G loss: 2.185667]\n",
      "498 [D loss: 1.130075, acc.: 50.00%] [G loss: 4.957610]\n",
      "499 [D loss: 1.066732, acc.: 50.00%] [G loss: 2.894413]\n",
      "500 [D loss: 1.089142, acc.: 50.00%] [G loss: 3.694052]\n",
      "501 [D loss: 1.090363, acc.: 50.00%] [G loss: 1.986071]\n",
      "502 [D loss: 1.058884, acc.: 50.00%] [G loss: 4.318422]\n",
      "503 [D loss: 0.979415, acc.: 50.00%] [G loss: 2.813986]\n",
      "504 [D loss: 1.052688, acc.: 50.00%] [G loss: 1.957051]\n",
      "505 [D loss: 1.162612, acc.: 50.00%] [G loss: 4.310843]\n",
      "506 [D loss: 1.150865, acc.: 50.00%] [G loss: 4.304372]\n",
      "507 [D loss: 1.003692, acc.: 50.00%] [G loss: 4.609427]\n",
      "508 [D loss: 1.271078, acc.: 50.00%] [G loss: 3.278253]\n",
      "509 [D loss: 1.054043, acc.: 50.00%] [G loss: 4.433474]\n",
      "510 [D loss: 1.368359, acc.: 50.00%] [G loss: 2.801987]\n",
      "511 [D loss: 1.116768, acc.: 50.00%] [G loss: 2.220920]\n",
      "512 [D loss: 1.131099, acc.: 50.00%] [G loss: 3.208236]\n",
      "513 [D loss: 1.181532, acc.: 50.00%] [G loss: 4.122991]\n",
      "514 [D loss: 1.086007, acc.: 50.00%] [G loss: 3.207258]\n",
      "515 [D loss: 1.147730, acc.: 50.00%] [G loss: 3.708405]\n",
      "516 [D loss: 0.986143, acc.: 50.00%] [G loss: 3.645437]\n",
      "517 [D loss: 0.920799, acc.: 50.00%] [G loss: 5.581917]\n",
      "518 [D loss: 1.121572, acc.: 50.00%] [G loss: 5.134486]\n",
      "519 [D loss: 1.117380, acc.: 50.00%] [G loss: 2.351588]\n",
      "520 [D loss: 1.055047, acc.: 50.00%] [G loss: 4.278426]\n",
      "521 [D loss: 1.027465, acc.: 50.00%] [G loss: 2.424192]\n",
      "522 [D loss: 1.027413, acc.: 50.00%] [G loss: 3.683130]\n",
      "523 [D loss: 1.045777, acc.: 50.00%] [G loss: 2.923026]\n",
      "524 [D loss: 0.972701, acc.: 50.00%] [G loss: 2.174274]\n",
      "525 [D loss: 1.088008, acc.: 50.00%] [G loss: 3.028503]\n",
      "526 [D loss: 1.027963, acc.: 50.00%] [G loss: 5.045094]\n",
      "527 [D loss: 0.967225, acc.: 50.00%] [G loss: 2.891513]\n",
      "528 [D loss: 1.088939, acc.: 50.00%] [G loss: 2.138476]\n",
      "529 [D loss: 1.020354, acc.: 50.00%] [G loss: 4.097909]\n",
      "530 [D loss: 0.967304, acc.: 50.00%] [G loss: 2.295716]\n",
      "531 [D loss: 1.076668, acc.: 50.00%] [G loss: 3.486019]\n",
      "532 [D loss: 1.028853, acc.: 50.00%] [G loss: 4.563355]\n",
      "533 [D loss: 1.040650, acc.: 50.00%] [G loss: 3.947723]\n",
      "534 [D loss: 1.089844, acc.: 50.00%] [G loss: 5.449433]\n",
      "535 [D loss: 1.047112, acc.: 50.00%] [G loss: 3.337415]\n",
      "536 [D loss: 1.032825, acc.: 50.00%] [G loss: 3.311153]\n",
      "537 [D loss: 1.073132, acc.: 50.00%] [G loss: 3.638793]\n",
      "538 [D loss: 0.990680, acc.: 50.00%] [G loss: 4.203837]\n",
      "539 [D loss: 1.464520, acc.: 50.00%] [G loss: 3.081866]\n",
      "540 [D loss: 1.014976, acc.: 50.00%] [G loss: 2.238768]\n",
      "541 [D loss: 1.346191, acc.: 50.00%] [G loss: 3.840493]\n",
      "542 [D loss: 1.037824, acc.: 50.00%] [G loss: 4.013111]\n",
      "543 [D loss: 0.990525, acc.: 50.00%] [G loss: 2.496045]\n",
      "544 [D loss: 1.040794, acc.: 50.00%] [G loss: 4.077997]\n",
      "545 [D loss: 1.157048, acc.: 50.00%] [G loss: 4.817135]\n",
      "546 [D loss: 1.008958, acc.: 50.00%] [G loss: 4.643224]\n",
      "547 [D loss: 0.988400, acc.: 50.00%] [G loss: 4.630370]\n",
      "548 [D loss: 1.074029, acc.: 50.00%] [G loss: 3.157164]\n",
      "549 [D loss: 0.999114, acc.: 50.00%] [G loss: 2.508349]\n",
      "550 [D loss: 1.011596, acc.: 50.00%] [G loss: 4.107298]\n",
      "551 [D loss: 1.118977, acc.: 50.00%] [G loss: 4.811942]\n",
      "552 [D loss: 1.181602, acc.: 50.00%] [G loss: 2.840714]\n",
      "553 [D loss: 1.040414, acc.: 50.00%] [G loss: 1.991051]\n",
      "554 [D loss: 1.018230, acc.: 50.00%] [G loss: 3.166450]\n",
      "555 [D loss: 0.931223, acc.: 50.00%] [G loss: 4.610686]\n",
      "556 [D loss: 0.959916, acc.: 52.50%] [G loss: 3.682781]\n",
      "557 [D loss: 0.885422, acc.: 50.00%] [G loss: 3.664735]\n",
      "558 [D loss: 0.985133, acc.: 50.00%] [G loss: 2.627295]\n",
      "559 [D loss: 1.043097, acc.: 50.00%] [G loss: 3.887692]\n",
      "560 [D loss: 1.080858, acc.: 50.00%] [G loss: 5.650702]\n",
      "561 [D loss: 1.067542, acc.: 50.00%] [G loss: 3.387515]\n",
      "562 [D loss: 0.834503, acc.: 50.00%] [G loss: 2.674487]\n",
      "563 [D loss: 1.085503, acc.: 50.00%] [G loss: 3.997089]\n",
      "564 [D loss: 0.997522, acc.: 50.00%] [G loss: 2.493282]\n",
      "565 [D loss: 0.988346, acc.: 50.00%] [G loss: 4.138860]\n",
      "566 [D loss: 1.002626, acc.: 52.50%] [G loss: 2.781534]\n",
      "567 [D loss: 1.024866, acc.: 50.00%] [G loss: 4.272272]\n",
      "568 [D loss: 1.019517, acc.: 50.00%] [G loss: 3.758387]\n",
      "569 [D loss: 1.003356, acc.: 50.00%] [G loss: 2.677750]\n",
      "570 [D loss: 0.976512, acc.: 50.00%] [G loss: 4.022381]\n",
      "571 [D loss: 0.882024, acc.: 50.00%] [G loss: 3.392327]\n",
      "572 [D loss: 0.998010, acc.: 50.00%] [G loss: 2.370553]\n",
      "573 [D loss: 0.973125, acc.: 50.00%] [G loss: 3.576302]\n",
      "574 [D loss: 1.047876, acc.: 50.00%] [G loss: 3.883516]\n",
      "575 [D loss: 0.959936, acc.: 50.00%] [G loss: 4.184945]\n",
      "576 [D loss: 1.008139, acc.: 50.00%] [G loss: 5.368124]\n",
      "577 [D loss: 0.964099, acc.: 50.00%] [G loss: 2.227613]\n",
      "578 [D loss: 0.947881, acc.: 50.00%] [G loss: 2.916600]\n",
      "579 [D loss: 1.290875, acc.: 50.00%] [G loss: 3.205796]\n",
      "580 [D loss: 0.960755, acc.: 50.00%] [G loss: 3.380406]\n",
      "581 [D loss: 1.068209, acc.: 52.50%] [G loss: 2.071048]\n",
      "582 [D loss: 1.047042, acc.: 50.00%] [G loss: 3.530441]\n",
      "583 [D loss: 0.911452, acc.: 50.00%] [G loss: 3.407138]\n",
      "584 [D loss: 0.991590, acc.: 50.00%] [G loss: 2.606628]\n",
      "585 [D loss: 1.016065, acc.: 50.00%] [G loss: 2.328174]\n",
      "586 [D loss: 1.088810, acc.: 50.00%] [G loss: 4.980927]\n",
      "587 [D loss: 0.996745, acc.: 50.00%] [G loss: 3.497571]\n",
      "588 [D loss: 0.945677, acc.: 50.00%] [G loss: 3.441466]\n",
      "589 [D loss: 1.154333, acc.: 50.00%] [G loss: 4.290142]\n",
      "590 [D loss: 1.053046, acc.: 50.00%] [G loss: 4.051919]\n",
      "591 [D loss: 0.849843, acc.: 52.50%] [G loss: 3.673823]\n",
      "592 [D loss: 1.008907, acc.: 50.00%] [G loss: 2.766534]\n",
      "593 [D loss: 1.010091, acc.: 50.00%] [G loss: 3.725279]\n",
      "594 [D loss: 0.966748, acc.: 50.00%] [G loss: 4.408260]\n",
      "595 [D loss: 1.021971, acc.: 50.00%] [G loss: 4.993264]\n",
      "596 [D loss: 0.942894, acc.: 50.00%] [G loss: 4.220927]\n",
      "597 [D loss: 0.918962, acc.: 50.00%] [G loss: 4.725865]\n",
      "598 [D loss: 1.037599, acc.: 50.00%] [G loss: 5.532129]\n",
      "599 [D loss: 0.856095, acc.: 50.00%] [G loss: 4.600780]\n",
      "600 [D loss: 0.861241, acc.: 52.50%] [G loss: 8.776476]\n",
      "601 [D loss: 1.015880, acc.: 50.00%] [G loss: 4.305900]\n",
      "602 [D loss: 0.935080, acc.: 50.00%] [G loss: 3.020035]\n",
      "603 [D loss: 0.968350, acc.: 50.00%] [G loss: 4.873932]\n",
      "604 [D loss: 1.322912, acc.: 50.00%] [G loss: 4.060892]\n",
      "605 [D loss: 0.948501, acc.: 50.00%] [G loss: 3.680216]\n",
      "606 [D loss: 1.010799, acc.: 50.00%] [G loss: 2.544216]\n",
      "607 [D loss: 0.939805, acc.: 50.00%] [G loss: 2.884613]\n",
      "608 [D loss: 0.834507, acc.: 50.00%] [G loss: 2.704520]\n",
      "609 [D loss: 0.883287, acc.: 50.00%] [G loss: 3.325787]\n",
      "610 [D loss: 1.059568, acc.: 50.00%] [G loss: 3.547347]\n",
      "611 [D loss: 0.853516, acc.: 50.00%] [G loss: 3.466165]\n",
      "612 [D loss: 0.940488, acc.: 50.00%] [G loss: 5.083949]\n",
      "613 [D loss: 1.296206, acc.: 50.00%] [G loss: 5.751750]\n",
      "614 [D loss: 0.923907, acc.: 50.00%] [G loss: 4.580563]\n",
      "615 [D loss: 1.009706, acc.: 50.00%] [G loss: 6.833692]\n",
      "616 [D loss: 0.962975, acc.: 50.00%] [G loss: 4.062791]\n",
      "617 [D loss: 0.845985, acc.: 50.00%] [G loss: 4.778279]\n",
      "618 [D loss: 0.923658, acc.: 50.00%] [G loss: 4.750474]\n",
      "619 [D loss: 0.884218, acc.: 52.50%] [G loss: 3.320926]\n",
      "620 [D loss: 0.873040, acc.: 52.50%] [G loss: 3.362201]\n",
      "621 [D loss: 0.914474, acc.: 52.50%] [G loss: 3.842838]\n",
      "622 [D loss: 0.943958, acc.: 52.50%] [G loss: 5.808666]\n",
      "623 [D loss: 0.930421, acc.: 52.50%] [G loss: 4.294215]\n",
      "624 [D loss: 0.995049, acc.: 52.50%] [G loss: 5.488568]\n",
      "625 [D loss: 0.805563, acc.: 55.00%] [G loss: 3.275937]\n",
      "626 [D loss: 0.878785, acc.: 50.00%] [G loss: 4.213054]\n",
      "627 [D loss: 0.946508, acc.: 52.50%] [G loss: 5.215231]\n",
      "628 [D loss: 0.878673, acc.: 52.50%] [G loss: 4.845888]\n",
      "629 [D loss: 0.855374, acc.: 57.50%] [G loss: 2.667561]\n",
      "630 [D loss: 0.795139, acc.: 50.00%] [G loss: 3.570896]\n",
      "631 [D loss: 0.914176, acc.: 52.50%] [G loss: 5.446193]\n",
      "632 [D loss: 0.877869, acc.: 52.50%] [G loss: 4.942328]\n",
      "633 [D loss: 1.012677, acc.: 52.50%] [G loss: 4.974668]\n",
      "634 [D loss: 1.214610, acc.: 55.00%] [G loss: 3.188256]\n",
      "635 [D loss: 0.981440, acc.: 50.00%] [G loss: 2.931950]\n",
      "636 [D loss: 0.823338, acc.: 55.00%] [G loss: 4.944858]\n",
      "637 [D loss: 0.809811, acc.: 50.00%] [G loss: 4.068902]\n",
      "638 [D loss: 0.839298, acc.: 55.00%] [G loss: 4.391122]\n",
      "639 [D loss: 0.877287, acc.: 55.00%] [G loss: 3.798867]\n",
      "640 [D loss: 0.891898, acc.: 50.00%] [G loss: 4.231155]\n",
      "641 [D loss: 0.910220, acc.: 50.00%] [G loss: 4.606108]\n",
      "642 [D loss: 0.877319, acc.: 52.50%] [G loss: 3.785190]\n",
      "643 [D loss: 1.006192, acc.: 50.00%] [G loss: 2.704933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644 [D loss: 0.946612, acc.: 50.00%] [G loss: 3.501363]\n",
      "645 [D loss: 0.903752, acc.: 50.00%] [G loss: 2.303643]\n",
      "646 [D loss: 0.831078, acc.: 55.00%] [G loss: 5.460768]\n",
      "647 [D loss: 0.806139, acc.: 57.50%] [G loss: 4.377551]\n",
      "648 [D loss: 0.798134, acc.: 52.50%] [G loss: 2.319908]\n",
      "649 [D loss: 0.959806, acc.: 55.00%] [G loss: 4.101094]\n",
      "650 [D loss: 0.886448, acc.: 57.50%] [G loss: 3.803226]\n",
      "651 [D loss: 0.959564, acc.: 50.00%] [G loss: 3.901130]\n",
      "652 [D loss: 0.785155, acc.: 52.50%] [G loss: 3.909902]\n",
      "653 [D loss: 1.136843, acc.: 50.00%] [G loss: 2.612437]\n",
      "654 [D loss: 0.940009, acc.: 50.00%] [G loss: 3.163989]\n",
      "655 [D loss: 1.238528, acc.: 50.00%] [G loss: 4.365246]\n",
      "656 [D loss: 0.886760, acc.: 52.50%] [G loss: 4.364095]\n",
      "657 [D loss: 0.893481, acc.: 52.50%] [G loss: 4.241508]\n",
      "658 [D loss: 0.884482, acc.: 52.50%] [G loss: 4.573610]\n",
      "659 [D loss: 0.856914, acc.: 57.50%] [G loss: 4.484897]\n",
      "660 [D loss: 0.917758, acc.: 55.00%] [G loss: 3.128552]\n",
      "661 [D loss: 0.974150, acc.: 50.00%] [G loss: 4.168338]\n",
      "662 [D loss: 0.884509, acc.: 57.50%] [G loss: 4.299925]\n",
      "663 [D loss: 0.823753, acc.: 55.00%] [G loss: 3.235774]\n",
      "664 [D loss: 0.858790, acc.: 52.50%] [G loss: 3.834071]\n",
      "665 [D loss: 0.822122, acc.: 57.50%] [G loss: 2.649158]\n",
      "666 [D loss: 0.820185, acc.: 55.00%] [G loss: 2.532275]\n",
      "667 [D loss: 0.887295, acc.: 52.50%] [G loss: 3.673460]\n",
      "668 [D loss: 0.812362, acc.: 55.00%] [G loss: 2.927842]\n",
      "669 [D loss: 1.054333, acc.: 52.50%] [G loss: 4.202971]\n",
      "670 [D loss: 0.848761, acc.: 57.50%] [G loss: 3.075050]\n",
      "671 [D loss: 1.281362, acc.: 52.50%] [G loss: 3.595038]\n",
      "672 [D loss: 0.837408, acc.: 50.00%] [G loss: 1.995577]\n",
      "673 [D loss: 0.833086, acc.: 62.50%] [G loss: 2.660907]\n",
      "674 [D loss: 1.157124, acc.: 57.50%] [G loss: 3.002851]\n",
      "675 [D loss: 0.777559, acc.: 60.00%] [G loss: 2.797679]\n",
      "676 [D loss: 0.780579, acc.: 57.50%] [G loss: 2.958458]\n",
      "677 [D loss: 0.691809, acc.: 60.00%] [G loss: 2.778042]\n",
      "678 [D loss: 0.885515, acc.: 52.50%] [G loss: 2.399097]\n",
      "679 [D loss: 0.722307, acc.: 60.00%] [G loss: 3.697032]\n",
      "680 [D loss: 0.776403, acc.: 57.50%] [G loss: 4.066955]\n",
      "681 [D loss: 0.766966, acc.: 57.50%] [G loss: 3.237689]\n",
      "682 [D loss: 0.856655, acc.: 52.50%] [G loss: 2.614068]\n",
      "683 [D loss: 0.816173, acc.: 52.50%] [G loss: 2.390558]\n",
      "684 [D loss: 0.879717, acc.: 52.50%] [G loss: 3.293195]\n",
      "685 [D loss: 0.934361, acc.: 52.50%] [G loss: 4.417311]\n",
      "686 [D loss: 1.200207, acc.: 57.50%] [G loss: 4.994685]\n",
      "687 [D loss: 0.745746, acc.: 57.50%] [G loss: 3.098026]\n",
      "688 [D loss: 0.872084, acc.: 55.00%] [G loss: 3.782031]\n",
      "689 [D loss: 0.834410, acc.: 55.00%] [G loss: 4.189476]\n",
      "690 [D loss: 1.004671, acc.: 52.50%] [G loss: 3.612253]\n",
      "691 [D loss: 0.815114, acc.: 57.50%] [G loss: 3.435124]\n",
      "692 [D loss: 0.852050, acc.: 55.00%] [G loss: 2.606801]\n",
      "693 [D loss: 0.858138, acc.: 55.00%] [G loss: 5.208884]\n",
      "694 [D loss: 0.816931, acc.: 60.00%] [G loss: 3.418781]\n",
      "695 [D loss: 0.832096, acc.: 57.50%] [G loss: 3.994617]\n",
      "696 [D loss: 0.704855, acc.: 60.00%] [G loss: 2.612013]\n",
      "697 [D loss: 0.864410, acc.: 55.00%] [G loss: 3.729977]\n",
      "698 [D loss: 0.733879, acc.: 65.00%] [G loss: 2.720671]\n",
      "699 [D loss: 0.783559, acc.: 57.50%] [G loss: 2.533612]\n",
      "700 [D loss: 0.855461, acc.: 55.00%] [G loss: 3.353926]\n",
      "701 [D loss: 0.828808, acc.: 57.50%] [G loss: 2.572040]\n",
      "702 [D loss: 0.711962, acc.: 62.50%] [G loss: 2.708369]\n",
      "703 [D loss: 0.866508, acc.: 55.00%] [G loss: 3.518744]\n",
      "704 [D loss: 0.824752, acc.: 50.00%] [G loss: 4.414937]\n",
      "705 [D loss: 0.827628, acc.: 60.00%] [G loss: 3.506104]\n",
      "706 [D loss: 0.742785, acc.: 57.50%] [G loss: 2.849982]\n",
      "707 [D loss: 0.902906, acc.: 55.00%] [G loss: 2.741550]\n",
      "708 [D loss: 0.822068, acc.: 55.00%] [G loss: 5.304039]\n",
      "709 [D loss: 0.794822, acc.: 52.50%] [G loss: 3.235979]\n",
      "710 [D loss: 0.899260, acc.: 57.50%] [G loss: 3.665384]\n",
      "711 [D loss: 0.699504, acc.: 65.00%] [G loss: 4.425969]\n",
      "712 [D loss: 0.797130, acc.: 60.00%] [G loss: 3.817631]\n",
      "713 [D loss: 0.777099, acc.: 57.50%] [G loss: 3.020366]\n",
      "714 [D loss: 0.736064, acc.: 57.50%] [G loss: 1.551614]\n",
      "715 [D loss: 0.826609, acc.: 55.00%] [G loss: 4.128947]\n",
      "716 [D loss: 0.924323, acc.: 52.50%] [G loss: 3.659231]\n",
      "717 [D loss: 0.832261, acc.: 57.50%] [G loss: 5.230064]\n",
      "718 [D loss: 0.962375, acc.: 52.50%] [G loss: 3.907925]\n",
      "719 [D loss: 0.867991, acc.: 55.00%] [G loss: 3.322126]\n",
      "720 [D loss: 0.780024, acc.: 60.00%] [G loss: 3.155416]\n",
      "721 [D loss: 0.884540, acc.: 55.00%] [G loss: 2.878871]\n",
      "722 [D loss: 0.881884, acc.: 52.50%] [G loss: 3.123016]\n",
      "723 [D loss: 0.820503, acc.: 57.50%] [G loss: 2.659158]\n",
      "724 [D loss: 0.725747, acc.: 62.50%] [G loss: 3.758421]\n",
      "725 [D loss: 0.725108, acc.: 62.50%] [G loss: 2.399218]\n",
      "726 [D loss: 0.911870, acc.: 52.50%] [G loss: 3.499820]\n",
      "727 [D loss: 0.851722, acc.: 55.00%] [G loss: 4.192095]\n",
      "728 [D loss: 0.930039, acc.: 57.50%] [G loss: 1.986460]\n",
      "729 [D loss: 1.136636, acc.: 52.50%] [G loss: 2.508300]\n",
      "730 [D loss: 0.869464, acc.: 57.50%] [G loss: 4.734643]\n",
      "731 [D loss: 0.738305, acc.: 65.00%] [G loss: 5.172549]\n",
      "732 [D loss: 0.757008, acc.: 57.50%] [G loss: 4.046910]\n",
      "733 [D loss: 0.936909, acc.: 55.00%] [G loss: 4.696650]\n",
      "734 [D loss: 0.929649, acc.: 52.50%] [G loss: 3.784832]\n",
      "735 [D loss: 0.926835, acc.: 55.00%] [G loss: 4.245485]\n",
      "736 [D loss: 1.181422, acc.: 62.50%] [G loss: 3.174487]\n",
      "737 [D loss: 1.253617, acc.: 55.00%] [G loss: 3.147769]\n",
      "738 [D loss: 0.953144, acc.: 52.50%] [G loss: 3.046280]\n",
      "739 [D loss: 0.878630, acc.: 55.00%] [G loss: 3.079403]\n",
      "740 [D loss: 0.831680, acc.: 57.50%] [G loss: 2.194542]\n",
      "741 [D loss: 0.787053, acc.: 62.50%] [G loss: 4.261744]\n",
      "742 [D loss: 0.874087, acc.: 55.00%] [G loss: 3.285411]\n",
      "743 [D loss: 0.938211, acc.: 57.50%] [G loss: 1.644197]\n",
      "744 [D loss: 0.844263, acc.: 52.50%] [G loss: 3.195490]\n",
      "745 [D loss: 0.804702, acc.: 62.50%] [G loss: 1.886232]\n",
      "746 [D loss: 0.974565, acc.: 52.50%] [G loss: 1.875781]\n",
      "747 [D loss: 0.752792, acc.: 62.50%] [G loss: 3.883235]\n",
      "748 [D loss: 0.977529, acc.: 50.00%] [G loss: 3.193181]\n",
      "749 [D loss: 0.814315, acc.: 62.50%] [G loss: 3.314819]\n",
      "750 [D loss: 0.914620, acc.: 57.50%] [G loss: 2.625164]\n",
      "751 [D loss: 0.936501, acc.: 50.00%] [G loss: 4.233491]\n",
      "752 [D loss: 0.757273, acc.: 62.50%] [G loss: 2.622123]\n",
      "753 [D loss: 0.924962, acc.: 62.50%] [G loss: 1.326460]\n",
      "754 [D loss: 0.876931, acc.: 60.00%] [G loss: 3.687424]\n",
      "755 [D loss: 0.927496, acc.: 57.50%] [G loss: 4.684056]\n",
      "756 [D loss: 0.833791, acc.: 62.50%] [G loss: 3.012133]\n",
      "757 [D loss: 1.224800, acc.: 62.50%] [G loss: 2.012221]\n",
      "758 [D loss: 0.857910, acc.: 55.00%] [G loss: 3.012234]\n",
      "759 [D loss: 0.867754, acc.: 57.50%] [G loss: 1.718639]\n",
      "760 [D loss: 1.180920, acc.: 60.00%] [G loss: 2.667559]\n",
      "761 [D loss: 1.040415, acc.: 55.00%] [G loss: 2.328766]\n",
      "762 [D loss: 0.822189, acc.: 57.50%] [G loss: 3.331306]\n",
      "763 [D loss: 0.761098, acc.: 62.50%] [G loss: 3.333826]\n",
      "764 [D loss: 0.964952, acc.: 57.50%] [G loss: 2.124116]\n",
      "765 [D loss: 0.802522, acc.: 57.50%] [G loss: 4.300284]\n",
      "766 [D loss: 0.888501, acc.: 57.50%] [G loss: 2.629219]\n",
      "767 [D loss: 0.778471, acc.: 60.00%] [G loss: 4.654527]\n",
      "768 [D loss: 0.874098, acc.: 57.50%] [G loss: 3.083606]\n",
      "769 [D loss: 0.701614, acc.: 62.50%] [G loss: 6.893336]\n",
      "770 [D loss: 0.848547, acc.: 52.50%] [G loss: 3.425706]\n",
      "771 [D loss: 1.169037, acc.: 60.00%] [G loss: 3.698208]\n",
      "772 [D loss: 1.160166, acc.: 60.00%] [G loss: 3.074781]\n",
      "773 [D loss: 0.781563, acc.: 60.00%] [G loss: 3.174970]\n",
      "774 [D loss: 0.698249, acc.: 65.00%] [G loss: 3.287153]\n",
      "775 [D loss: 0.802621, acc.: 55.00%] [G loss: 2.864119]\n",
      "776 [D loss: 0.905833, acc.: 60.00%] [G loss: 2.921458]\n",
      "777 [D loss: 0.736807, acc.: 55.00%] [G loss: 3.218456]\n",
      "778 [D loss: 0.766979, acc.: 62.50%] [G loss: 3.368241]\n",
      "779 [D loss: 0.800583, acc.: 60.00%] [G loss: 3.188106]\n",
      "780 [D loss: 0.823065, acc.: 55.00%] [G loss: 2.247754]\n",
      "781 [D loss: 0.738735, acc.: 60.00%] [G loss: 4.310809]\n",
      "782 [D loss: 0.675416, acc.: 62.50%] [G loss: 2.359138]\n",
      "783 [D loss: 0.773484, acc.: 62.50%] [G loss: 3.708203]\n",
      "784 [D loss: 0.702386, acc.: 65.00%] [G loss: 1.091008]\n",
      "785 [D loss: 0.816686, acc.: 65.00%] [G loss: 2.325142]\n",
      "786 [D loss: 0.888267, acc.: 57.50%] [G loss: 2.906264]\n",
      "787 [D loss: 0.830415, acc.: 55.00%] [G loss: 3.700359]\n",
      "788 [D loss: 0.803141, acc.: 60.00%] [G loss: 3.951932]\n",
      "789 [D loss: 0.919166, acc.: 55.00%] [G loss: 3.385380]\n",
      "790 [D loss: 0.816233, acc.: 60.00%] [G loss: 2.866771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "791 [D loss: 0.794243, acc.: 62.50%] [G loss: 2.504639]\n",
      "792 [D loss: 0.812912, acc.: 57.50%] [G loss: 4.469030]\n",
      "793 [D loss: 1.316010, acc.: 52.50%] [G loss: 2.891165]\n",
      "794 [D loss: 0.968035, acc.: 57.50%] [G loss: 4.361262]\n",
      "795 [D loss: 0.921803, acc.: 55.00%] [G loss: 4.238760]\n",
      "796 [D loss: 0.784896, acc.: 57.50%] [G loss: 3.303508]\n",
      "797 [D loss: 0.831855, acc.: 65.00%] [G loss: 2.989004]\n",
      "798 [D loss: 0.923231, acc.: 60.00%] [G loss: 2.952878]\n",
      "799 [D loss: 0.930987, acc.: 55.00%] [G loss: 2.770823]\n",
      "800 [D loss: 0.719607, acc.: 62.50%] [G loss: 3.805974]\n",
      "801 [D loss: 0.749491, acc.: 57.50%] [G loss: 3.570810]\n",
      "802 [D loss: 0.724673, acc.: 67.50%] [G loss: 2.375461]\n",
      "803 [D loss: 0.897489, acc.: 55.00%] [G loss: 2.788605]\n",
      "804 [D loss: 0.729525, acc.: 65.00%] [G loss: 2.247434]\n",
      "805 [D loss: 0.822562, acc.: 65.00%] [G loss: 2.941476]\n",
      "806 [D loss: 0.924923, acc.: 52.50%] [G loss: 2.272722]\n",
      "807 [D loss: 0.802278, acc.: 62.50%] [G loss: 3.715699]\n",
      "808 [D loss: 0.933695, acc.: 55.00%] [G loss: 2.998704]\n",
      "809 [D loss: 0.798290, acc.: 55.00%] [G loss: 1.900674]\n",
      "810 [D loss: 0.892645, acc.: 57.50%] [G loss: 3.171241]\n",
      "811 [D loss: 0.718565, acc.: 62.50%] [G loss: 3.316857]\n",
      "812 [D loss: 0.922507, acc.: 65.00%] [G loss: 4.604486]\n",
      "813 [D loss: 0.827381, acc.: 60.00%] [G loss: 2.693229]\n",
      "814 [D loss: 0.801460, acc.: 67.50%] [G loss: 2.893384]\n",
      "815 [D loss: 0.828475, acc.: 65.00%] [G loss: 3.255255]\n",
      "816 [D loss: 0.739885, acc.: 65.00%] [G loss: 3.596420]\n",
      "817 [D loss: 0.738174, acc.: 62.50%] [G loss: 4.908846]\n",
      "818 [D loss: 0.680406, acc.: 67.50%] [G loss: 2.214143]\n",
      "819 [D loss: 0.811203, acc.: 55.00%] [G loss: 2.500332]\n",
      "820 [D loss: 1.081374, acc.: 57.50%] [G loss: 2.111466]\n",
      "821 [D loss: 1.082564, acc.: 62.50%] [G loss: 2.378192]\n",
      "822 [D loss: 0.717549, acc.: 65.00%] [G loss: 2.769922]\n",
      "823 [D loss: 0.671528, acc.: 67.50%] [G loss: 2.930700]\n",
      "824 [D loss: 0.780375, acc.: 65.00%] [G loss: 2.264038]\n",
      "825 [D loss: 0.820452, acc.: 55.00%] [G loss: 2.175533]\n",
      "826 [D loss: 0.774711, acc.: 62.50%] [G loss: 2.194758]\n",
      "827 [D loss: 0.804600, acc.: 60.00%] [G loss: 2.151977]\n",
      "828 [D loss: 0.721085, acc.: 65.00%] [G loss: 2.053318]\n",
      "829 [D loss: 0.781232, acc.: 62.50%] [G loss: 2.189304]\n",
      "830 [D loss: 0.728884, acc.: 62.50%] [G loss: 1.912876]\n",
      "831 [D loss: 0.772668, acc.: 65.00%] [G loss: 1.896383]\n",
      "832 [D loss: 0.675016, acc.: 70.00%] [G loss: 1.999689]\n",
      "833 [D loss: 0.765479, acc.: 67.50%] [G loss: 2.101202]\n",
      "834 [D loss: 0.704488, acc.: 60.00%] [G loss: 1.959388]\n",
      "835 [D loss: 0.784913, acc.: 65.00%] [G loss: 1.897480]\n",
      "836 [D loss: 0.678045, acc.: 62.50%] [G loss: 1.913225]\n",
      "837 [D loss: 0.745728, acc.: 62.50%] [G loss: 2.021329]\n",
      "838 [D loss: 0.763221, acc.: 62.50%] [G loss: 1.982152]\n",
      "839 [D loss: 0.774513, acc.: 60.00%] [G loss: 1.989817]\n",
      "840 [D loss: 1.815575, acc.: 60.00%] [G loss: 2.049554]\n",
      "841 [D loss: 0.735189, acc.: 65.00%] [G loss: 1.933688]\n",
      "842 [D loss: 0.902949, acc.: 60.00%] [G loss: 1.982128]\n",
      "843 [D loss: 0.684415, acc.: 72.50%] [G loss: 2.070910]\n",
      "844 [D loss: 0.712277, acc.: 62.50%] [G loss: 1.781839]\n",
      "845 [D loss: 1.568811, acc.: 57.50%] [G loss: 1.800585]\n",
      "846 [D loss: 1.071154, acc.: 52.50%] [G loss: 1.915927]\n",
      "847 [D loss: 0.897984, acc.: 55.00%] [G loss: 1.982295]\n",
      "848 [D loss: 1.046634, acc.: 60.00%] [G loss: 1.915297]\n",
      "849 [D loss: 0.576255, acc.: 72.50%] [G loss: 2.049219]\n",
      "850 [D loss: 0.655205, acc.: 67.50%] [G loss: 2.048898]\n",
      "851 [D loss: 0.832359, acc.: 62.50%] [G loss: 2.133959]\n",
      "852 [D loss: 0.717507, acc.: 60.00%] [G loss: 1.970118]\n",
      "853 [D loss: 0.652850, acc.: 67.50%] [G loss: 1.902652]\n",
      "854 [D loss: 0.638101, acc.: 77.50%] [G loss: 1.926371]\n",
      "855 [D loss: 0.828141, acc.: 60.00%] [G loss: 1.877590]\n",
      "856 [D loss: 0.806056, acc.: 62.50%] [G loss: 1.853630]\n",
      "857 [D loss: 0.689871, acc.: 65.00%] [G loss: 1.976520]\n",
      "858 [D loss: 0.550936, acc.: 75.00%] [G loss: 1.973126]\n",
      "859 [D loss: 0.699774, acc.: 62.50%] [G loss: 2.012638]\n",
      "860 [D loss: 0.905587, acc.: 77.50%] [G loss: 2.056417]\n",
      "861 [D loss: 0.720802, acc.: 70.00%] [G loss: 1.911729]\n",
      "862 [D loss: 0.670901, acc.: 65.00%] [G loss: 1.921044]\n",
      "863 [D loss: 1.089954, acc.: 62.50%] [G loss: 1.925687]\n",
      "864 [D loss: 0.742186, acc.: 62.50%] [G loss: 1.927473]\n",
      "865 [D loss: 0.885314, acc.: 60.00%] [G loss: 1.979026]\n",
      "866 [D loss: 0.712328, acc.: 67.50%] [G loss: 2.038759]\n",
      "867 [D loss: 0.780854, acc.: 57.50%] [G loss: 1.985366]\n",
      "868 [D loss: 0.655594, acc.: 70.00%] [G loss: 1.786712]\n",
      "869 [D loss: 0.736453, acc.: 65.00%] [G loss: 1.830558]\n",
      "870 [D loss: 0.852457, acc.: 57.50%] [G loss: 1.827785]\n",
      "871 [D loss: 0.630818, acc.: 70.00%] [G loss: 1.975559]\n",
      "872 [D loss: 0.676123, acc.: 72.50%] [G loss: 1.999267]\n",
      "873 [D loss: 0.633335, acc.: 72.50%] [G loss: 1.876425]\n",
      "874 [D loss: 0.842178, acc.: 62.50%] [G loss: 1.960095]\n",
      "875 [D loss: 0.830110, acc.: 60.00%] [G loss: 1.995397]\n",
      "876 [D loss: 0.649157, acc.: 67.50%] [G loss: 1.895518]\n",
      "877 [D loss: 0.659408, acc.: 70.00%] [G loss: 1.994834]\n",
      "878 [D loss: 0.685630, acc.: 65.00%] [G loss: 1.876657]\n",
      "879 [D loss: 0.735335, acc.: 70.00%] [G loss: 1.867083]\n",
      "880 [D loss: 0.732822, acc.: 70.00%] [G loss: 1.894312]\n",
      "881 [D loss: 0.769344, acc.: 62.50%] [G loss: 1.926476]\n",
      "882 [D loss: 0.782791, acc.: 62.50%] [G loss: 1.831093]\n",
      "883 [D loss: 0.661370, acc.: 67.50%] [G loss: 2.045386]\n",
      "884 [D loss: 0.694202, acc.: 67.50%] [G loss: 1.903846]\n",
      "885 [D loss: 0.755857, acc.: 67.50%] [G loss: 1.872681]\n",
      "886 [D loss: 0.698918, acc.: 72.50%] [G loss: 1.968676]\n",
      "887 [D loss: 0.650835, acc.: 72.50%] [G loss: 1.920213]\n",
      "888 [D loss: 0.648931, acc.: 72.50%] [G loss: 1.885525]\n",
      "889 [D loss: 0.767352, acc.: 65.00%] [G loss: 1.793004]\n",
      "890 [D loss: 0.649377, acc.: 70.00%] [G loss: 1.886504]\n",
      "891 [D loss: 0.761707, acc.: 67.50%] [G loss: 1.932705]\n",
      "892 [D loss: 0.818285, acc.: 65.00%] [G loss: 1.877666]\n",
      "893 [D loss: 0.686008, acc.: 67.50%] [G loss: 1.873105]\n",
      "894 [D loss: 0.789090, acc.: 62.50%] [G loss: 1.882239]\n",
      "895 [D loss: 0.664640, acc.: 67.50%] [G loss: 1.779373]\n",
      "896 [D loss: 0.750930, acc.: 65.00%] [G loss: 1.919818]\n",
      "897 [D loss: 0.697833, acc.: 67.50%] [G loss: 1.884235]\n",
      "898 [D loss: 1.054240, acc.: 70.00%] [G loss: 1.736327]\n",
      "899 [D loss: 0.777587, acc.: 65.00%] [G loss: 1.895151]\n",
      "900 [D loss: 0.713731, acc.: 67.50%] [G loss: 1.797821]\n",
      "901 [D loss: 0.478683, acc.: 80.00%] [G loss: 1.879415]\n",
      "902 [D loss: 0.843893, acc.: 57.50%] [G loss: 1.907045]\n",
      "903 [D loss: 0.661542, acc.: 70.00%] [G loss: 1.778595]\n",
      "904 [D loss: 0.679141, acc.: 70.00%] [G loss: 1.911832]\n",
      "905 [D loss: 0.677650, acc.: 65.00%] [G loss: 1.993462]\n",
      "906 [D loss: 0.677414, acc.: 70.00%] [G loss: 1.878928]\n",
      "907 [D loss: 0.638192, acc.: 72.50%] [G loss: 1.897601]\n",
      "908 [D loss: 0.716864, acc.: 62.50%] [G loss: 1.821355]\n",
      "909 [D loss: 0.467940, acc.: 77.50%] [G loss: 1.662516]\n",
      "910 [D loss: 0.769280, acc.: 60.00%] [G loss: 1.876278]\n",
      "911 [D loss: 0.564326, acc.: 75.00%] [G loss: 2.016143]\n",
      "912 [D loss: 1.154060, acc.: 60.00%] [G loss: 1.838173]\n",
      "913 [D loss: 0.688182, acc.: 70.00%] [G loss: 1.823000]\n",
      "914 [D loss: 1.026654, acc.: 67.50%] [G loss: 1.847219]\n",
      "915 [D loss: 0.777806, acc.: 62.50%] [G loss: 2.019454]\n",
      "916 [D loss: 0.803958, acc.: 60.00%] [G loss: 1.800141]\n",
      "917 [D loss: 0.800263, acc.: 62.50%] [G loss: 1.885534]\n",
      "918 [D loss: 0.572549, acc.: 72.50%] [G loss: 1.842065]\n",
      "919 [D loss: 0.669357, acc.: 65.00%] [G loss: 1.853240]\n",
      "920 [D loss: 0.623738, acc.: 70.00%] [G loss: 1.832849]\n",
      "921 [D loss: 0.556071, acc.: 72.50%] [G loss: 1.883408]\n",
      "922 [D loss: 0.737102, acc.: 65.00%] [G loss: 1.874933]\n",
      "923 [D loss: 0.881825, acc.: 55.00%] [G loss: 1.850311]\n",
      "924 [D loss: 0.618501, acc.: 67.50%] [G loss: 1.851051]\n",
      "925 [D loss: 0.763872, acc.: 67.50%] [G loss: 1.776146]\n",
      "926 [D loss: 0.694380, acc.: 70.00%] [G loss: 1.716520]\n",
      "927 [D loss: 0.765240, acc.: 62.50%] [G loss: 1.861521]\n",
      "928 [D loss: 0.581723, acc.: 67.50%] [G loss: 1.934366]\n",
      "929 [D loss: 0.684308, acc.: 65.00%] [G loss: 1.777655]\n",
      "930 [D loss: 1.019914, acc.: 72.50%] [G loss: 1.745831]\n",
      "931 [D loss: 0.745747, acc.: 65.00%] [G loss: 1.668301]\n",
      "932 [D loss: 0.788522, acc.: 60.00%] [G loss: 1.929291]\n",
      "933 [D loss: 0.674312, acc.: 67.50%] [G loss: 1.830046]\n",
      "934 [D loss: 0.730981, acc.: 60.00%] [G loss: 1.762964]\n",
      "935 [D loss: 0.976049, acc.: 72.50%] [G loss: 1.854632]\n",
      "936 [D loss: 1.112833, acc.: 62.50%] [G loss: 1.875050]\n",
      "937 [D loss: 0.516563, acc.: 70.00%] [G loss: 1.871003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938 [D loss: 0.593722, acc.: 72.50%] [G loss: 1.818428]\n",
      "939 [D loss: 0.741051, acc.: 65.00%] [G loss: 1.808007]\n",
      "940 [D loss: 0.660037, acc.: 67.50%] [G loss: 1.904996]\n",
      "941 [D loss: 0.644459, acc.: 70.00%] [G loss: 1.811475]\n",
      "942 [D loss: 0.597174, acc.: 77.50%] [G loss: 1.759716]\n",
      "943 [D loss: 0.666529, acc.: 70.00%] [G loss: 1.764413]\n",
      "944 [D loss: 0.706877, acc.: 67.50%] [G loss: 1.958035]\n",
      "945 [D loss: 0.648003, acc.: 70.00%] [G loss: 1.848740]\n",
      "946 [D loss: 0.718055, acc.: 65.00%] [G loss: 1.797130]\n",
      "947 [D loss: 0.774425, acc.: 60.00%] [G loss: 1.869451]\n",
      "948 [D loss: 0.736901, acc.: 62.50%] [G loss: 1.824641]\n",
      "949 [D loss: 0.989089, acc.: 72.50%] [G loss: 1.740545]\n",
      "950 [D loss: 0.750483, acc.: 62.50%] [G loss: 1.870606]\n",
      "951 [D loss: 0.684431, acc.: 65.00%] [G loss: 1.851765]\n",
      "952 [D loss: 0.689115, acc.: 72.50%] [G loss: 1.878206]\n",
      "953 [D loss: 0.800226, acc.: 57.50%] [G loss: 1.942618]\n",
      "954 [D loss: 0.598981, acc.: 72.50%] [G loss: 1.810063]\n",
      "955 [D loss: 0.718693, acc.: 65.00%] [G loss: 1.930860]\n",
      "956 [D loss: 0.631883, acc.: 72.50%] [G loss: 1.826400]\n",
      "957 [D loss: 0.653953, acc.: 70.00%] [G loss: 1.763346]\n",
      "958 [D loss: 0.698519, acc.: 65.00%] [G loss: 1.804486]\n",
      "959 [D loss: 0.697603, acc.: 65.00%] [G loss: 1.852607]\n",
      "960 [D loss: 0.639164, acc.: 70.00%] [G loss: 1.859843]\n",
      "961 [D loss: 1.380419, acc.: 67.50%] [G loss: 1.754497]\n",
      "962 [D loss: 0.650068, acc.: 70.00%] [G loss: 1.861608]\n",
      "963 [D loss: 0.808651, acc.: 60.00%] [G loss: 1.740374]\n",
      "964 [D loss: 0.666642, acc.: 70.00%] [G loss: 1.713919]\n",
      "965 [D loss: 0.510300, acc.: 75.00%] [G loss: 1.753486]\n",
      "966 [D loss: 1.040348, acc.: 70.00%] [G loss: 1.791154]\n",
      "967 [D loss: 0.982756, acc.: 72.50%] [G loss: 1.901316]\n",
      "968 [D loss: 0.579306, acc.: 72.50%] [G loss: 1.841638]\n",
      "969 [D loss: 0.825738, acc.: 60.00%] [G loss: 1.839900]\n",
      "970 [D loss: 0.613474, acc.: 72.50%] [G loss: 1.871057]\n",
      "971 [D loss: 0.684150, acc.: 65.00%] [G loss: 1.767426]\n",
      "972 [D loss: 0.732988, acc.: 67.50%] [G loss: 1.794886]\n",
      "973 [D loss: 0.690858, acc.: 72.50%] [G loss: 1.829623]\n",
      "974 [D loss: 0.896975, acc.: 55.00%] [G loss: 1.873955]\n",
      "975 [D loss: 0.699790, acc.: 67.50%] [G loss: 1.800626]\n",
      "976 [D loss: 0.603949, acc.: 72.50%] [G loss: 1.763734]\n",
      "977 [D loss: 0.624872, acc.: 72.50%] [G loss: 1.745907]\n",
      "978 [D loss: 0.565605, acc.: 70.00%] [G loss: 1.677528]\n",
      "979 [D loss: 0.629822, acc.: 70.00%] [G loss: 1.787996]\n",
      "980 [D loss: 0.796362, acc.: 65.00%] [G loss: 1.967951]\n",
      "981 [D loss: 0.530230, acc.: 72.50%] [G loss: 1.818818]\n",
      "982 [D loss: 0.652556, acc.: 67.50%] [G loss: 1.810164]\n",
      "983 [D loss: 0.951902, acc.: 72.50%] [G loss: 1.728162]\n",
      "984 [D loss: 0.465335, acc.: 75.00%] [G loss: 1.771122]\n",
      "985 [D loss: 0.646689, acc.: 70.00%] [G loss: 1.824743]\n",
      "986 [D loss: 1.156884, acc.: 57.50%] [G loss: 1.695535]\n",
      "987 [D loss: 0.980335, acc.: 67.50%] [G loss: 1.767554]\n",
      "988 [D loss: 0.703394, acc.: 70.00%] [G loss: 1.896323]\n",
      "989 [D loss: 0.756303, acc.: 62.50%] [G loss: 1.865489]\n",
      "990 [D loss: 0.707548, acc.: 67.50%] [G loss: 1.864657]\n",
      "991 [D loss: 0.476090, acc.: 80.00%] [G loss: 1.890771]\n",
      "992 [D loss: 0.648053, acc.: 67.50%] [G loss: 1.993576]\n",
      "993 [D loss: 0.614083, acc.: 75.00%] [G loss: 1.936610]\n",
      "994 [D loss: 0.485401, acc.: 77.50%] [G loss: 1.816609]\n",
      "995 [D loss: 0.558538, acc.: 77.50%] [G loss: 1.831280]\n",
      "996 [D loss: 0.623791, acc.: 70.00%] [G loss: 1.796676]\n",
      "997 [D loss: 0.713723, acc.: 67.50%] [G loss: 1.755225]\n",
      "998 [D loss: 0.630246, acc.: 70.00%] [G loss: 1.885166]\n",
      "999 [D loss: 0.805240, acc.: 65.00%] [G loss: 1.746712]\n"
     ]
    }
   ],
   "source": [
    "lstmgan = LSTMGAN()\n",
    "lstmgan.train(epochs=1000, batch_size=20, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0h 34m 16s\n"
     ]
    }
   ],
   "source": [
    "end = datetime.datetime.now()\n",
    "\n",
    "diff = (end - start)\n",
    "\n",
    "datetime.timedelta(seconds=10, microseconds=885206)\n",
    "\n",
    "diff_seconds = int(diff.total_seconds())\n",
    "\n",
    "minute_seconds, seconds = divmod(diff_seconds, 60)\n",
    "hours, minutes = divmod(minute_seconds, 60)\n",
    "hms = f\"{hours}h {minutes}m {seconds}s\"\n",
    "\n",
    "'0h 0m 10s'\n",
    "print(hms) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lstmgan.train(epochs=1000, batch_size=20, save_interval=100)\n",
    "\n",
    "FailedPreconditionError:  Error while reading resource variable _AnonymousVar401 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar401/class tensorflow::Var does not exist.\n",
    "\t [[node mul_296/ReadVariableOp (defined at C:\\Users\\Admin\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_55197]\n",
    "\n",
    "Function call stack:\n",
    "keras_scratch_graph\n",
    "\n",
    "FailedPreconditionError reSOLVED BY\n",
    "\n",
    "#https://github.com/keras-team/keras/issues/13550\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "like this\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[0.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
      "         0.000e+00]],\n",
      "\n",
      "       [[0.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
      "         0.000e+00]],\n",
      "\n",
      "       [[0.000e+00, 1.488e-06, 0.000e+00, ..., 0.000e+00, 1.000e+00,\n",
      "         0.000e+00]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.000e+00, 1.488e-06, 0.000e+00, ..., 0.000e+00, 1.000e+00,\n",
      "         0.000e+00]],\n",
      "\n",
      "       [[0.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
      "         0.000e+00]],\n",
      "\n",
      "       [[0.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
      "         0.000e+00]]], dtype=float32)]\n",
      "(1, 40000, 1, 118)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "hidden_layers = keras.backend.function(\n",
    "[lstmgan.discriminator.layers[0].input],  # we will feed the function with the input of the first layer  \n",
    "[lstmgan.discriminator.layers[0].output,] # we want to get the output of the first layer\n",
    ")\n",
    "h=hidden_layers([df_train])\n",
    "print(h)\n",
    "print(np.shape(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=np.array(h)  \n",
    "\n",
    "arr_reshaped = np.array(h).reshape(40000, df_train.shape[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 118)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(arr_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('abnormal1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.573 0.035 1.\n",
      "   1.    0.    0.    0.06  0.06  0.    1.    0.071 0.07  0.07  0.\n",
      "   0.    1.    1.    0.    0.    0.    1.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    1.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    1.    0.    0.    0.    0.    0.   ]]]\n"
     ]
    }
   ],
   "source": [
    "#To check 1st row\n",
    "print(h[:,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_1 = pd.read_csv('abnormal1.csv')\n",
    "csv_2 = pd.read_csv('abnormallabel1.csv')\n",
    "\n",
    "result = pd.concat([csv_1, csv_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"lgcabnormalresult.csv\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ddda88a788>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['gen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ddda926488>,\n",
       " <matplotlib.lines.Line2D at 0x2ddda9265c8>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['dis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-eda2b5cc1a42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import RFE\n",
    "import itertools\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(train_x,train_y,train_size=0.70, random_state=2)\n",
    "\n",
    "train_x.shape\n",
    "\n",
    "# create the RFE model and select 10 attributes\n",
    "rfe = RFE(rfc, n_features_to_select=41)\n",
    "rfe = rfe.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "\n",
    "ac = accuracy_score(Y_test,rfe.predict(X_test))\n",
    "print('Accuracy is: ',ac)\n",
    "cm = confusion_matrix(Y_test,rfe.predict(X_test))\n",
    "sns.heatmap(cm,annot=True,fmt=\"d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
