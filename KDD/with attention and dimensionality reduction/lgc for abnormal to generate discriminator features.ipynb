{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/function9/bidirectional-lstm-gan-music-generation\n",
    "#https://www.kaggle.com/abhisheksinha28/bidirectional-lstm/data\n",
    "#https://www.kaggle.com/sekfook97/gan-for-anomaly-detection\n",
    "# importing libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Bidirectional, LSTM, Reshape, RepeatVector, TimeDistributed\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "\"feature1\": np.float16,\n",
    "\"feature2\": np.float16,\n",
    "\"feature3\": np.float16,\n",
    "\"feature4\": np.float16,\n",
    "\"feature5\": np.float16,\n",
    "\"feature6\": np.float16,\n",
    "\"feature7\": np.float16,\n",
    "\"feature8\": np.float16,\n",
    "\"label\": np.object}    \n",
    "columns = [\"feature1\",\"feature2\",\"feature3\",\"feature4\",\"feature5\",\"feature6\",\"feature7\",\"feature8\",\"feature9\",\"feature10\",\"feature11\",\"feature12\",\"feature13\",\"feature14\",\"feature15\",\"feature16\",\"feature17\",\"feature18\",\"feature19\",\"feature20\",\"label\"]\n",
    "#df = pd.read_csv(\"/kaggle/input/kdd-cup-1999-data/kddcup.data_10_percent_corrected\", sep=\",\", names=columns, dtype=dtypes, index_col=None)\n",
    "df = pd.read_csv(r\"C:\\Users\\admin\\ablation study - 20 neurons\\afterdimensionalityreduction\",sep=\",\", names=columns, dtype=str, index_col=None)\n",
    "#df_read = pd.read_csv(savefile, dtype=str, index_col=0)\n",
    "#print(type(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal      595798\n",
       "Abnormal    452778\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection\n",
    "new_features=[\"feature1\",\"feature2\",\"feature3\",\"feature4\",\"feature5\",\"feature6\",\"feature7\",\"feature8\",\"feature9\",\"feature10\",\"feature11\",\"feature12\",\"feature13\",\"feature14\",\"feature15\",\"feature16\",\"feature17\",\"feature18\",\"feature19\",\"feature20\",\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 for normal data and 1 for abnormalities\n",
    "df.label=df.label.apply(lambda x: 0 if x == 'Normal' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048576, 21)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[new_features]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#label encoding \n",
    "for column in df.columns:\n",
    "    if df[column].dtype == np.object:\n",
    "        encoded = LabelEncoder()\n",
    "        \n",
    "        encoded.fit(df[column])\n",
    "        df[column] = encoded.transform(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             feature1               feature2 feature3     feature4  \\\n",
       "0        0.049896017                    0.0      0.0  0.074576885   \n",
       "1         0.05703238                    0.0      0.0          0.0   \n",
       "2         0.05211395  0.0006721932400000001      0.0          0.0   \n",
       "3         0.04974308            0.003009851      0.0          0.0   \n",
       "4         0.04832898              0.0042979      0.0          0.0   \n",
       "...              ...                    ...      ...          ...   \n",
       "1048571  0.023582537            0.011685466      0.0          0.0   \n",
       "1048572  0.023108114             0.01054538      0.0          0.0   \n",
       "1048573  0.022631112            0.008008906      0.0          0.0   \n",
       "1048574  0.022127744            0.008818104      0.0          0.0   \n",
       "1048575   0.02162299   0.008149143000000001      0.0          0.0   \n",
       "\n",
       "             feature5              feature6 feature7              feature8  \\\n",
       "0                 0.0          0.0035594553      0.0           0.012225969   \n",
       "1         0.052905086           0.043763362      0.0                   0.0   \n",
       "2          0.01591806            0.02435071      0.0          0.0016837418   \n",
       "3        0.0029502455  0.017709356000000002      0.0          0.0066348887   \n",
       "4                 0.0           0.014554087      0.0           0.009138886   \n",
       "...               ...                   ...      ...                   ...   \n",
       "1048571           0.0           0.005893277      0.0           0.046702452   \n",
       "1048572           0.0          0.0058220904      0.0            0.04695243   \n",
       "1048573           0.0           0.005821051      0.0  0.047111400000000005   \n",
       "1048574           0.0           0.005707981      0.0           0.047399394   \n",
       "1048575           0.0  0.005666818499999999      0.0  0.047594800000000013   \n",
       "\n",
       "        feature9    feature10  ... feature12 feature13 feature14    feature15  \\\n",
       "0            0.0  0.019287573  ...       0.0       0.0       0.0          0.0   \n",
       "1            0.0  0.031135669  ...       0.0       0.0       0.0  0.006044184   \n",
       "2            0.0   0.02647767  ...       0.0       0.0       0.0          0.0   \n",
       "3            0.0  0.025008397  ...       0.0       0.0       0.0          0.0   \n",
       "4            0.0  0.024143761  ...       0.0       0.0       0.0          0.0   \n",
       "...          ...          ...  ...       ...       ...       ...          ...   \n",
       "1048571      0.0          0.0  ...       0.0       0.0       0.0  0.015696432   \n",
       "1048572      0.0          0.0  ...       0.0       0.0       0.0  0.015654337   \n",
       "1048573      0.0          0.0  ...       0.0       0.0       0.0  0.015895598   \n",
       "1048574      0.0          0.0  ...       0.0       0.0       0.0  0.015462209   \n",
       "1048575      0.0          0.0  ...       0.0       0.0       0.0  0.015328575   \n",
       "\n",
       "        feature16   feature17 feature18 feature19 feature20 label  \n",
       "0             0.0  0.06614828       0.0       0.0       0.0     0  \n",
       "1             0.0         0.0       0.0       0.0       0.0     0  \n",
       "2             0.0         0.0       0.0       0.0       0.0     0  \n",
       "3             0.0         0.0       0.0       0.0       0.0     0  \n",
       "4             0.0         0.0       0.0       0.0       0.0     0  \n",
       "...           ...         ...       ...       ...       ...   ...  \n",
       "1048571       0.0         0.0       0.0       0.0       0.0     0  \n",
       "1048572       0.0         0.0       0.0       0.0       0.0     0  \n",
       "1048573       0.0         0.0       0.0       0.0       0.0     0  \n",
       "1048574       0.0         0.0       0.0       0.0       0.0     0  \n",
       "1048575       0.0         0.0       0.0       0.0       0.0     0  \n",
       "\n",
       "[1048576 rows x 21 columns]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.where(df.values >= np.finfo(np.float64).max)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df=df.replace(np.inf, np.nan)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#df=df[~df.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "#new_dataframe = df_train[~np.isinf(df_train),8]\n",
    "df=df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             feature1               feature2 feature3     feature4  \\\n",
       "0        0.049896017                    0.0      0.0  0.074576885   \n",
       "1         0.05703238                    0.0      0.0          0.0   \n",
       "2         0.05211395  0.0006721932400000001      0.0          0.0   \n",
       "3         0.04974308            0.003009851      0.0          0.0   \n",
       "4         0.04832898              0.0042979      0.0          0.0   \n",
       "...              ...                    ...      ...          ...   \n",
       "1048571  0.023582537            0.011685466      0.0          0.0   \n",
       "1048572  0.023108114             0.01054538      0.0          0.0   \n",
       "1048573  0.022631112            0.008008906      0.0          0.0   \n",
       "1048574  0.022127744            0.008818104      0.0          0.0   \n",
       "1048575   0.02162299   0.008149143000000001      0.0          0.0   \n",
       "\n",
       "             feature5              feature6 feature7              feature8  \\\n",
       "0                 0.0          0.0035594553      0.0           0.012225969   \n",
       "1         0.052905086           0.043763362      0.0                   0.0   \n",
       "2          0.01591806            0.02435071      0.0          0.0016837418   \n",
       "3        0.0029502455  0.017709356000000002      0.0          0.0066348887   \n",
       "4                 0.0           0.014554087      0.0           0.009138886   \n",
       "...               ...                   ...      ...                   ...   \n",
       "1048571           0.0           0.005893277      0.0           0.046702452   \n",
       "1048572           0.0          0.0058220904      0.0            0.04695243   \n",
       "1048573           0.0           0.005821051      0.0  0.047111400000000005   \n",
       "1048574           0.0           0.005707981      0.0           0.047399394   \n",
       "1048575           0.0  0.005666818499999999      0.0  0.047594800000000013   \n",
       "\n",
       "        feature9    feature10  ... feature12 feature13 feature14    feature15  \\\n",
       "0            0.0  0.019287573  ...       0.0       0.0       0.0          0.0   \n",
       "1            0.0  0.031135669  ...       0.0       0.0       0.0  0.006044184   \n",
       "2            0.0   0.02647767  ...       0.0       0.0       0.0          0.0   \n",
       "3            0.0  0.025008397  ...       0.0       0.0       0.0          0.0   \n",
       "4            0.0  0.024143761  ...       0.0       0.0       0.0          0.0   \n",
       "...          ...          ...  ...       ...       ...       ...          ...   \n",
       "1048571      0.0          0.0  ...       0.0       0.0       0.0  0.015696432   \n",
       "1048572      0.0          0.0  ...       0.0       0.0       0.0  0.015654337   \n",
       "1048573      0.0          0.0  ...       0.0       0.0       0.0  0.015895598   \n",
       "1048574      0.0          0.0  ...       0.0       0.0       0.0  0.015462209   \n",
       "1048575      0.0          0.0  ...       0.0       0.0       0.0  0.015328575   \n",
       "\n",
       "        feature16   feature17 feature18 feature19 feature20 label  \n",
       "0             0.0  0.06614828       0.0       0.0       0.0     0  \n",
       "1             0.0         0.0       0.0       0.0       0.0     0  \n",
       "2             0.0         0.0       0.0       0.0       0.0     0  \n",
       "3             0.0         0.0       0.0       0.0       0.0     0  \n",
       "4             0.0         0.0       0.0       0.0       0.0     0  \n",
       "...           ...         ...       ...       ...       ...   ...  \n",
       "1048571       0.0         0.0       0.0       0.0       0.0     0  \n",
       "1048572       0.0         0.0       0.0       0.0       0.0     0  \n",
       "1048573       0.0         0.0       0.0       0.0       0.0     0  \n",
       "1048574       0.0         0.0       0.0       0.0       0.0     0  \n",
       "1048575       0.0         0.0       0.0       0.0       0.0     0  \n",
       "\n",
       "[1048576 rows x 21 columns]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Helper function for scaling continous values\n",
    "def minmax_scale_values(df, column):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler = scaler.fit(df[column].values.reshape(-1, 1))\n",
    "    df_values_standardized = scaler.transform(df[column].values.reshape(-1, 1))\n",
    "    df = df_values_standardized\n",
    "    #test_values_standardized = scaler.transform(test[col_name].values.reshape(-1, 1))\n",
    "    #test[col_name] = test_values_standardized"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for column in df.columns:\n",
    "    minmax_scale_values(df, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048576, 21)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             feature1               feature2 feature3     feature4  \\\n",
       "0        0.049896017                    0.0      0.0  0.074576885   \n",
       "1         0.05703238                    0.0      0.0          0.0   \n",
       "2         0.05211395  0.0006721932400000001      0.0          0.0   \n",
       "3         0.04974308            0.003009851      0.0          0.0   \n",
       "4         0.04832898              0.0042979      0.0          0.0   \n",
       "...              ...                    ...      ...          ...   \n",
       "1048571  0.023582537            0.011685466      0.0          0.0   \n",
       "1048572  0.023108114             0.01054538      0.0          0.0   \n",
       "1048573  0.022631112            0.008008906      0.0          0.0   \n",
       "1048574  0.022127744            0.008818104      0.0          0.0   \n",
       "1048575   0.02162299   0.008149143000000001      0.0          0.0   \n",
       "\n",
       "             feature5              feature6 feature7              feature8  \\\n",
       "0                 0.0          0.0035594553      0.0           0.012225969   \n",
       "1         0.052905086           0.043763362      0.0                   0.0   \n",
       "2          0.01591806            0.02435071      0.0          0.0016837418   \n",
       "3        0.0029502455  0.017709356000000002      0.0          0.0066348887   \n",
       "4                 0.0           0.014554087      0.0           0.009138886   \n",
       "...               ...                   ...      ...                   ...   \n",
       "1048571           0.0           0.005893277      0.0           0.046702452   \n",
       "1048572           0.0          0.0058220904      0.0            0.04695243   \n",
       "1048573           0.0           0.005821051      0.0  0.047111400000000005   \n",
       "1048574           0.0           0.005707981      0.0           0.047399394   \n",
       "1048575           0.0  0.005666818499999999      0.0  0.047594800000000013   \n",
       "\n",
       "        feature9    feature10  ... feature12 feature13 feature14    feature15  \\\n",
       "0            0.0  0.019287573  ...       0.0       0.0       0.0          0.0   \n",
       "1            0.0  0.031135669  ...       0.0       0.0       0.0  0.006044184   \n",
       "2            0.0   0.02647767  ...       0.0       0.0       0.0          0.0   \n",
       "3            0.0  0.025008397  ...       0.0       0.0       0.0          0.0   \n",
       "4            0.0  0.024143761  ...       0.0       0.0       0.0          0.0   \n",
       "...          ...          ...  ...       ...       ...       ...          ...   \n",
       "1048571      0.0          0.0  ...       0.0       0.0       0.0  0.015696432   \n",
       "1048572      0.0          0.0  ...       0.0       0.0       0.0  0.015654337   \n",
       "1048573      0.0          0.0  ...       0.0       0.0       0.0  0.015895598   \n",
       "1048574      0.0          0.0  ...       0.0       0.0       0.0  0.015462209   \n",
       "1048575      0.0          0.0  ...       0.0       0.0       0.0  0.015328575   \n",
       "\n",
       "        feature16   feature17 feature18 feature19 feature20 label  \n",
       "0             0.0  0.06614828       0.0       0.0       0.0     0  \n",
       "1             0.0         0.0       0.0       0.0       0.0     0  \n",
       "2             0.0         0.0       0.0       0.0       0.0     0  \n",
       "3             0.0         0.0       0.0       0.0       0.0     0  \n",
       "4             0.0         0.0       0.0       0.0       0.0     0  \n",
       "...           ...         ...       ...       ...       ...   ...  \n",
       "1048571       0.0         0.0       0.0       0.0       0.0     0  \n",
       "1048572       0.0         0.0       0.0       0.0       0.0     0  \n",
       "1048573       0.0         0.0       0.0       0.0       0.0     0  \n",
       "1048574       0.0         0.0       0.0       0.0       0.0     0  \n",
       "1048575       0.0         0.0       0.0       0.0       0.0     0  \n",
       "\n",
       "[1048576 rows x 21 columns]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)   \n",
    "df1 = pd.DataFrame(df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    595798\n",
       "1    452778\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly sample 500 data point for training\n",
    "df_train=df[df.label==1].sample(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 21)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the item sampled from our dataset\n",
    "#index_list=df_train.index\n",
    "#df=df.drop(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1            0.0\n",
       "feature2            0.0\n",
       "feature3            0.0\n",
       "feature4            0.0\n",
       "feature5            0.0\n",
       "feature6      0.1171062\n",
       "feature7            0.0\n",
       "feature8            0.0\n",
       "feature9     0.31030872\n",
       "feature10           0.0\n",
       "feature11           0.0\n",
       "feature12           0.0\n",
       "feature13           0.0\n",
       "feature14           0.0\n",
       "feature15           0.0\n",
       "feature16           0.0\n",
       "feature17           0.0\n",
       "feature18           0.0\n",
       "feature19           0.0\n",
       "feature20           0.0\n",
       "label                 1\n",
       "Name: 436000, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature12</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.049896017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074576885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0035594553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012225969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019287573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06614828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05703238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052905086</td>\n",
       "      <td>0.043763362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031135669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006044184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.05211395</td>\n",
       "      <td>0.0006721932400000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01591806</td>\n",
       "      <td>0.02435071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0016837418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02647767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.04974308</td>\n",
       "      <td>0.003009851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0029502455</td>\n",
       "      <td>0.017709356000000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0066348887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025008397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04832898</td>\n",
       "      <td>0.0042979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014554087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009138886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024143761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature1               feature2 feature3     feature4      feature5  \\\n",
       "0  0.049896017                    0.0      0.0  0.074576885           0.0   \n",
       "1   0.05703238                    0.0      0.0          0.0   0.052905086   \n",
       "2   0.05211395  0.0006721932400000001      0.0          0.0    0.01591806   \n",
       "3   0.04974308            0.003009851      0.0          0.0  0.0029502455   \n",
       "4   0.04832898              0.0042979      0.0          0.0           0.0   \n",
       "\n",
       "               feature6 feature7      feature8 feature9    feature10  ...  \\\n",
       "0          0.0035594553      0.0   0.012225969      0.0  0.019287573  ...   \n",
       "1           0.043763362      0.0           0.0      0.0  0.031135669  ...   \n",
       "2            0.02435071      0.0  0.0016837418      0.0   0.02647767  ...   \n",
       "3  0.017709356000000002      0.0  0.0066348887      0.0  0.025008397  ...   \n",
       "4           0.014554087      0.0   0.009138886      0.0  0.024143761  ...   \n",
       "\n",
       "  feature12 feature13 feature14    feature15 feature16   feature17 feature18  \\\n",
       "0       0.0       0.0       0.0          0.0       0.0  0.06614828       0.0   \n",
       "1       0.0       0.0       0.0  0.006044184       0.0         0.0       0.0   \n",
       "2       0.0       0.0       0.0          0.0       0.0         0.0       0.0   \n",
       "3       0.0       0.0       0.0          0.0       0.0         0.0       0.0   \n",
       "4       0.0       0.0       0.0          0.0       0.0         0.0       0.0   \n",
       "\n",
       "  feature19 feature20 label  \n",
       "0       0.0       0.0     0  \n",
       "1       0.0       0.0     0  \n",
       "2       0.0       0.0     0  \n",
       "3       0.0       0.0     0  \n",
       "4       0.0       0.0     0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the label columns\n",
    "df_label=df_train.label\n",
    "df_train=df_train.drop('label',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label\n",
    "df_label.to_csv('abnormallabel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 20)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1            0.0\n",
       "feature2            0.0\n",
       "feature3            0.0\n",
       "feature4            0.0\n",
       "feature5            0.0\n",
       "feature6      0.1171062\n",
       "feature7            0.0\n",
       "feature8            0.0\n",
       "feature9     0.31030872\n",
       "feature10           0.0\n",
       "feature11           0.0\n",
       "feature12           0.0\n",
       "feature13           0.0\n",
       "feature14           0.0\n",
       "feature15           0.0\n",
       "feature16           0.0\n",
       "feature17           0.0\n",
       "feature18           0.0\n",
       "feature19           0.0\n",
       "feature20           0.0\n",
       "Name: 436000, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 20)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.values.reshape((df_train.shape[0], 1, df_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1, 20)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature12</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.049896017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074576885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0035594553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012225969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019287573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06614828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05703238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052905086</td>\n",
       "      <td>0.043763362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031135669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006044184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.05211395</td>\n",
       "      <td>0.0006721932400000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01591806</td>\n",
       "      <td>0.02435071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0016837418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02647767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.04974308</td>\n",
       "      <td>0.003009851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0029502455</td>\n",
       "      <td>0.017709356000000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0066348887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025008397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04832898</td>\n",
       "      <td>0.0042979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014554087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009138886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024143761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature1               feature2 feature3     feature4      feature5  \\\n",
       "0  0.049896017                    0.0      0.0  0.074576885           0.0   \n",
       "1   0.05703238                    0.0      0.0          0.0   0.052905086   \n",
       "2   0.05211395  0.0006721932400000001      0.0          0.0    0.01591806   \n",
       "3   0.04974308            0.003009851      0.0          0.0  0.0029502455   \n",
       "4   0.04832898              0.0042979      0.0          0.0           0.0   \n",
       "\n",
       "               feature6 feature7      feature8 feature9    feature10  ...  \\\n",
       "0          0.0035594553      0.0   0.012225969      0.0  0.019287573  ...   \n",
       "1           0.043763362      0.0           0.0      0.0  0.031135669  ...   \n",
       "2            0.02435071      0.0  0.0016837418      0.0   0.02647767  ...   \n",
       "3  0.017709356000000002      0.0  0.0066348887      0.0  0.025008397  ...   \n",
       "4           0.014554087      0.0   0.009138886      0.0  0.024143761  ...   \n",
       "\n",
       "  feature12 feature13 feature14    feature15 feature16   feature17 feature18  \\\n",
       "0       0.0       0.0       0.0          0.0       0.0  0.06614828       0.0   \n",
       "1       0.0       0.0       0.0  0.006044184       0.0         0.0       0.0   \n",
       "2       0.0       0.0       0.0          0.0       0.0         0.0       0.0   \n",
       "3       0.0       0.0       0.0          0.0       0.0         0.0       0.0   \n",
       "4       0.0       0.0       0.0          0.0       0.0         0.0       0.0   \n",
       "\n",
       "  feature19 feature20 label  \n",
       "0       0.0       0.0     0  \n",
       "1       0.0       0.0     0  \n",
       "2       0.0       0.0     0  \n",
       "3       0.0       0.0     0  \n",
       "4       0.0       0.0     0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "timesteps =  df_train.shape[1] # equal to the lookback\n",
    "n_features =  df_train.shape[2] # 59\n",
    "\n",
    "epochs = 15\n",
    "batch = 64\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=dict()\n",
    "history['gen']=[]\n",
    "history['dis']=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error\n",
    "                             ,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 1\n",
    "        self.img_cols = 20\n",
    "        self.img_shape = (self.img_rows, self.img_cols)\n",
    "        self.latent_dim = 20\n",
    "\n",
    "        optimizer = Adam(0.0001, 0.4)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates song\n",
    "        z = Input(shape=(1,20))\n",
    "    \n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(1, 20)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #specifying output to have 40 timesteps\n",
    "        model.add(RepeatVector(20))\n",
    "        #specifying 1 feature as the output\n",
    "        \n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.3))   \n",
    "        model.add(TimeDistributed(Dense(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(1,20))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "        #return Model(img)\n",
    "    \n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(64, use_bias=False, input_shape=(20, 1)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Convolution1D(32, 3, strides=2, padding='same', use_bias=False))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Convolution1D(1, 3, strides=2, padding='same', use_bias=False))\n",
    "        model.add(Flatten())        \n",
    "        model.add(RepeatVector(1))        \n",
    "        model.add(TimeDistributed(Dense(32, activation = 'relu')))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(1, activation = 'relu')))\n",
    "        #model.add(TimeDistributed(Dense(1)))\n",
    "        #model.add(Dense(1, activation='tanh'))\n",
    "        #model.add(Dense(1))\n",
    "        #model.add(Activation('sigmoid'))\n",
    "        model.summary()\n",
    "        #model.add(layers.BatchNormalization())\n",
    "        #model.add(sigmoid())\n",
    "       \n",
    "        img = Input(shape=(20,1))\n",
    "        validity = model(img)\n",
    "        print(\"exit\")\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs=15, batch_size=64, save_interval=50):\n",
    "        print(\"enter\")  \n",
    "        # Load the dataset\n",
    "        X_train = df_train\n",
    "       \n",
    "        # Rescale 0 to 1\n",
    "      #  X_train = X_train / 128\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size,1,1))\n",
    "        fake = np.zeros((batch_size,1,1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of songs\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            imgs = np.array(imgs)\n",
    "            imgs = imgs.reshape(len(imgs),20,1)\n",
    "\n",
    "            # Sample noise and generate a batch of new songs\n",
    "            noise = np.random.normal(0, 1, (batch_size,1,20))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake songs as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "            history['gen'].append(g_loss)\n",
    "            \n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            history['dis'].append(d_loss) \n",
    "            #return (g_loss,d_loss)\n",
    "            # If at save interval => save model\n",
    "#            if epoch % save_interval == 0:\n",
    " #               self.generator.save(\"LSTM_generator.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 20, 64)            64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 20, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 10, 32)            6144      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5, 1)              96        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 1, 5)              0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 32)             192       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 1, 1)              33        \n",
      "=================================================================\n",
      "Total params: 6,529\n",
      "Trainable params: 6,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "exit\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 1, 256)            152576    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 1, 256)            394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 20, 256)           394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 20, 256)           394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 20, 256)           394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 20, 128)           32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 20, 128)           16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 20, 1)             129       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 20, 1)             0         \n",
      "=================================================================\n",
      "Total params: 2,173,313\n",
      "Trainable params: 2,173,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstmgan = LSTMGAN()\n",
    "#lstmgan.train(epochs=1000, batch_size=20, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 4.790564, acc.: 50.00%] [G loss: 7.530931]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 4.326564, acc.: 50.00%] [G loss: 6.737534]\n",
      "2 [D loss: 3.488968, acc.: 50.00%] [G loss: 6.232628]\n",
      "3 [D loss: 3.408130, acc.: 50.00%] [G loss: 5.927962]\n",
      "4 [D loss: 3.422532, acc.: 50.00%] [G loss: 5.819317]\n",
      "5 [D loss: 3.019253, acc.: 50.00%] [G loss: 5.748940]\n",
      "6 [D loss: 3.601867, acc.: 50.00%] [G loss: 5.521670]\n",
      "7 [D loss: 2.723381, acc.: 50.00%] [G loss: 5.518656]\n",
      "8 [D loss: 3.177216, acc.: 50.00%] [G loss: 5.371750]\n",
      "9 [D loss: 2.657425, acc.: 50.00%] [G loss: 5.390020]\n",
      "10 [D loss: 2.980903, acc.: 50.00%] [G loss: 5.261878]\n",
      "11 [D loss: 2.653734, acc.: 50.00%] [G loss: 5.192923]\n",
      "12 [D loss: 2.836585, acc.: 50.00%] [G loss: 5.060465]\n",
      "13 [D loss: 2.582577, acc.: 50.00%] [G loss: 4.965248]\n",
      "14 [D loss: 2.534016, acc.: 50.00%] [G loss: 5.006494]\n",
      "15 [D loss: 2.448184, acc.: 50.00%] [G loss: 4.935673]\n",
      "16 [D loss: 2.433904, acc.: 50.00%] [G loss: 4.924684]\n",
      "17 [D loss: 2.478136, acc.: 50.00%] [G loss: 4.853754]\n",
      "18 [D loss: 2.361714, acc.: 50.00%] [G loss: 4.845382]\n",
      "19 [D loss: 2.417836, acc.: 50.00%] [G loss: 4.821574]\n",
      "20 [D loss: 2.383004, acc.: 50.00%] [G loss: 4.762210]\n",
      "21 [D loss: 2.343431, acc.: 50.00%] [G loss: 4.905173]\n",
      "22 [D loss: 2.327125, acc.: 50.00%] [G loss: 4.791149]\n",
      "23 [D loss: 2.316975, acc.: 50.00%] [G loss: 4.761780]\n",
      "24 [D loss: 2.342750, acc.: 50.00%] [G loss: 4.659678]\n",
      "25 [D loss: 2.303134, acc.: 50.00%] [G loss: 4.583181]\n",
      "26 [D loss: 2.287925, acc.: 50.00%] [G loss: 4.583200]\n",
      "27 [D loss: 2.296507, acc.: 50.00%] [G loss: 4.621813]\n",
      "28 [D loss: 2.279667, acc.: 50.00%] [G loss: 4.525758]\n",
      "29 [D loss: 2.255373, acc.: 50.00%] [G loss: 4.496149]\n",
      "30 [D loss: 2.323640, acc.: 50.00%] [G loss: 4.511004]\n",
      "31 [D loss: 2.272646, acc.: 50.00%] [G loss: 4.457171]\n",
      "32 [D loss: 2.232152, acc.: 50.00%] [G loss: 4.494154]\n",
      "33 [D loss: 2.242454, acc.: 50.00%] [G loss: 4.498270]\n",
      "34 [D loss: 2.169178, acc.: 50.00%] [G loss: 4.332695]\n",
      "35 [D loss: 2.132293, acc.: 50.00%] [G loss: 4.438805]\n",
      "36 [D loss: 2.179567, acc.: 50.00%] [G loss: 4.399426]\n",
      "37 [D loss: 2.171608, acc.: 50.00%] [G loss: 4.318834]\n",
      "38 [D loss: 2.185572, acc.: 50.00%] [G loss: 4.320913]\n",
      "39 [D loss: 2.138478, acc.: 50.00%] [G loss: 4.268208]\n",
      "40 [D loss: 2.165663, acc.: 50.00%] [G loss: 4.318160]\n",
      "41 [D loss: 2.163471, acc.: 50.00%] [G loss: 4.273617]\n",
      "42 [D loss: 2.123664, acc.: 50.00%] [G loss: 4.268477]\n",
      "43 [D loss: 2.055163, acc.: 50.00%] [G loss: 4.202785]\n",
      "44 [D loss: 2.205491, acc.: 50.00%] [G loss: 4.283255]\n",
      "45 [D loss: 2.110071, acc.: 50.00%] [G loss: 4.292932]\n",
      "46 [D loss: 2.067669, acc.: 50.00%] [G loss: 4.230117]\n",
      "47 [D loss: 2.102667, acc.: 50.00%] [G loss: 4.240656]\n",
      "48 [D loss: 2.075282, acc.: 50.00%] [G loss: 4.247023]\n",
      "49 [D loss: 2.059392, acc.: 50.00%] [G loss: 4.153280]\n",
      "50 [D loss: 2.033054, acc.: 50.00%] [G loss: 4.232409]\n",
      "51 [D loss: 2.043039, acc.: 50.00%] [G loss: 4.030655]\n",
      "52 [D loss: 2.082624, acc.: 50.00%] [G loss: 4.061298]\n",
      "53 [D loss: 2.030350, acc.: 50.00%] [G loss: 4.071331]\n",
      "54 [D loss: 2.074118, acc.: 50.00%] [G loss: 4.035845]\n",
      "55 [D loss: 2.036338, acc.: 50.00%] [G loss: 4.100265]\n",
      "56 [D loss: 2.041241, acc.: 50.00%] [G loss: 4.074461]\n",
      "57 [D loss: 2.051213, acc.: 50.00%] [G loss: 4.095312]\n",
      "58 [D loss: 1.978493, acc.: 50.00%] [G loss: 4.113867]\n",
      "59 [D loss: 1.989964, acc.: 50.00%] [G loss: 4.060279]\n",
      "60 [D loss: 1.948803, acc.: 50.00%] [G loss: 3.992219]\n",
      "61 [D loss: 1.989159, acc.: 50.00%] [G loss: 3.958301]\n",
      "62 [D loss: 1.983249, acc.: 50.00%] [G loss: 3.949300]\n",
      "63 [D loss: 2.060709, acc.: 50.00%] [G loss: 3.907459]\n",
      "64 [D loss: 1.985459, acc.: 50.00%] [G loss: 3.953188]\n",
      "65 [D loss: 1.970452, acc.: 50.00%] [G loss: 3.992543]\n",
      "66 [D loss: 2.000019, acc.: 50.00%] [G loss: 3.962029]\n",
      "67 [D loss: 1.970957, acc.: 50.00%] [G loss: 3.921053]\n",
      "68 [D loss: 1.970285, acc.: 50.00%] [G loss: 3.915898]\n",
      "69 [D loss: 1.993320, acc.: 50.00%] [G loss: 3.942813]\n",
      "70 [D loss: 1.961056, acc.: 50.00%] [G loss: 3.901077]\n",
      "71 [D loss: 1.960110, acc.: 50.00%] [G loss: 3.940397]\n",
      "72 [D loss: 1.944355, acc.: 50.00%] [G loss: 3.874651]\n",
      "73 [D loss: 1.922497, acc.: 50.00%] [G loss: 3.900602]\n",
      "74 [D loss: 1.979919, acc.: 50.00%] [G loss: 3.897393]\n",
      "75 [D loss: 1.913130, acc.: 50.00%] [G loss: 3.889746]\n",
      "76 [D loss: 1.909169, acc.: 50.00%] [G loss: 3.871245]\n",
      "77 [D loss: 1.968683, acc.: 50.00%] [G loss: 3.826991]\n",
      "78 [D loss: 1.936084, acc.: 50.00%] [G loss: 3.776180]\n",
      "79 [D loss: 1.919371, acc.: 50.00%] [G loss: 3.735524]\n",
      "80 [D loss: 1.917603, acc.: 50.00%] [G loss: 3.808550]\n",
      "81 [D loss: 1.876628, acc.: 50.00%] [G loss: 3.822694]\n",
      "82 [D loss: 1.941830, acc.: 50.00%] [G loss: 3.748205]\n",
      "83 [D loss: 1.923529, acc.: 50.00%] [G loss: 3.853134]\n",
      "84 [D loss: 1.888077, acc.: 50.00%] [G loss: 3.770017]\n",
      "85 [D loss: 1.920779, acc.: 50.00%] [G loss: 3.739441]\n",
      "86 [D loss: 1.889819, acc.: 50.00%] [G loss: 3.714790]\n",
      "87 [D loss: 1.886303, acc.: 50.00%] [G loss: 3.709327]\n",
      "88 [D loss: 1.886995, acc.: 50.00%] [G loss: 3.764490]\n",
      "89 [D loss: 1.830129, acc.: 50.00%] [G loss: 3.723533]\n",
      "90 [D loss: 1.910061, acc.: 50.00%] [G loss: 3.822174]\n",
      "91 [D loss: 1.830198, acc.: 50.00%] [G loss: 3.703198]\n",
      "92 [D loss: 1.835816, acc.: 50.00%] [G loss: 3.776418]\n",
      "93 [D loss: 1.908192, acc.: 50.00%] [G loss: 3.687097]\n",
      "94 [D loss: 1.779550, acc.: 50.00%] [G loss: 3.644344]\n",
      "95 [D loss: 1.789622, acc.: 50.00%] [G loss: 3.591489]\n",
      "96 [D loss: 1.844283, acc.: 50.00%] [G loss: 3.653066]\n",
      "97 [D loss: 1.845466, acc.: 50.00%] [G loss: 3.694315]\n",
      "98 [D loss: 1.763894, acc.: 50.00%] [G loss: 3.593499]\n",
      "99 [D loss: 1.805362, acc.: 50.00%] [G loss: 3.606994]\n",
      "100 [D loss: 1.773880, acc.: 50.00%] [G loss: 3.578801]\n",
      "101 [D loss: 1.775130, acc.: 50.00%] [G loss: 3.617829]\n",
      "102 [D loss: 1.795443, acc.: 50.00%] [G loss: 3.578726]\n",
      "103 [D loss: 1.830906, acc.: 50.00%] [G loss: 3.643252]\n",
      "104 [D loss: 1.870120, acc.: 50.00%] [G loss: 3.654593]\n",
      "105 [D loss: 1.764001, acc.: 50.00%] [G loss: 3.515376]\n",
      "106 [D loss: 1.770888, acc.: 50.00%] [G loss: 3.597311]\n",
      "107 [D loss: 1.734178, acc.: 50.00%] [G loss: 3.605138]\n",
      "108 [D loss: 1.733575, acc.: 50.00%] [G loss: 3.748696]\n",
      "109 [D loss: 1.915057, acc.: 50.00%] [G loss: 3.588245]\n",
      "110 [D loss: 1.772310, acc.: 50.00%] [G loss: 3.565586]\n",
      "111 [D loss: 1.821777, acc.: 50.00%] [G loss: 3.537416]\n",
      "112 [D loss: 1.772317, acc.: 50.00%] [G loss: 3.573251]\n",
      "113 [D loss: 1.721883, acc.: 50.00%] [G loss: 3.568077]\n",
      "114 [D loss: 1.737889, acc.: 50.00%] [G loss: 3.565731]\n",
      "115 [D loss: 1.798938, acc.: 50.00%] [G loss: 3.573893]\n",
      "116 [D loss: 1.715659, acc.: 50.00%] [G loss: 3.593536]\n",
      "117 [D loss: 1.702765, acc.: 50.00%] [G loss: 3.569450]\n",
      "118 [D loss: 1.698523, acc.: 50.00%] [G loss: 3.489942]\n",
      "119 [D loss: 1.758251, acc.: 50.00%] [G loss: 3.496644]\n",
      "120 [D loss: 1.788998, acc.: 50.00%] [G loss: 3.582411]\n",
      "121 [D loss: 1.728345, acc.: 50.00%] [G loss: 3.475507]\n",
      "122 [D loss: 1.708570, acc.: 50.00%] [G loss: 3.646968]\n",
      "123 [D loss: 1.756877, acc.: 50.00%] [G loss: 3.526314]\n",
      "124 [D loss: 1.718081, acc.: 50.00%] [G loss: 3.416847]\n",
      "125 [D loss: 1.697396, acc.: 50.00%] [G loss: 3.487111]\n",
      "126 [D loss: 1.802343, acc.: 50.00%] [G loss: 3.445091]\n",
      "127 [D loss: 1.744237, acc.: 50.00%] [G loss: 3.519655]\n",
      "128 [D loss: 1.694493, acc.: 50.00%] [G loss: 3.485467]\n",
      "129 [D loss: 1.722163, acc.: 50.00%] [G loss: 3.369049]\n",
      "130 [D loss: 1.682260, acc.: 50.00%] [G loss: 3.495217]\n",
      "131 [D loss: 1.700948, acc.: 50.00%] [G loss: 3.455256]\n",
      "132 [D loss: 1.724743, acc.: 50.00%] [G loss: 3.449847]\n",
      "133 [D loss: 1.657823, acc.: 50.00%] [G loss: 3.367440]\n",
      "134 [D loss: 1.682198, acc.: 50.00%] [G loss: 3.339275]\n",
      "135 [D loss: 1.706517, acc.: 50.00%] [G loss: 3.416474]\n",
      "136 [D loss: 1.714422, acc.: 50.00%] [G loss: 3.455893]\n",
      "137 [D loss: 1.721608, acc.: 50.00%] [G loss: 3.368902]\n",
      "138 [D loss: 1.697993, acc.: 50.00%] [G loss: 3.373494]\n",
      "139 [D loss: 1.737529, acc.: 50.00%] [G loss: 3.460614]\n",
      "140 [D loss: 1.677460, acc.: 50.00%] [G loss: 3.375767]\n",
      "141 [D loss: 1.702402, acc.: 50.00%] [G loss: 3.443974]\n",
      "142 [D loss: 1.634342, acc.: 50.00%] [G loss: 3.350050]\n",
      "143 [D loss: 1.698632, acc.: 50.00%] [G loss: 3.296775]\n",
      "144 [D loss: 1.690191, acc.: 50.00%] [G loss: 3.339888]\n",
      "145 [D loss: 1.708144, acc.: 50.00%] [G loss: 3.419384]\n",
      "146 [D loss: 1.679768, acc.: 50.00%] [G loss: 3.350975]\n",
      "147 [D loss: 1.660587, acc.: 50.00%] [G loss: 3.287776]\n",
      "148 [D loss: 1.691492, acc.: 50.00%] [G loss: 3.332227]\n",
      "149 [D loss: 1.633618, acc.: 50.00%] [G loss: 3.344011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 [D loss: 1.621742, acc.: 50.00%] [G loss: 3.388722]\n",
      "151 [D loss: 1.656626, acc.: 50.00%] [G loss: 3.364377]\n",
      "152 [D loss: 1.649006, acc.: 50.00%] [G loss: 3.302552]\n",
      "153 [D loss: 1.642778, acc.: 50.00%] [G loss: 3.292558]\n",
      "154 [D loss: 1.661460, acc.: 50.00%] [G loss: 3.321251]\n",
      "155 [D loss: 1.653894, acc.: 50.00%] [G loss: 3.310697]\n",
      "156 [D loss: 1.583409, acc.: 50.00%] [G loss: 3.283447]\n",
      "157 [D loss: 1.694911, acc.: 50.00%] [G loss: 3.331925]\n",
      "158 [D loss: 1.620653, acc.: 50.00%] [G loss: 3.314803]\n",
      "159 [D loss: 1.626543, acc.: 50.00%] [G loss: 3.235073]\n",
      "160 [D loss: 1.634686, acc.: 50.00%] [G loss: 3.219673]\n",
      "161 [D loss: 1.626157, acc.: 50.00%] [G loss: 3.247142]\n",
      "162 [D loss: 1.653254, acc.: 50.00%] [G loss: 3.260070]\n",
      "163 [D loss: 1.610307, acc.: 50.00%] [G loss: 3.323451]\n",
      "164 [D loss: 1.637792, acc.: 50.00%] [G loss: 3.204430]\n",
      "165 [D loss: 1.686441, acc.: 50.00%] [G loss: 3.130700]\n",
      "166 [D loss: 1.625927, acc.: 50.00%] [G loss: 3.251459]\n",
      "167 [D loss: 1.595613, acc.: 50.00%] [G loss: 3.241356]\n",
      "168 [D loss: 1.623125, acc.: 50.00%] [G loss: 3.249667]\n",
      "169 [D loss: 1.655474, acc.: 50.00%] [G loss: 3.234087]\n",
      "170 [D loss: 1.646360, acc.: 50.00%] [G loss: 3.153454]\n",
      "171 [D loss: 1.561386, acc.: 50.00%] [G loss: 3.179559]\n",
      "172 [D loss: 1.624621, acc.: 50.00%] [G loss: 3.176686]\n",
      "173 [D loss: 1.628587, acc.: 50.00%] [G loss: 3.226064]\n",
      "174 [D loss: 1.652993, acc.: 50.00%] [G loss: 3.190968]\n",
      "175 [D loss: 1.591560, acc.: 50.00%] [G loss: 3.168539]\n",
      "176 [D loss: 1.612154, acc.: 50.00%] [G loss: 3.229627]\n",
      "177 [D loss: 1.584708, acc.: 50.00%] [G loss: 3.153473]\n",
      "178 [D loss: 1.614448, acc.: 50.00%] [G loss: 3.066970]\n",
      "179 [D loss: 1.517851, acc.: 50.00%] [G loss: 3.184415]\n",
      "180 [D loss: 1.639348, acc.: 50.00%] [G loss: 3.130416]\n",
      "181 [D loss: 1.614354, acc.: 50.00%] [G loss: 3.264376]\n",
      "182 [D loss: 1.579337, acc.: 50.00%] [G loss: 3.152088]\n",
      "183 [D loss: 1.554626, acc.: 50.00%] [G loss: 3.190537]\n",
      "184 [D loss: 1.522146, acc.: 50.00%] [G loss: 3.237379]\n",
      "185 [D loss: 1.589574, acc.: 50.00%] [G loss: 3.210471]\n",
      "186 [D loss: 1.583141, acc.: 50.00%] [G loss: 3.161961]\n",
      "187 [D loss: 1.570841, acc.: 50.00%] [G loss: 3.193757]\n",
      "188 [D loss: 1.561138, acc.: 50.00%] [G loss: 3.151889]\n",
      "189 [D loss: 1.548654, acc.: 50.00%] [G loss: 3.165743]\n",
      "190 [D loss: 1.596646, acc.: 50.00%] [G loss: 3.153988]\n",
      "191 [D loss: 1.481083, acc.: 50.00%] [G loss: 3.115394]\n",
      "192 [D loss: 1.571625, acc.: 50.00%] [G loss: 3.104658]\n",
      "193 [D loss: 1.562301, acc.: 50.00%] [G loss: 3.120150]\n",
      "194 [D loss: 1.578842, acc.: 50.00%] [G loss: 3.131507]\n",
      "195 [D loss: 1.538327, acc.: 50.00%] [G loss: 3.108656]\n",
      "196 [D loss: 1.521709, acc.: 50.00%] [G loss: 3.163756]\n",
      "197 [D loss: 1.533833, acc.: 50.00%] [G loss: 3.072725]\n",
      "198 [D loss: 1.528721, acc.: 50.00%] [G loss: 3.109361]\n",
      "199 [D loss: 1.561383, acc.: 50.00%] [G loss: 3.158838]\n",
      "200 [D loss: 1.518212, acc.: 50.00%] [G loss: 3.048115]\n",
      "201 [D loss: 1.499598, acc.: 50.00%] [G loss: 3.099009]\n",
      "202 [D loss: 1.532396, acc.: 50.00%] [G loss: 3.025623]\n",
      "203 [D loss: 1.529776, acc.: 50.00%] [G loss: 3.062612]\n",
      "204 [D loss: 1.557003, acc.: 50.00%] [G loss: 3.083251]\n",
      "205 [D loss: 1.537371, acc.: 50.00%] [G loss: 3.148636]\n",
      "206 [D loss: 1.520298, acc.: 50.00%] [G loss: 3.175648]\n",
      "207 [D loss: 1.587900, acc.: 50.00%] [G loss: 3.143830]\n",
      "208 [D loss: 1.555525, acc.: 50.00%] [G loss: 3.173401]\n",
      "209 [D loss: 1.509625, acc.: 50.00%] [G loss: 3.122348]\n",
      "210 [D loss: 1.529254, acc.: 50.00%] [G loss: 2.970564]\n",
      "211 [D loss: 1.499597, acc.: 50.00%] [G loss: 3.047947]\n",
      "212 [D loss: 1.530210, acc.: 50.00%] [G loss: 3.120416]\n",
      "213 [D loss: 1.529078, acc.: 50.00%] [G loss: 3.079086]\n",
      "214 [D loss: 1.528328, acc.: 50.00%] [G loss: 3.119009]\n",
      "215 [D loss: 1.577125, acc.: 50.00%] [G loss: 3.027390]\n",
      "216 [D loss: 1.483660, acc.: 50.00%] [G loss: 3.103764]\n",
      "217 [D loss: 1.480563, acc.: 50.00%] [G loss: 3.049090]\n",
      "218 [D loss: 1.535260, acc.: 50.00%] [G loss: 3.037400]\n",
      "219 [D loss: 1.553349, acc.: 50.00%] [G loss: 3.072494]\n",
      "220 [D loss: 1.463207, acc.: 50.00%] [G loss: 2.931850]\n",
      "221 [D loss: 1.556306, acc.: 50.00%] [G loss: 3.098998]\n",
      "222 [D loss: 1.489824, acc.: 50.00%] [G loss: 3.052540]\n",
      "223 [D loss: 1.458640, acc.: 50.00%] [G loss: 3.204423]\n",
      "224 [D loss: 1.490115, acc.: 50.00%] [G loss: 3.041991]\n",
      "225 [D loss: 1.489713, acc.: 50.00%] [G loss: 3.045914]\n",
      "226 [D loss: 1.504341, acc.: 50.00%] [G loss: 2.950791]\n",
      "227 [D loss: 1.451270, acc.: 50.00%] [G loss: 2.960870]\n",
      "228 [D loss: 1.464264, acc.: 50.00%] [G loss: 3.055352]\n",
      "229 [D loss: 1.430222, acc.: 50.00%] [G loss: 3.041387]\n",
      "230 [D loss: 1.497062, acc.: 50.00%] [G loss: 3.134918]\n",
      "231 [D loss: 1.476370, acc.: 50.00%] [G loss: 2.931449]\n",
      "232 [D loss: 1.532463, acc.: 50.00%] [G loss: 2.967765]\n",
      "233 [D loss: 1.506611, acc.: 50.00%] [G loss: 3.016504]\n",
      "234 [D loss: 1.505721, acc.: 50.00%] [G loss: 2.922331]\n",
      "235 [D loss: 1.477786, acc.: 50.00%] [G loss: 3.013265]\n",
      "236 [D loss: 1.452982, acc.: 50.00%] [G loss: 2.951556]\n",
      "237 [D loss: 1.436412, acc.: 50.00%] [G loss: 2.885213]\n",
      "238 [D loss: 1.448921, acc.: 50.00%] [G loss: 2.934204]\n",
      "239 [D loss: 1.457059, acc.: 50.00%] [G loss: 3.040145]\n",
      "240 [D loss: 1.454417, acc.: 50.00%] [G loss: 2.944738]\n",
      "241 [D loss: 1.522624, acc.: 50.00%] [G loss: 2.951533]\n",
      "242 [D loss: 1.428672, acc.: 50.00%] [G loss: 2.967214]\n",
      "243 [D loss: 1.469054, acc.: 50.00%] [G loss: 2.954957]\n",
      "244 [D loss: 1.462369, acc.: 50.00%] [G loss: 2.944753]\n",
      "245 [D loss: 1.420740, acc.: 50.00%] [G loss: 2.847521]\n",
      "246 [D loss: 1.465961, acc.: 50.00%] [G loss: 2.959795]\n",
      "247 [D loss: 1.442834, acc.: 50.00%] [G loss: 2.929069]\n",
      "248 [D loss: 1.415136, acc.: 50.00%] [G loss: 2.951822]\n",
      "249 [D loss: 1.462736, acc.: 50.00%] [G loss: 2.987365]\n",
      "250 [D loss: 1.415337, acc.: 50.00%] [G loss: 2.979731]\n",
      "251 [D loss: 1.429803, acc.: 50.00%] [G loss: 2.913885]\n",
      "252 [D loss: 1.446042, acc.: 50.00%] [G loss: 2.879372]\n",
      "253 [D loss: 1.497208, acc.: 50.00%] [G loss: 2.883136]\n",
      "254 [D loss: 1.465884, acc.: 50.00%] [G loss: 2.836497]\n",
      "255 [D loss: 1.450888, acc.: 50.00%] [G loss: 2.868139]\n",
      "256 [D loss: 1.445159, acc.: 50.00%] [G loss: 2.913114]\n",
      "257 [D loss: 1.420283, acc.: 50.00%] [G loss: 2.874743]\n",
      "258 [D loss: 1.389906, acc.: 50.00%] [G loss: 2.933940]\n",
      "259 [D loss: 1.435115, acc.: 50.00%] [G loss: 2.954334]\n",
      "260 [D loss: 1.413227, acc.: 50.00%] [G loss: 2.998614]\n",
      "261 [D loss: 1.341169, acc.: 50.00%] [G loss: 2.841686]\n",
      "262 [D loss: 1.416217, acc.: 50.00%] [G loss: 2.922306]\n",
      "263 [D loss: 1.370802, acc.: 50.00%] [G loss: 2.775443]\n",
      "264 [D loss: 1.425961, acc.: 50.00%] [G loss: 2.957965]\n",
      "265 [D loss: 1.419974, acc.: 50.00%] [G loss: 2.900191]\n",
      "266 [D loss: 1.459117, acc.: 50.00%] [G loss: 2.702189]\n",
      "267 [D loss: 1.398352, acc.: 50.00%] [G loss: 2.724625]\n",
      "268 [D loss: 1.441793, acc.: 50.00%] [G loss: 4.079239]\n",
      "269 [D loss: 1.527326, acc.: 47.50%] [G loss: 3.888123]\n",
      "270 [D loss: 1.588285, acc.: 42.50%] [G loss: 3.004142]\n",
      "271 [D loss: 1.551621, acc.: 47.50%] [G loss: 5.527474]\n",
      "272 [D loss: 1.547275, acc.: 47.50%] [G loss: 4.996383]\n",
      "273 [D loss: 1.468343, acc.: 45.00%] [G loss: 6.896974]\n",
      "274 [D loss: 1.462398, acc.: 47.50%] [G loss: 6.634181]\n",
      "275 [D loss: 1.467927, acc.: 47.50%] [G loss: 5.793058]\n",
      "276 [D loss: 1.511292, acc.: 47.50%] [G loss: 7.436990]\n",
      "277 [D loss: 1.442069, acc.: 47.50%] [G loss: 7.919431]\n",
      "278 [D loss: 1.416556, acc.: 50.00%] [G loss: 5.796475]\n",
      "279 [D loss: 1.440801, acc.: 50.00%] [G loss: 6.143864]\n",
      "280 [D loss: 1.508037, acc.: 47.50%] [G loss: 5.442182]\n",
      "281 [D loss: 1.391200, acc.: 50.00%] [G loss: 7.488898]\n",
      "282 [D loss: 1.393290, acc.: 50.00%] [G loss: 8.748610]\n",
      "283 [D loss: 1.406924, acc.: 50.00%] [G loss: 8.122266]\n",
      "284 [D loss: 1.403224, acc.: 50.00%] [G loss: 5.394619]\n",
      "285 [D loss: 1.381082, acc.: 50.00%] [G loss: 7.464665]\n",
      "286 [D loss: 1.416747, acc.: 50.00%] [G loss: 7.151233]\n",
      "287 [D loss: 1.457896, acc.: 50.00%] [G loss: 4.656282]\n",
      "288 [D loss: 1.371642, acc.: 50.00%] [G loss: 4.989272]\n",
      "289 [D loss: 1.385993, acc.: 50.00%] [G loss: 5.895160]\n",
      "290 [D loss: 1.421819, acc.: 50.00%] [G loss: 5.855485]\n",
      "291 [D loss: 1.378280, acc.: 50.00%] [G loss: 5.289514]\n",
      "292 [D loss: 1.352525, acc.: 50.00%] [G loss: 4.232578]\n",
      "293 [D loss: 1.383476, acc.: 50.00%] [G loss: 3.406546]\n",
      "294 [D loss: 1.404479, acc.: 50.00%] [G loss: 2.900078]\n",
      "295 [D loss: 1.375174, acc.: 50.00%] [G loss: 2.823604]\n",
      "296 [D loss: 1.386701, acc.: 50.00%] [G loss: 2.722651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 [D loss: 1.395933, acc.: 50.00%] [G loss: 2.831754]\n",
      "298 [D loss: 1.387817, acc.: 50.00%] [G loss: 2.757848]\n",
      "299 [D loss: 1.421250, acc.: 50.00%] [G loss: 2.747105]\n",
      "300 [D loss: 1.383885, acc.: 50.00%] [G loss: 2.802102]\n",
      "301 [D loss: 1.323843, acc.: 50.00%] [G loss: 2.760962]\n",
      "302 [D loss: 1.365358, acc.: 50.00%] [G loss: 2.731079]\n",
      "303 [D loss: 1.379513, acc.: 50.00%] [G loss: 2.854000]\n",
      "304 [D loss: 1.391289, acc.: 50.00%] [G loss: 2.730787]\n",
      "305 [D loss: 1.434415, acc.: 50.00%] [G loss: 2.783452]\n",
      "306 [D loss: 1.374862, acc.: 50.00%] [G loss: 2.786022]\n",
      "307 [D loss: 1.322755, acc.: 50.00%] [G loss: 2.719787]\n",
      "308 [D loss: 1.422624, acc.: 50.00%] [G loss: 2.778891]\n",
      "309 [D loss: 1.399338, acc.: 50.00%] [G loss: 2.708071]\n",
      "310 [D loss: 1.315843, acc.: 50.00%] [G loss: 2.761788]\n",
      "311 [D loss: 1.341386, acc.: 50.00%] [G loss: 2.773743]\n",
      "312 [D loss: 1.310891, acc.: 50.00%] [G loss: 2.816254]\n",
      "313 [D loss: 1.392208, acc.: 50.00%] [G loss: 2.786212]\n",
      "314 [D loss: 1.361949, acc.: 50.00%] [G loss: 2.800538]\n",
      "315 [D loss: 1.346107, acc.: 50.00%] [G loss: 2.742872]\n",
      "316 [D loss: 1.366250, acc.: 50.00%] [G loss: 2.777540]\n",
      "317 [D loss: 1.370501, acc.: 50.00%] [G loss: 2.773314]\n",
      "318 [D loss: 1.362933, acc.: 50.00%] [G loss: 2.737438]\n",
      "319 [D loss: 1.336707, acc.: 50.00%] [G loss: 2.781381]\n",
      "320 [D loss: 1.314474, acc.: 50.00%] [G loss: 2.840715]\n",
      "321 [D loss: 1.329333, acc.: 50.00%] [G loss: 2.780301]\n",
      "322 [D loss: 1.298041, acc.: 50.00%] [G loss: 2.819511]\n",
      "323 [D loss: 1.294175, acc.: 50.00%] [G loss: 2.760779]\n",
      "324 [D loss: 1.253971, acc.: 50.00%] [G loss: 2.746301]\n",
      "325 [D loss: 1.352621, acc.: 50.00%] [G loss: 2.780685]\n",
      "326 [D loss: 1.298048, acc.: 50.00%] [G loss: 2.750560]\n",
      "327 [D loss: 1.298618, acc.: 50.00%] [G loss: 2.780506]\n",
      "328 [D loss: 1.348362, acc.: 50.00%] [G loss: 2.778013]\n",
      "329 [D loss: 1.356043, acc.: 50.00%] [G loss: 2.769660]\n",
      "330 [D loss: 1.309953, acc.: 50.00%] [G loss: 2.702859]\n",
      "331 [D loss: 1.316007, acc.: 50.00%] [G loss: 2.750579]\n",
      "332 [D loss: 1.339409, acc.: 50.00%] [G loss: 2.712508]\n",
      "333 [D loss: 1.346075, acc.: 50.00%] [G loss: 2.797024]\n",
      "334 [D loss: 1.323813, acc.: 50.00%] [G loss: 2.695719]\n",
      "335 [D loss: 1.260400, acc.: 50.00%] [G loss: 2.782390]\n",
      "336 [D loss: 1.308176, acc.: 50.00%] [G loss: 2.678457]\n",
      "337 [D loss: 1.230221, acc.: 50.00%] [G loss: 2.779494]\n",
      "338 [D loss: 1.268256, acc.: 50.00%] [G loss: 2.773231]\n",
      "339 [D loss: 1.345149, acc.: 50.00%] [G loss: 2.715115]\n",
      "340 [D loss: 1.323601, acc.: 50.00%] [G loss: 2.681859]\n",
      "341 [D loss: 1.340295, acc.: 50.00%] [G loss: 2.775691]\n",
      "342 [D loss: 1.321287, acc.: 50.00%] [G loss: 2.690658]\n",
      "343 [D loss: 1.265781, acc.: 50.00%] [G loss: 2.709236]\n",
      "344 [D loss: 1.299601, acc.: 50.00%] [G loss: 2.633268]\n",
      "345 [D loss: 1.269448, acc.: 50.00%] [G loss: 2.738335]\n",
      "346 [D loss: 1.280723, acc.: 50.00%] [G loss: 2.695967]\n",
      "347 [D loss: 1.268244, acc.: 50.00%] [G loss: 2.721893]\n",
      "348 [D loss: 1.393411, acc.: 50.00%] [G loss: 2.725177]\n",
      "349 [D loss: 1.274096, acc.: 50.00%] [G loss: 2.747538]\n",
      "350 [D loss: 1.320320, acc.: 50.00%] [G loss: 2.728162]\n",
      "351 [D loss: 1.306215, acc.: 50.00%] [G loss: 2.685919]\n",
      "352 [D loss: 1.340935, acc.: 50.00%] [G loss: 2.747072]\n",
      "353 [D loss: 1.249191, acc.: 50.00%] [G loss: 2.615458]\n",
      "354 [D loss: 1.289650, acc.: 50.00%] [G loss: 2.601474]\n",
      "355 [D loss: 1.249041, acc.: 50.00%] [G loss: 2.654177]\n",
      "356 [D loss: 1.281522, acc.: 50.00%] [G loss: 2.694955]\n",
      "357 [D loss: 1.280418, acc.: 50.00%] [G loss: 2.594111]\n",
      "358 [D loss: 1.335244, acc.: 50.00%] [G loss: 2.597510]\n",
      "359 [D loss: 1.302654, acc.: 50.00%] [G loss: 2.597830]\n",
      "360 [D loss: 1.277769, acc.: 50.00%] [G loss: 2.636894]\n",
      "361 [D loss: 1.338546, acc.: 50.00%] [G loss: 2.615607]\n",
      "362 [D loss: 1.271770, acc.: 50.00%] [G loss: 2.677456]\n",
      "363 [D loss: 1.349331, acc.: 50.00%] [G loss: 2.593094]\n",
      "364 [D loss: 1.269345, acc.: 50.00%] [G loss: 2.620573]\n",
      "365 [D loss: 1.287638, acc.: 50.00%] [G loss: 2.625465]\n",
      "366 [D loss: 1.224569, acc.: 50.00%] [G loss: 2.576273]\n",
      "367 [D loss: 1.329527, acc.: 50.00%] [G loss: 2.605705]\n",
      "368 [D loss: 1.308888, acc.: 50.00%] [G loss: 2.684847]\n",
      "369 [D loss: 1.305066, acc.: 50.00%] [G loss: 2.587266]\n",
      "370 [D loss: 1.277474, acc.: 50.00%] [G loss: 2.668862]\n",
      "371 [D loss: 1.247691, acc.: 50.00%] [G loss: 2.666335]\n",
      "372 [D loss: 1.319915, acc.: 50.00%] [G loss: 2.688216]\n",
      "373 [D loss: 1.216678, acc.: 50.00%] [G loss: 2.590559]\n",
      "374 [D loss: 1.286766, acc.: 50.00%] [G loss: 2.737699]\n",
      "375 [D loss: 1.250007, acc.: 50.00%] [G loss: 2.544129]\n",
      "376 [D loss: 1.253219, acc.: 50.00%] [G loss: 2.713605]\n",
      "377 [D loss: 1.169060, acc.: 50.00%] [G loss: 2.720612]\n",
      "378 [D loss: 1.245731, acc.: 50.00%] [G loss: 2.574784]\n",
      "379 [D loss: 1.209033, acc.: 50.00%] [G loss: 2.672358]\n",
      "380 [D loss: 1.251922, acc.: 50.00%] [G loss: 2.611705]\n",
      "381 [D loss: 1.273127, acc.: 50.00%] [G loss: 2.643338]\n",
      "382 [D loss: 1.267650, acc.: 50.00%] [G loss: 2.613349]\n",
      "383 [D loss: 1.267136, acc.: 50.00%] [G loss: 2.661885]\n",
      "384 [D loss: 1.204672, acc.: 50.00%] [G loss: 2.642101]\n",
      "385 [D loss: 1.200849, acc.: 50.00%] [G loss: 2.539401]\n",
      "386 [D loss: 1.292216, acc.: 50.00%] [G loss: 2.558549]\n",
      "387 [D loss: 1.220578, acc.: 50.00%] [G loss: 2.690475]\n",
      "388 [D loss: 1.240016, acc.: 50.00%] [G loss: 2.568032]\n",
      "389 [D loss: 1.274709, acc.: 50.00%] [G loss: 2.593869]\n",
      "390 [D loss: 1.205936, acc.: 50.00%] [G loss: 2.618678]\n",
      "391 [D loss: 1.245221, acc.: 50.00%] [G loss: 2.552455]\n",
      "392 [D loss: 1.242791, acc.: 50.00%] [G loss: 2.604492]\n",
      "393 [D loss: 1.271280, acc.: 50.00%] [G loss: 2.604104]\n",
      "394 [D loss: 1.176502, acc.: 50.00%] [G loss: 2.636272]\n",
      "395 [D loss: 1.258771, acc.: 50.00%] [G loss: 2.580971]\n",
      "396 [D loss: 1.207428, acc.: 50.00%] [G loss: 2.591016]\n",
      "397 [D loss: 1.261251, acc.: 50.00%] [G loss: 2.544456]\n",
      "398 [D loss: 1.165760, acc.: 50.00%] [G loss: 2.582148]\n",
      "399 [D loss: 1.253084, acc.: 50.00%] [G loss: 2.536731]\n",
      "400 [D loss: 1.197053, acc.: 50.00%] [G loss: 2.619689]\n",
      "401 [D loss: 1.227710, acc.: 50.00%] [G loss: 2.539371]\n",
      "402 [D loss: 1.172882, acc.: 50.00%] [G loss: 2.585534]\n",
      "403 [D loss: 1.212494, acc.: 50.00%] [G loss: 2.404138]\n",
      "404 [D loss: 1.174660, acc.: 50.00%] [G loss: 2.579495]\n",
      "405 [D loss: 1.126498, acc.: 50.00%] [G loss: 2.573149]\n",
      "406 [D loss: 1.226058, acc.: 50.00%] [G loss: 2.644938]\n",
      "407 [D loss: 1.184145, acc.: 50.00%] [G loss: 2.546838]\n",
      "408 [D loss: 1.268235, acc.: 50.00%] [G loss: 2.564376]\n",
      "409 [D loss: 1.194554, acc.: 50.00%] [G loss: 2.526608]\n",
      "410 [D loss: 1.215103, acc.: 50.00%] [G loss: 2.580940]\n",
      "411 [D loss: 1.185848, acc.: 50.00%] [G loss: 2.434930]\n",
      "412 [D loss: 1.191944, acc.: 50.00%] [G loss: 2.516848]\n",
      "413 [D loss: 1.186797, acc.: 50.00%] [G loss: 2.463620]\n",
      "414 [D loss: 1.167239, acc.: 50.00%] [G loss: 2.528253]\n",
      "415 [D loss: 1.092032, acc.: 50.00%] [G loss: 2.546261]\n",
      "416 [D loss: 1.157348, acc.: 50.00%] [G loss: 2.467463]\n",
      "417 [D loss: 1.148840, acc.: 50.00%] [G loss: 2.506985]\n",
      "418 [D loss: 1.191528, acc.: 50.00%] [G loss: 2.458750]\n",
      "419 [D loss: 1.187675, acc.: 50.00%] [G loss: 2.510522]\n",
      "420 [D loss: 1.257154, acc.: 50.00%] [G loss: 2.591898]\n",
      "421 [D loss: 1.228267, acc.: 50.00%] [G loss: 2.512389]\n",
      "422 [D loss: 1.144512, acc.: 50.00%] [G loss: 2.521799]\n",
      "423 [D loss: 1.236427, acc.: 50.00%] [G loss: 2.594799]\n",
      "424 [D loss: 1.134105, acc.: 50.00%] [G loss: 2.612590]\n",
      "425 [D loss: 1.153365, acc.: 50.00%] [G loss: 2.522897]\n",
      "426 [D loss: 1.159609, acc.: 50.00%] [G loss: 2.614418]\n",
      "427 [D loss: 1.226549, acc.: 50.00%] [G loss: 2.476289]\n",
      "428 [D loss: 1.177283, acc.: 50.00%] [G loss: 2.611742]\n",
      "429 [D loss: 1.213248, acc.: 50.00%] [G loss: 2.472495]\n",
      "430 [D loss: 1.159060, acc.: 50.00%] [G loss: 2.588066]\n",
      "431 [D loss: 1.239687, acc.: 50.00%] [G loss: 2.521544]\n",
      "432 [D loss: 1.151451, acc.: 50.00%] [G loss: 2.679063]\n",
      "433 [D loss: 1.169313, acc.: 50.00%] [G loss: 2.517853]\n",
      "434 [D loss: 1.222787, acc.: 50.00%] [G loss: 2.458582]\n",
      "435 [D loss: 1.154643, acc.: 50.00%] [G loss: 2.631684]\n",
      "436 [D loss: 1.142724, acc.: 50.00%] [G loss: 2.472066]\n",
      "437 [D loss: 1.154994, acc.: 50.00%] [G loss: 2.510280]\n",
      "438 [D loss: 1.151057, acc.: 50.00%] [G loss: 2.557304]\n",
      "439 [D loss: 1.161885, acc.: 50.00%] [G loss: 2.564013]\n",
      "440 [D loss: 1.143915, acc.: 50.00%] [G loss: 2.559364]\n",
      "441 [D loss: 1.132370, acc.: 50.00%] [G loss: 2.498730]\n",
      "442 [D loss: 1.230899, acc.: 50.00%] [G loss: 2.491953]\n",
      "443 [D loss: 1.164335, acc.: 50.00%] [G loss: 2.482293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444 [D loss: 1.135211, acc.: 50.00%] [G loss: 2.491488]\n",
      "445 [D loss: 1.113622, acc.: 50.00%] [G loss: 2.476728]\n",
      "446 [D loss: 1.207978, acc.: 50.00%] [G loss: 2.452290]\n",
      "447 [D loss: 1.178055, acc.: 50.00%] [G loss: 2.464275]\n",
      "448 [D loss: 1.051241, acc.: 50.00%] [G loss: 2.497684]\n",
      "449 [D loss: 1.154449, acc.: 50.00%] [G loss: 2.516216]\n",
      "450 [D loss: 1.109884, acc.: 50.00%] [G loss: 2.481813]\n",
      "451 [D loss: 1.153408, acc.: 50.00%] [G loss: 2.480247]\n",
      "452 [D loss: 1.186598, acc.: 50.00%] [G loss: 2.512868]\n",
      "453 [D loss: 1.066421, acc.: 50.00%] [G loss: 2.391934]\n",
      "454 [D loss: 1.163452, acc.: 50.00%] [G loss: 2.540417]\n",
      "455 [D loss: 1.032024, acc.: 50.00%] [G loss: 2.460399]\n",
      "456 [D loss: 1.143509, acc.: 50.00%] [G loss: 2.434130]\n",
      "457 [D loss: 1.167480, acc.: 50.00%] [G loss: 2.442801]\n",
      "458 [D loss: 1.125498, acc.: 50.00%] [G loss: 2.497920]\n",
      "459 [D loss: 1.211866, acc.: 50.00%] [G loss: 2.431490]\n",
      "460 [D loss: 1.158190, acc.: 50.00%] [G loss: 2.420029]\n",
      "461 [D loss: 1.108116, acc.: 50.00%] [G loss: 2.443459]\n",
      "462 [D loss: 1.146372, acc.: 50.00%] [G loss: 2.438981]\n",
      "463 [D loss: 1.059491, acc.: 50.00%] [G loss: 2.387159]\n",
      "464 [D loss: 1.151731, acc.: 50.00%] [G loss: 2.439612]\n",
      "465 [D loss: 1.132976, acc.: 50.00%] [G loss: 2.501970]\n",
      "466 [D loss: 1.126815, acc.: 50.00%] [G loss: 2.447369]\n",
      "467 [D loss: 1.075604, acc.: 50.00%] [G loss: 2.436366]\n",
      "468 [D loss: 1.064644, acc.: 50.00%] [G loss: 2.501488]\n",
      "469 [D loss: 1.061173, acc.: 50.00%] [G loss: 2.467552]\n",
      "470 [D loss: 1.117080, acc.: 50.00%] [G loss: 2.486027]\n",
      "471 [D loss: 1.150576, acc.: 50.00%] [G loss: 2.500987]\n",
      "472 [D loss: 1.133965, acc.: 50.00%] [G loss: 2.442956]\n",
      "473 [D loss: 1.158714, acc.: 50.00%] [G loss: 2.341279]\n",
      "474 [D loss: 1.107795, acc.: 50.00%] [G loss: 2.475336]\n",
      "475 [D loss: 1.095027, acc.: 50.00%] [G loss: 2.421633]\n",
      "476 [D loss: 1.118258, acc.: 50.00%] [G loss: 2.484147]\n",
      "477 [D loss: 1.096572, acc.: 50.00%] [G loss: 2.448425]\n",
      "478 [D loss: 1.096829, acc.: 50.00%] [G loss: 2.455811]\n",
      "479 [D loss: 1.097897, acc.: 50.00%] [G loss: 2.424554]\n",
      "480 [D loss: 1.053108, acc.: 50.00%] [G loss: 2.452672]\n",
      "481 [D loss: 1.104388, acc.: 50.00%] [G loss: 2.438700]\n",
      "482 [D loss: 1.098877, acc.: 50.00%] [G loss: 2.437104]\n",
      "483 [D loss: 1.147931, acc.: 50.00%] [G loss: 2.411184]\n",
      "484 [D loss: 1.102857, acc.: 50.00%] [G loss: 2.419961]\n",
      "485 [D loss: 1.028265, acc.: 50.00%] [G loss: 2.298936]\n",
      "486 [D loss: 1.131212, acc.: 50.00%] [G loss: 2.367531]\n",
      "487 [D loss: 1.049610, acc.: 50.00%] [G loss: 2.375185]\n",
      "488 [D loss: 1.138507, acc.: 50.00%] [G loss: 2.434044]\n",
      "489 [D loss: 1.085151, acc.: 50.00%] [G loss: 2.423050]\n",
      "490 [D loss: 1.087507, acc.: 50.00%] [G loss: 2.509063]\n",
      "491 [D loss: 1.135947, acc.: 50.00%] [G loss: 2.432551]\n",
      "492 [D loss: 1.050001, acc.: 50.00%] [G loss: 2.420842]\n",
      "493 [D loss: 1.063697, acc.: 50.00%] [G loss: 2.405267]\n",
      "494 [D loss: 1.038308, acc.: 50.00%] [G loss: 2.404476]\n",
      "495 [D loss: 1.046809, acc.: 50.00%] [G loss: 2.414044]\n",
      "496 [D loss: 1.069485, acc.: 50.00%] [G loss: 2.357571]\n",
      "497 [D loss: 0.987589, acc.: 50.00%] [G loss: 2.387167]\n",
      "498 [D loss: 1.192235, acc.: 50.00%] [G loss: 2.489233]\n",
      "499 [D loss: 1.116719, acc.: 50.00%] [G loss: 2.332628]\n",
      "500 [D loss: 1.035531, acc.: 50.00%] [G loss: 2.370931]\n",
      "501 [D loss: 1.100139, acc.: 50.00%] [G loss: 2.340708]\n",
      "502 [D loss: 1.124668, acc.: 50.00%] [G loss: 2.367085]\n",
      "503 [D loss: 1.036189, acc.: 50.00%] [G loss: 2.368169]\n",
      "504 [D loss: 1.121105, acc.: 50.00%] [G loss: 2.438032]\n",
      "505 [D loss: 1.055431, acc.: 50.00%] [G loss: 2.421992]\n",
      "506 [D loss: 1.057712, acc.: 50.00%] [G loss: 2.419927]\n",
      "507 [D loss: 1.105641, acc.: 50.00%] [G loss: 2.429599]\n",
      "508 [D loss: 1.089492, acc.: 50.00%] [G loss: 2.416126]\n",
      "509 [D loss: 1.089319, acc.: 50.00%] [G loss: 2.369003]\n",
      "510 [D loss: 1.042192, acc.: 50.00%] [G loss: 2.409026]\n",
      "511 [D loss: 1.060039, acc.: 50.00%] [G loss: 2.354422]\n",
      "512 [D loss: 1.039044, acc.: 50.00%] [G loss: 2.364153]\n",
      "513 [D loss: 1.097918, acc.: 50.00%] [G loss: 2.380024]\n",
      "514 [D loss: 1.058637, acc.: 50.00%] [G loss: 2.401440]\n",
      "515 [D loss: 1.052356, acc.: 50.00%] [G loss: 2.357995]\n",
      "516 [D loss: 1.059760, acc.: 50.00%] [G loss: 2.389569]\n",
      "517 [D loss: 1.107656, acc.: 50.00%] [G loss: 2.288164]\n",
      "518 [D loss: 1.048210, acc.: 50.00%] [G loss: 2.338230]\n",
      "519 [D loss: 1.069917, acc.: 50.00%] [G loss: 2.304502]\n",
      "520 [D loss: 1.016282, acc.: 50.00%] [G loss: 2.370657]\n",
      "521 [D loss: 1.055308, acc.: 50.00%] [G loss: 2.441104]\n",
      "522 [D loss: 1.074703, acc.: 50.00%] [G loss: 2.345206]\n",
      "523 [D loss: 1.093053, acc.: 50.00%] [G loss: 2.379979]\n",
      "524 [D loss: 1.044963, acc.: 50.00%] [G loss: 2.313957]\n",
      "525 [D loss: 1.040496, acc.: 50.00%] [G loss: 2.303972]\n",
      "526 [D loss: 1.120837, acc.: 50.00%] [G loss: 2.328296]\n",
      "527 [D loss: 1.045070, acc.: 50.00%] [G loss: 2.329776]\n",
      "528 [D loss: 1.045419, acc.: 50.00%] [G loss: 2.332957]\n",
      "529 [D loss: 1.100915, acc.: 50.00%] [G loss: 2.315260]\n",
      "530 [D loss: 1.055505, acc.: 50.00%] [G loss: 2.358806]\n",
      "531 [D loss: 1.098793, acc.: 50.00%] [G loss: 2.374162]\n",
      "532 [D loss: 1.016299, acc.: 50.00%] [G loss: 2.367452]\n",
      "533 [D loss: 1.038395, acc.: 50.00%] [G loss: 2.348351]\n",
      "534 [D loss: 1.111861, acc.: 50.00%] [G loss: 2.331804]\n",
      "535 [D loss: 1.052293, acc.: 50.00%] [G loss: 2.245757]\n",
      "536 [D loss: 1.056142, acc.: 50.00%] [G loss: 2.257798]\n",
      "537 [D loss: 1.015414, acc.: 50.00%] [G loss: 2.450089]\n",
      "538 [D loss: 1.097327, acc.: 50.00%] [G loss: 2.258564]\n",
      "539 [D loss: 1.173541, acc.: 50.00%] [G loss: 2.316180]\n",
      "540 [D loss: 1.089304, acc.: 50.00%] [G loss: 2.289040]\n",
      "541 [D loss: 1.022344, acc.: 50.00%] [G loss: 2.367192]\n",
      "542 [D loss: 1.053695, acc.: 50.00%] [G loss: 2.363097]\n",
      "543 [D loss: 1.078339, acc.: 50.00%] [G loss: 2.362151]\n",
      "544 [D loss: 1.058217, acc.: 50.00%] [G loss: 2.397113]\n",
      "545 [D loss: 1.005182, acc.: 50.00%] [G loss: 2.276678]\n",
      "546 [D loss: 0.933135, acc.: 50.00%] [G loss: 2.338961]\n",
      "547 [D loss: 1.066075, acc.: 50.00%] [G loss: 2.320419]\n",
      "548 [D loss: 1.075418, acc.: 50.00%] [G loss: 2.345066]\n",
      "549 [D loss: 1.094287, acc.: 50.00%] [G loss: 2.269847]\n",
      "550 [D loss: 1.001041, acc.: 50.00%] [G loss: 2.375808]\n",
      "551 [D loss: 0.987789, acc.: 50.00%] [G loss: 2.381244]\n",
      "552 [D loss: 1.093553, acc.: 50.00%] [G loss: 2.346953]\n",
      "553 [D loss: 0.999831, acc.: 50.00%] [G loss: 2.289241]\n",
      "554 [D loss: 1.136413, acc.: 50.00%] [G loss: 2.258158]\n",
      "555 [D loss: 0.923889, acc.: 50.00%] [G loss: 2.273952]\n",
      "556 [D loss: 0.900498, acc.: 50.00%] [G loss: 2.410801]\n",
      "557 [D loss: 1.025042, acc.: 50.00%] [G loss: 2.242522]\n",
      "558 [D loss: 1.012329, acc.: 50.00%] [G loss: 2.247271]\n",
      "559 [D loss: 1.083906, acc.: 50.00%] [G loss: 2.337796]\n",
      "560 [D loss: 1.089245, acc.: 50.00%] [G loss: 2.384575]\n",
      "561 [D loss: 1.030556, acc.: 50.00%] [G loss: 2.319460]\n",
      "562 [D loss: 0.912662, acc.: 50.00%] [G loss: 2.176270]\n",
      "563 [D loss: 1.014896, acc.: 50.00%] [G loss: 2.282523]\n",
      "564 [D loss: 1.012245, acc.: 50.00%] [G loss: 2.329702]\n",
      "565 [D loss: 0.998188, acc.: 50.00%] [G loss: 2.324295]\n",
      "566 [D loss: 1.017957, acc.: 50.00%] [G loss: 2.267844]\n",
      "567 [D loss: 1.013133, acc.: 50.00%] [G loss: 2.305918]\n",
      "568 [D loss: 1.101061, acc.: 50.00%] [G loss: 2.354956]\n",
      "569 [D loss: 0.957488, acc.: 50.00%] [G loss: 2.175174]\n",
      "570 [D loss: 1.030131, acc.: 50.00%] [G loss: 2.287187]\n",
      "571 [D loss: 0.945861, acc.: 50.00%] [G loss: 2.314636]\n",
      "572 [D loss: 1.027849, acc.: 50.00%] [G loss: 2.282426]\n",
      "573 [D loss: 0.956604, acc.: 50.00%] [G loss: 2.237298]\n",
      "574 [D loss: 0.961487, acc.: 50.00%] [G loss: 2.243633]\n",
      "575 [D loss: 1.041843, acc.: 50.00%] [G loss: 2.316420]\n",
      "576 [D loss: 1.033085, acc.: 50.00%] [G loss: 2.233383]\n",
      "577 [D loss: 0.968729, acc.: 50.00%] [G loss: 2.277040]\n",
      "578 [D loss: 1.114189, acc.: 50.00%] [G loss: 2.213727]\n",
      "579 [D loss: 0.975533, acc.: 50.00%] [G loss: 2.310682]\n",
      "580 [D loss: 0.893021, acc.: 50.00%] [G loss: 2.200695]\n",
      "581 [D loss: 0.944872, acc.: 50.00%] [G loss: 2.223254]\n",
      "582 [D loss: 1.074470, acc.: 50.00%] [G loss: 2.295696]\n",
      "583 [D loss: 0.936487, acc.: 50.00%] [G loss: 2.350392]\n",
      "584 [D loss: 1.021840, acc.: 50.00%] [G loss: 2.248621]\n",
      "585 [D loss: 1.065232, acc.: 50.00%] [G loss: 2.266357]\n",
      "586 [D loss: 0.869793, acc.: 50.00%] [G loss: 2.293726]\n",
      "587 [D loss: 1.001987, acc.: 50.00%] [G loss: 2.349571]\n",
      "588 [D loss: 1.074745, acc.: 50.00%] [G loss: 2.217098]\n",
      "589 [D loss: 1.063577, acc.: 50.00%] [G loss: 2.293860]\n",
      "590 [D loss: 0.985842, acc.: 50.00%] [G loss: 2.287003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591 [D loss: 0.871210, acc.: 50.00%] [G loss: 2.280814]\n",
      "592 [D loss: 1.030178, acc.: 50.00%] [G loss: 2.312541]\n",
      "593 [D loss: 0.995778, acc.: 50.00%] [G loss: 2.267689]\n",
      "594 [D loss: 1.038607, acc.: 50.00%] [G loss: 2.266664]\n",
      "595 [D loss: 1.087044, acc.: 50.00%] [G loss: 2.248202]\n",
      "596 [D loss: 0.943376, acc.: 50.00%] [G loss: 2.293661]\n",
      "597 [D loss: 0.987809, acc.: 50.00%] [G loss: 2.296037]\n",
      "598 [D loss: 0.880498, acc.: 50.00%] [G loss: 2.207221]\n",
      "599 [D loss: 0.975846, acc.: 50.00%] [G loss: 2.214980]\n",
      "600 [D loss: 0.969116, acc.: 50.00%] [G loss: 2.229888]\n",
      "601 [D loss: 0.916150, acc.: 50.00%] [G loss: 2.281934]\n",
      "602 [D loss: 0.924923, acc.: 50.00%] [G loss: 2.072102]\n",
      "603 [D loss: 1.107460, acc.: 50.00%] [G loss: 2.111438]\n",
      "604 [D loss: 0.969732, acc.: 50.00%] [G loss: 2.164081]\n",
      "605 [D loss: 0.949559, acc.: 50.00%] [G loss: 2.209099]\n",
      "606 [D loss: 0.968257, acc.: 50.00%] [G loss: 2.200952]\n",
      "607 [D loss: 0.903859, acc.: 50.00%] [G loss: 2.198009]\n",
      "608 [D loss: 0.994721, acc.: 50.00%] [G loss: 2.138970]\n",
      "609 [D loss: 0.956423, acc.: 50.00%] [G loss: 2.227336]\n",
      "610 [D loss: 0.891178, acc.: 50.00%] [G loss: 2.250043]\n",
      "611 [D loss: 0.953100, acc.: 50.00%] [G loss: 2.151450]\n",
      "612 [D loss: 0.994186, acc.: 50.00%] [G loss: 2.210321]\n",
      "613 [D loss: 0.963536, acc.: 50.00%] [G loss: 2.256137]\n",
      "614 [D loss: 1.025495, acc.: 50.00%] [G loss: 2.232034]\n",
      "615 [D loss: 1.029310, acc.: 50.00%] [G loss: 2.206811]\n",
      "616 [D loss: 0.828844, acc.: 50.00%] [G loss: 2.266414]\n",
      "617 [D loss: 0.858216, acc.: 50.00%] [G loss: 2.200782]\n",
      "618 [D loss: 0.994468, acc.: 50.00%] [G loss: 2.169230]\n",
      "619 [D loss: 0.909412, acc.: 50.00%] [G loss: 1.781282]\n",
      "620 [D loss: 1.078216, acc.: 42.50%] [G loss: 2.903635]\n",
      "621 [D loss: 1.097342, acc.: 40.00%] [G loss: 2.595456]\n",
      "622 [D loss: 1.234198, acc.: 37.50%] [G loss: 3.030302]\n",
      "623 [D loss: 1.174158, acc.: 37.50%] [G loss: 1.354440]\n",
      "624 [D loss: 0.996230, acc.: 40.00%] [G loss: 3.186198]\n",
      "625 [D loss: 1.140311, acc.: 40.00%] [G loss: 3.253217]\n",
      "626 [D loss: 1.684866, acc.: 25.00%] [G loss: 3.258803]\n",
      "627 [D loss: 1.868218, acc.: 25.00%] [G loss: 2.378884]\n",
      "628 [D loss: 1.440893, acc.: 35.00%] [G loss: 6.097822]\n",
      "629 [D loss: 1.260787, acc.: 37.50%] [G loss: 4.610139]\n",
      "630 [D loss: 1.534152, acc.: 32.50%] [G loss: 2.291672]\n",
      "631 [D loss: 1.468210, acc.: 32.50%] [G loss: 8.286809]\n",
      "632 [D loss: 1.134223, acc.: 37.50%] [G loss: 6.008991]\n",
      "633 [D loss: 1.034668, acc.: 47.50%] [G loss: 6.667504]\n",
      "634 [D loss: 1.854085, acc.: 35.00%] [G loss: 4.300914]\n",
      "635 [D loss: 1.164108, acc.: 45.00%] [G loss: 6.404312]\n",
      "636 [D loss: 1.192937, acc.: 35.00%] [G loss: 6.901035]\n",
      "637 [D loss: 0.979271, acc.: 42.50%] [G loss: 7.989833]\n",
      "638 [D loss: 0.990936, acc.: 50.00%] [G loss: 5.939510]\n",
      "639 [D loss: 1.069512, acc.: 40.00%] [G loss: 6.575145]\n",
      "640 [D loss: 1.008523, acc.: 47.50%] [G loss: 7.586340]\n",
      "641 [D loss: 1.008249, acc.: 50.00%] [G loss: 8.356713]\n",
      "642 [D loss: 1.026389, acc.: 40.00%] [G loss: 4.007840]\n",
      "643 [D loss: 1.061591, acc.: 45.00%] [G loss: 6.391299]\n",
      "644 [D loss: 1.056943, acc.: 40.00%] [G loss: 8.462563]\n",
      "645 [D loss: 1.104645, acc.: 45.00%] [G loss: 5.586963]\n",
      "646 [D loss: 1.036280, acc.: 50.00%] [G loss: 6.143224]\n",
      "647 [D loss: 0.972431, acc.: 50.00%] [G loss: 4.629504]\n",
      "648 [D loss: 0.893337, acc.: 50.00%] [G loss: 6.976140]\n",
      "649 [D loss: 1.407192, acc.: 47.50%] [G loss: 8.155804]\n",
      "650 [D loss: 0.957762, acc.: 47.50%] [G loss: 7.552763]\n",
      "651 [D loss: 0.983159, acc.: 42.50%] [G loss: 11.028312]\n",
      "652 [D loss: 1.022983, acc.: 50.00%] [G loss: 4.548411]\n",
      "653 [D loss: 0.971873, acc.: 50.00%] [G loss: 6.746799]\n",
      "654 [D loss: 0.988361, acc.: 47.50%] [G loss: 4.390798]\n",
      "655 [D loss: 0.920336, acc.: 47.50%] [G loss: 5.289142]\n",
      "656 [D loss: 1.029870, acc.: 45.00%] [G loss: 7.005123]\n",
      "657 [D loss: 0.914233, acc.: 50.00%] [G loss: 8.039232]\n",
      "658 [D loss: 1.043990, acc.: 45.00%] [G loss: 7.134635]\n",
      "659 [D loss: 0.923216, acc.: 47.50%] [G loss: 8.561483]\n",
      "660 [D loss: 0.829554, acc.: 50.00%] [G loss: 7.560719]\n",
      "661 [D loss: 0.963625, acc.: 50.00%] [G loss: 8.934868]\n",
      "662 [D loss: 0.878681, acc.: 47.50%] [G loss: 5.249265]\n",
      "663 [D loss: 0.969279, acc.: 50.00%] [G loss: 3.716717]\n",
      "664 [D loss: 0.986990, acc.: 47.50%] [G loss: 6.548482]\n",
      "665 [D loss: 1.102573, acc.: 42.50%] [G loss: 8.228643]\n",
      "666 [D loss: 0.901515, acc.: 50.00%] [G loss: 5.933852]\n",
      "667 [D loss: 0.811286, acc.: 50.00%] [G loss: 8.163462]\n",
      "668 [D loss: 0.869018, acc.: 47.50%] [G loss: 10.293230]\n",
      "669 [D loss: 0.946805, acc.: 47.50%] [G loss: 8.163992]\n",
      "670 [D loss: 0.892809, acc.: 50.00%] [G loss: 6.967700]\n",
      "671 [D loss: 0.948717, acc.: 50.00%] [G loss: 4.480905]\n",
      "672 [D loss: 1.025148, acc.: 50.00%] [G loss: 7.354112]\n",
      "673 [D loss: 0.930724, acc.: 47.50%] [G loss: 6.750797]\n",
      "674 [D loss: 1.096355, acc.: 50.00%] [G loss: 6.007945]\n",
      "675 [D loss: 0.888731, acc.: 50.00%] [G loss: 8.177397]\n",
      "676 [D loss: 0.927531, acc.: 47.50%] [G loss: 8.240890]\n",
      "677 [D loss: 0.769165, acc.: 50.00%] [G loss: 8.751606]\n",
      "678 [D loss: 0.890865, acc.: 50.00%] [G loss: 3.848354]\n",
      "679 [D loss: 0.923611, acc.: 50.00%] [G loss: 7.696867]\n",
      "680 [D loss: 0.985521, acc.: 50.00%] [G loss: 6.169356]\n",
      "681 [D loss: 0.862801, acc.: 50.00%] [G loss: 8.318571]\n",
      "682 [D loss: 0.846848, acc.: 50.00%] [G loss: 5.954602]\n",
      "683 [D loss: 0.946750, acc.: 50.00%] [G loss: 7.145536]\n",
      "684 [D loss: 0.904888, acc.: 50.00%] [G loss: 7.552493]\n",
      "685 [D loss: 0.852060, acc.: 50.00%] [G loss: 5.154420]\n",
      "686 [D loss: 0.924798, acc.: 50.00%] [G loss: 5.523889]\n",
      "687 [D loss: 0.912632, acc.: 50.00%] [G loss: 6.925378]\n",
      "688 [D loss: 0.933255, acc.: 50.00%] [G loss: 8.066698]\n",
      "689 [D loss: 0.837086, acc.: 50.00%] [G loss: 8.728688]\n",
      "690 [D loss: 0.945477, acc.: 50.00%] [G loss: 6.932667]\n",
      "691 [D loss: 0.904161, acc.: 47.50%] [G loss: 8.168116]\n",
      "692 [D loss: 0.911776, acc.: 50.00%] [G loss: 5.893092]\n",
      "693 [D loss: 0.856870, acc.: 50.00%] [G loss: 6.037248]\n",
      "694 [D loss: 0.784289, acc.: 50.00%] [G loss: 5.956549]\n",
      "695 [D loss: 0.934227, acc.: 50.00%] [G loss: 7.788968]\n",
      "696 [D loss: 0.802282, acc.: 50.00%] [G loss: 8.901866]\n",
      "697 [D loss: 0.915344, acc.: 50.00%] [G loss: 5.729142]\n",
      "698 [D loss: 0.944148, acc.: 50.00%] [G loss: 5.844431]\n",
      "699 [D loss: 0.963921, acc.: 47.50%] [G loss: 7.314954]\n",
      "700 [D loss: 0.873512, acc.: 50.00%] [G loss: 6.179289]\n",
      "701 [D loss: 0.821556, acc.: 47.50%] [G loss: 7.738149]\n",
      "702 [D loss: 0.946126, acc.: 50.00%] [G loss: 7.888241]\n",
      "703 [D loss: 0.859476, acc.: 50.00%] [G loss: 6.609914]\n",
      "704 [D loss: 0.913071, acc.: 50.00%] [G loss: 9.221861]\n",
      "705 [D loss: 0.916888, acc.: 50.00%] [G loss: 6.099640]\n",
      "706 [D loss: 0.888508, acc.: 50.00%] [G loss: 7.907411]\n",
      "707 [D loss: 0.927233, acc.: 50.00%] [G loss: 6.100715]\n",
      "708 [D loss: 0.983503, acc.: 50.00%] [G loss: 9.252484]\n",
      "709 [D loss: 0.899352, acc.: 47.50%] [G loss: 9.746696]\n",
      "710 [D loss: 0.914272, acc.: 50.00%] [G loss: 9.601200]\n",
      "711 [D loss: 0.865740, acc.: 50.00%] [G loss: 7.331334]\n",
      "712 [D loss: 0.910290, acc.: 50.00%] [G loss: 7.778737]\n",
      "713 [D loss: 0.814803, acc.: 50.00%] [G loss: 9.630136]\n",
      "714 [D loss: 0.766329, acc.: 50.00%] [G loss: 8.073537]\n",
      "715 [D loss: 0.855172, acc.: 50.00%] [G loss: 9.454882]\n",
      "716 [D loss: 0.863702, acc.: 50.00%] [G loss: 8.453173]\n",
      "717 [D loss: 0.822484, acc.: 50.00%] [G loss: 6.609177]\n",
      "718 [D loss: 0.865671, acc.: 50.00%] [G loss: 7.494221]\n",
      "719 [D loss: 0.903704, acc.: 50.00%] [G loss: 8.083240]\n",
      "720 [D loss: 0.873721, acc.: 50.00%] [G loss: 7.263432]\n",
      "721 [D loss: 0.875738, acc.: 50.00%] [G loss: 8.385203]\n",
      "722 [D loss: 0.782010, acc.: 50.00%] [G loss: 8.275359]\n",
      "723 [D loss: 0.880340, acc.: 50.00%] [G loss: 5.332541]\n",
      "724 [D loss: 0.806632, acc.: 50.00%] [G loss: 8.862711]\n",
      "725 [D loss: 0.936837, acc.: 50.00%] [G loss: 8.109491]\n",
      "726 [D loss: 0.884242, acc.: 50.00%] [G loss: 8.938917]\n",
      "727 [D loss: 0.772611, acc.: 50.00%] [G loss: 9.286304]\n",
      "728 [D loss: 0.841920, acc.: 50.00%] [G loss: 4.326535]\n",
      "729 [D loss: 0.821497, acc.: 50.00%] [G loss: 4.604619]\n",
      "730 [D loss: 0.942816, acc.: 50.00%] [G loss: 6.150296]\n",
      "731 [D loss: 0.825996, acc.: 50.00%] [G loss: 8.145175]\n",
      "732 [D loss: 0.849489, acc.: 50.00%] [G loss: 8.474745]\n",
      "733 [D loss: 0.839566, acc.: 50.00%] [G loss: 8.055346]\n",
      "734 [D loss: 0.897847, acc.: 50.00%] [G loss: 6.139195]\n",
      "735 [D loss: 0.901638, acc.: 47.50%] [G loss: 6.771052]\n",
      "736 [D loss: 0.908466, acc.: 50.00%] [G loss: 4.099144]\n",
      "737 [D loss: 0.951900, acc.: 50.00%] [G loss: 8.117867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738 [D loss: 0.826664, acc.: 50.00%] [G loss: 6.361764]\n",
      "739 [D loss: 0.739913, acc.: 50.00%] [G loss: 8.190435]\n",
      "740 [D loss: 0.898615, acc.: 47.50%] [G loss: 5.365908]\n",
      "741 [D loss: 0.809016, acc.: 50.00%] [G loss: 8.576971]\n",
      "742 [D loss: 0.819749, acc.: 52.50%] [G loss: 5.710984]\n",
      "743 [D loss: 0.867350, acc.: 50.00%] [G loss: 9.142164]\n",
      "744 [D loss: 0.809684, acc.: 50.00%] [G loss: 8.081362]\n",
      "745 [D loss: 0.822044, acc.: 50.00%] [G loss: 7.329423]\n",
      "746 [D loss: 0.849089, acc.: 50.00%] [G loss: 8.977072]\n",
      "747 [D loss: 0.730374, acc.: 52.50%] [G loss: 10.245598]\n",
      "748 [D loss: 0.848657, acc.: 50.00%] [G loss: 8.736257]\n",
      "749 [D loss: 0.838251, acc.: 50.00%] [G loss: 6.220009]\n",
      "750 [D loss: 0.802158, acc.: 50.00%] [G loss: 6.904691]\n",
      "751 [D loss: 0.779423, acc.: 50.00%] [G loss: 6.131171]\n",
      "752 [D loss: 0.827705, acc.: 50.00%] [G loss: 7.242630]\n",
      "753 [D loss: 0.880707, acc.: 50.00%] [G loss: 5.422708]\n",
      "754 [D loss: 0.841416, acc.: 50.00%] [G loss: 6.594401]\n",
      "755 [D loss: 0.888797, acc.: 47.50%] [G loss: 8.368410]\n",
      "756 [D loss: 0.874950, acc.: 50.00%] [G loss: 6.514904]\n",
      "757 [D loss: 0.891012, acc.: 50.00%] [G loss: 8.391230]\n",
      "758 [D loss: 1.009428, acc.: 47.50%] [G loss: 6.902405]\n",
      "759 [D loss: 0.792334, acc.: 55.00%] [G loss: 5.170049]\n",
      "760 [D loss: 0.856204, acc.: 50.00%] [G loss: 11.005020]\n",
      "761 [D loss: 0.755293, acc.: 50.00%] [G loss: 5.236878]\n",
      "762 [D loss: 0.829948, acc.: 50.00%] [G loss: 6.433475]\n",
      "763 [D loss: 0.804970, acc.: 50.00%] [G loss: 9.508173]\n",
      "764 [D loss: 0.831016, acc.: 50.00%] [G loss: 7.515900]\n",
      "765 [D loss: 0.791948, acc.: 50.00%] [G loss: 8.512038]\n",
      "766 [D loss: 0.912011, acc.: 50.00%] [G loss: 8.148096]\n",
      "767 [D loss: 0.913168, acc.: 50.00%] [G loss: 6.677246]\n",
      "768 [D loss: 0.754314, acc.: 52.50%] [G loss: 6.076589]\n",
      "769 [D loss: 0.824691, acc.: 50.00%] [G loss: 5.800404]\n",
      "770 [D loss: 0.832454, acc.: 50.00%] [G loss: 5.924674]\n",
      "771 [D loss: 0.852754, acc.: 50.00%] [G loss: 7.338034]\n",
      "772 [D loss: 0.796047, acc.: 50.00%] [G loss: 7.877992]\n",
      "773 [D loss: 0.888532, acc.: 50.00%] [G loss: 7.219084]\n",
      "774 [D loss: 0.771704, acc.: 50.00%] [G loss: 5.212670]\n",
      "775 [D loss: 0.854270, acc.: 50.00%] [G loss: 8.037498]\n",
      "776 [D loss: 0.884895, acc.: 50.00%] [G loss: 7.871748]\n",
      "777 [D loss: 0.753592, acc.: 50.00%] [G loss: 6.352572]\n",
      "778 [D loss: 0.859235, acc.: 50.00%] [G loss: 7.408513]\n",
      "779 [D loss: 0.826479, acc.: 52.50%] [G loss: 7.908456]\n",
      "780 [D loss: 0.839495, acc.: 47.50%] [G loss: 4.739005]\n",
      "781 [D loss: 0.857579, acc.: 47.50%] [G loss: 6.563220]\n",
      "782 [D loss: 0.794597, acc.: 50.00%] [G loss: 5.312269]\n",
      "783 [D loss: 0.793008, acc.: 52.50%] [G loss: 7.588191]\n",
      "784 [D loss: 0.787344, acc.: 50.00%] [G loss: 8.217169]\n",
      "785 [D loss: 0.771488, acc.: 47.50%] [G loss: 6.108443]\n",
      "786 [D loss: 0.938425, acc.: 50.00%] [G loss: 4.188378]\n",
      "787 [D loss: 0.895924, acc.: 47.50%] [G loss: 5.896944]\n",
      "788 [D loss: 0.786077, acc.: 52.50%] [G loss: 4.595660]\n",
      "789 [D loss: 0.805102, acc.: 52.50%] [G loss: 7.868843]\n",
      "790 [D loss: 0.804598, acc.: 50.00%] [G loss: 7.916738]\n",
      "791 [D loss: 0.815071, acc.: 55.00%] [G loss: 8.345576]\n",
      "792 [D loss: 0.799285, acc.: 47.50%] [G loss: 4.051750]\n",
      "793 [D loss: 0.828654, acc.: 50.00%] [G loss: 9.083937]\n",
      "794 [D loss: 0.774882, acc.: 50.00%] [G loss: 5.655386]\n",
      "795 [D loss: 0.743720, acc.: 50.00%] [G loss: 6.844489]\n",
      "796 [D loss: 0.734524, acc.: 52.50%] [G loss: 5.850183]\n",
      "797 [D loss: 0.813580, acc.: 52.50%] [G loss: 6.313542]\n",
      "798 [D loss: 0.798689, acc.: 52.50%] [G loss: 7.901446]\n",
      "799 [D loss: 0.815712, acc.: 55.00%] [G loss: 5.925748]\n",
      "800 [D loss: 0.891597, acc.: 52.50%] [G loss: 8.004565]\n",
      "801 [D loss: 0.772538, acc.: 50.00%] [G loss: 8.260954]\n",
      "802 [D loss: 0.807811, acc.: 52.50%] [G loss: 5.884872]\n",
      "803 [D loss: 0.750052, acc.: 47.50%] [G loss: 5.125970]\n",
      "804 [D loss: 0.741415, acc.: 52.50%] [G loss: 2.587942]\n",
      "805 [D loss: 0.883758, acc.: 50.00%] [G loss: 3.698302]\n",
      "806 [D loss: 0.796206, acc.: 52.50%] [G loss: 6.581066]\n",
      "807 [D loss: 0.774248, acc.: 50.00%] [G loss: 8.133444]\n",
      "808 [D loss: 0.754118, acc.: 55.00%] [G loss: 7.616994]\n",
      "809 [D loss: 0.911206, acc.: 47.50%] [G loss: 6.530227]\n",
      "810 [D loss: 0.866715, acc.: 50.00%] [G loss: 6.571272]\n",
      "811 [D loss: 0.742102, acc.: 52.50%] [G loss: 7.293190]\n",
      "812 [D loss: 0.692959, acc.: 52.50%] [G loss: 6.344179]\n",
      "813 [D loss: 0.842788, acc.: 47.50%] [G loss: 3.623012]\n",
      "814 [D loss: 0.760875, acc.: 55.00%] [G loss: 7.348405]\n",
      "815 [D loss: 0.884127, acc.: 50.00%] [G loss: 6.741309]\n",
      "816 [D loss: 0.914572, acc.: 52.50%] [G loss: 7.583472]\n",
      "817 [D loss: 0.856787, acc.: 52.50%] [G loss: 5.099932]\n",
      "818 [D loss: 0.932523, acc.: 52.50%] [G loss: 4.638685]\n",
      "819 [D loss: 0.815880, acc.: 55.00%] [G loss: 8.679774]\n",
      "820 [D loss: 0.803933, acc.: 50.00%] [G loss: 4.902240]\n",
      "821 [D loss: 0.778935, acc.: 50.00%] [G loss: 5.603614]\n",
      "822 [D loss: 0.773356, acc.: 50.00%] [G loss: 5.114532]\n",
      "823 [D loss: 0.785499, acc.: 55.00%] [G loss: 8.851452]\n",
      "824 [D loss: 0.733914, acc.: 52.50%] [G loss: 4.202057]\n",
      "825 [D loss: 0.839126, acc.: 57.50%] [G loss: 8.197420]\n",
      "826 [D loss: 0.776105, acc.: 52.50%] [G loss: 8.017149]\n",
      "827 [D loss: 0.673695, acc.: 50.00%] [G loss: 4.354455]\n",
      "828 [D loss: 0.785513, acc.: 50.00%] [G loss: 5.929476]\n",
      "829 [D loss: 0.787236, acc.: 50.00%] [G loss: 7.309685]\n",
      "830 [D loss: 0.820257, acc.: 50.00%] [G loss: 7.179888]\n",
      "831 [D loss: 0.771645, acc.: 52.50%] [G loss: 7.382715]\n",
      "832 [D loss: 0.714434, acc.: 57.50%] [G loss: 4.419314]\n",
      "833 [D loss: 0.818173, acc.: 52.50%] [G loss: 8.129633]\n",
      "834 [D loss: 0.738107, acc.: 52.50%] [G loss: 5.161584]\n",
      "835 [D loss: 0.859643, acc.: 47.50%] [G loss: 4.114673]\n",
      "836 [D loss: 0.769197, acc.: 55.00%] [G loss: 6.590825]\n",
      "837 [D loss: 0.786456, acc.: 55.00%] [G loss: 6.517062]\n",
      "838 [D loss: 0.763797, acc.: 52.50%] [G loss: 3.664822]\n",
      "839 [D loss: 0.794726, acc.: 55.00%] [G loss: 3.669509]\n",
      "840 [D loss: 0.715371, acc.: 52.50%] [G loss: 5.973428]\n",
      "841 [D loss: 0.750185, acc.: 50.00%] [G loss: 5.942590]\n",
      "842 [D loss: 0.890493, acc.: 45.00%] [G loss: 2.966806]\n",
      "843 [D loss: 0.736854, acc.: 50.00%] [G loss: 6.004131]\n",
      "844 [D loss: 0.719357, acc.: 62.50%] [G loss: 5.637803]\n",
      "845 [D loss: 0.829359, acc.: 55.00%] [G loss: 6.522167]\n",
      "846 [D loss: 0.763901, acc.: 57.50%] [G loss: 9.351949]\n",
      "847 [D loss: 0.829816, acc.: 55.00%] [G loss: 7.361022]\n",
      "848 [D loss: 0.804644, acc.: 50.00%] [G loss: 8.000300]\n",
      "849 [D loss: 0.840793, acc.: 52.50%] [G loss: 6.758325]\n",
      "850 [D loss: 0.832982, acc.: 57.50%] [G loss: 8.189817]\n",
      "851 [D loss: 0.862007, acc.: 52.50%] [G loss: 8.286599]\n",
      "852 [D loss: 0.897310, acc.: 47.50%] [G loss: 6.338347]\n",
      "853 [D loss: 0.724047, acc.: 60.00%] [G loss: 4.332963]\n",
      "854 [D loss: 0.803084, acc.: 47.50%] [G loss: 6.587609]\n",
      "855 [D loss: 0.728860, acc.: 52.50%] [G loss: 4.789810]\n",
      "856 [D loss: 0.749299, acc.: 57.50%] [G loss: 6.184151]\n",
      "857 [D loss: 0.759324, acc.: 52.50%] [G loss: 11.908800]\n",
      "858 [D loss: 0.674175, acc.: 55.00%] [G loss: 7.478986]\n",
      "859 [D loss: 0.736905, acc.: 50.00%] [G loss: 7.658142]\n",
      "860 [D loss: 0.755401, acc.: 55.00%] [G loss: 8.177427]\n",
      "861 [D loss: 0.737412, acc.: 50.00%] [G loss: 7.400990]\n",
      "862 [D loss: 0.727153, acc.: 52.50%] [G loss: 6.042068]\n",
      "863 [D loss: 0.658059, acc.: 55.00%] [G loss: 8.073523]\n",
      "864 [D loss: 0.696699, acc.: 50.00%] [G loss: 7.460822]\n",
      "865 [D loss: 0.617893, acc.: 60.00%] [G loss: 5.213320]\n",
      "866 [D loss: 0.781407, acc.: 50.00%] [G loss: 8.153905]\n",
      "867 [D loss: 0.831951, acc.: 55.00%] [G loss: 5.274619]\n",
      "868 [D loss: 0.694313, acc.: 57.50%] [G loss: 5.832496]\n",
      "869 [D loss: 0.757316, acc.: 52.50%] [G loss: 6.726456]\n",
      "870 [D loss: 0.763552, acc.: 50.00%] [G loss: 8.781046]\n",
      "871 [D loss: 0.661385, acc.: 60.00%] [G loss: 7.873075]\n",
      "872 [D loss: 0.698495, acc.: 52.50%] [G loss: 8.891655]\n",
      "873 [D loss: 0.738026, acc.: 52.50%] [G loss: 7.262999]\n",
      "874 [D loss: 0.810567, acc.: 52.50%] [G loss: 7.503759]\n",
      "875 [D loss: 0.738655, acc.: 55.00%] [G loss: 7.403854]\n",
      "876 [D loss: 0.671635, acc.: 50.00%] [G loss: 8.038385]\n",
      "877 [D loss: 0.673369, acc.: 60.00%] [G loss: 9.983004]\n",
      "878 [D loss: 0.772015, acc.: 52.50%] [G loss: 10.084239]\n",
      "879 [D loss: 0.746242, acc.: 50.00%] [G loss: 6.607484]\n",
      "880 [D loss: 0.709760, acc.: 55.00%] [G loss: 8.760253]\n",
      "881 [D loss: 0.745872, acc.: 55.00%] [G loss: 6.282837]\n",
      "882 [D loss: 0.679406, acc.: 60.00%] [G loss: 9.613396]\n",
      "883 [D loss: 0.609213, acc.: 60.00%] [G loss: 5.565129]\n",
      "884 [D loss: 0.709762, acc.: 55.00%] [G loss: 7.237991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885 [D loss: 0.731159, acc.: 55.00%] [G loss: 6.010399]\n",
      "886 [D loss: 0.791729, acc.: 57.50%] [G loss: 8.063124]\n",
      "887 [D loss: 0.642992, acc.: 60.00%] [G loss: 4.465185]\n",
      "888 [D loss: 0.733069, acc.: 52.50%] [G loss: 5.617355]\n",
      "889 [D loss: 0.749316, acc.: 52.50%] [G loss: 8.042116]\n",
      "890 [D loss: 0.748415, acc.: 55.00%] [G loss: 5.542595]\n",
      "891 [D loss: 0.841223, acc.: 52.50%] [G loss: 5.165083]\n",
      "892 [D loss: 0.733380, acc.: 52.50%] [G loss: 8.233009]\n",
      "893 [D loss: 0.752853, acc.: 52.50%] [G loss: 8.753809]\n",
      "894 [D loss: 0.661101, acc.: 62.50%] [G loss: 6.413085]\n",
      "895 [D loss: 0.739621, acc.: 57.50%] [G loss: 5.825082]\n",
      "896 [D loss: 0.755308, acc.: 52.50%] [G loss: 4.401139]\n",
      "897 [D loss: 0.630294, acc.: 60.00%] [G loss: 5.666683]\n",
      "898 [D loss: 0.754666, acc.: 50.00%] [G loss: 7.148555]\n",
      "899 [D loss: 0.762280, acc.: 57.50%] [G loss: 8.739190]\n",
      "900 [D loss: 0.758488, acc.: 57.50%] [G loss: 5.943063]\n",
      "901 [D loss: 0.631076, acc.: 57.50%] [G loss: 6.648524]\n",
      "902 [D loss: 0.699723, acc.: 57.50%] [G loss: 6.905040]\n",
      "903 [D loss: 0.642021, acc.: 67.50%] [G loss: 5.049661]\n",
      "904 [D loss: 0.838586, acc.: 50.00%] [G loss: 5.701980]\n",
      "905 [D loss: 0.816791, acc.: 50.00%] [G loss: 4.499394]\n",
      "906 [D loss: 0.853602, acc.: 50.00%] [G loss: 9.626133]\n",
      "907 [D loss: 0.764469, acc.: 55.00%] [G loss: 5.749667]\n",
      "908 [D loss: 0.829613, acc.: 52.50%] [G loss: 5.860671]\n",
      "909 [D loss: 0.777060, acc.: 55.00%] [G loss: 5.119945]\n",
      "910 [D loss: 0.721837, acc.: 60.00%] [G loss: 5.852820]\n",
      "911 [D loss: 0.730927, acc.: 55.00%] [G loss: 7.993636]\n",
      "912 [D loss: 0.695962, acc.: 52.50%] [G loss: 7.325639]\n",
      "913 [D loss: 0.727592, acc.: 60.00%] [G loss: 6.391161]\n",
      "914 [D loss: 0.790894, acc.: 52.50%] [G loss: 6.638196]\n",
      "915 [D loss: 0.758466, acc.: 55.00%] [G loss: 4.685287]\n",
      "916 [D loss: 0.797165, acc.: 50.00%] [G loss: 4.914568]\n",
      "917 [D loss: 0.799307, acc.: 52.50%] [G loss: 7.332087]\n",
      "918 [D loss: 0.790838, acc.: 55.00%] [G loss: 6.131725]\n",
      "919 [D loss: 0.841770, acc.: 57.50%] [G loss: 6.800752]\n",
      "920 [D loss: 0.782743, acc.: 47.50%] [G loss: 4.384787]\n",
      "921 [D loss: 0.728616, acc.: 55.00%] [G loss: 8.063478]\n",
      "922 [D loss: 0.895486, acc.: 52.50%] [G loss: 7.279948]\n",
      "923 [D loss: 0.706612, acc.: 55.00%] [G loss: 6.424034]\n",
      "924 [D loss: 0.769971, acc.: 55.00%] [G loss: 7.515471]\n",
      "925 [D loss: 0.854237, acc.: 50.00%] [G loss: 7.305214]\n",
      "926 [D loss: 0.776686, acc.: 55.00%] [G loss: 8.821198]\n",
      "927 [D loss: 0.760546, acc.: 62.50%] [G loss: 2.841300]\n",
      "928 [D loss: 0.746823, acc.: 60.00%] [G loss: 9.376020]\n",
      "929 [D loss: 0.704277, acc.: 57.50%] [G loss: 2.755900]\n",
      "930 [D loss: 0.754400, acc.: 57.50%] [G loss: 6.994823]\n",
      "931 [D loss: 0.687162, acc.: 60.00%] [G loss: 7.926391]\n",
      "932 [D loss: 0.702433, acc.: 60.00%] [G loss: 4.919742]\n",
      "933 [D loss: 0.640282, acc.: 52.50%] [G loss: 9.448303]\n",
      "934 [D loss: 0.681419, acc.: 57.50%] [G loss: 4.152163]\n",
      "935 [D loss: 0.850496, acc.: 50.00%] [G loss: 7.403604]\n",
      "936 [D loss: 0.768647, acc.: 60.00%] [G loss: 5.712553]\n",
      "937 [D loss: 0.732320, acc.: 50.00%] [G loss: 8.118958]\n",
      "938 [D loss: 0.708202, acc.: 52.50%] [G loss: 6.358558]\n",
      "939 [D loss: 0.813762, acc.: 50.00%] [G loss: 3.430724]\n",
      "940 [D loss: 0.724792, acc.: 55.00%] [G loss: 4.801376]\n",
      "941 [D loss: 0.572185, acc.: 65.00%] [G loss: 4.623082]\n",
      "942 [D loss: 0.784900, acc.: 57.50%] [G loss: 4.901413]\n",
      "943 [D loss: 0.699556, acc.: 57.50%] [G loss: 5.092745]\n",
      "944 [D loss: 0.685680, acc.: 65.00%] [G loss: 7.209525]\n",
      "945 [D loss: 0.587352, acc.: 65.00%] [G loss: 7.776002]\n",
      "946 [D loss: 0.599510, acc.: 60.00%] [G loss: 4.409438]\n",
      "947 [D loss: 0.711595, acc.: 57.50%] [G loss: 6.417531]\n",
      "948 [D loss: 0.587461, acc.: 52.50%] [G loss: 7.788606]\n",
      "949 [D loss: 0.663717, acc.: 60.00%] [G loss: 3.314885]\n",
      "950 [D loss: 0.657420, acc.: 57.50%] [G loss: 6.060655]\n",
      "951 [D loss: 0.746109, acc.: 55.00%] [G loss: 5.815862]\n",
      "952 [D loss: 0.566604, acc.: 62.50%] [G loss: 8.190204]\n",
      "953 [D loss: 0.688804, acc.: 57.50%] [G loss: 8.274973]\n",
      "954 [D loss: 0.660390, acc.: 60.00%] [G loss: 10.175632]\n",
      "955 [D loss: 0.785212, acc.: 55.00%] [G loss: 6.375127]\n",
      "956 [D loss: 0.773058, acc.: 57.50%] [G loss: 3.532311]\n",
      "957 [D loss: 0.722586, acc.: 55.00%] [G loss: 4.917747]\n",
      "958 [D loss: 0.603566, acc.: 62.50%] [G loss: 5.921890]\n",
      "959 [D loss: 0.739826, acc.: 57.50%] [G loss: 7.116907]\n",
      "960 [D loss: 0.644353, acc.: 57.50%] [G loss: 8.734549]\n",
      "961 [D loss: 0.631409, acc.: 67.50%] [G loss: 5.540209]\n",
      "962 [D loss: 0.589126, acc.: 65.00%] [G loss: 7.263001]\n",
      "963 [D loss: 0.708721, acc.: 60.00%] [G loss: 7.909911]\n",
      "964 [D loss: 0.657783, acc.: 60.00%] [G loss: 8.138211]\n",
      "965 [D loss: 0.729993, acc.: 57.50%] [G loss: 6.093195]\n",
      "966 [D loss: 0.767452, acc.: 57.50%] [G loss: 7.941785]\n",
      "967 [D loss: 0.688164, acc.: 57.50%] [G loss: 5.217421]\n",
      "968 [D loss: 0.698037, acc.: 52.50%] [G loss: 6.602562]\n",
      "969 [D loss: 0.699584, acc.: 55.00%] [G loss: 5.503797]\n",
      "970 [D loss: 0.705890, acc.: 60.00%] [G loss: 4.339761]\n",
      "971 [D loss: 0.756847, acc.: 57.50%] [G loss: 7.890124]\n",
      "972 [D loss: 0.818384, acc.: 55.00%] [G loss: 5.661977]\n",
      "973 [D loss: 0.708250, acc.: 52.50%] [G loss: 7.930930]\n",
      "974 [D loss: 0.768364, acc.: 52.50%] [G loss: 7.051433]\n",
      "975 [D loss: 0.901784, acc.: 42.50%] [G loss: 7.046149]\n",
      "976 [D loss: 0.655587, acc.: 65.00%] [G loss: 5.681695]\n",
      "977 [D loss: 0.766329, acc.: 62.50%] [G loss: 5.827838]\n",
      "978 [D loss: 0.642356, acc.: 62.50%] [G loss: 5.622109]\n",
      "979 [D loss: 0.603861, acc.: 65.00%] [G loss: 7.278885]\n",
      "980 [D loss: 0.824914, acc.: 50.00%] [G loss: 8.758098]\n",
      "981 [D loss: 0.824585, acc.: 52.50%] [G loss: 5.537127]\n",
      "982 [D loss: 0.685341, acc.: 65.00%] [G loss: 7.851848]\n",
      "983 [D loss: 0.757169, acc.: 55.00%] [G loss: 5.733338]\n",
      "984 [D loss: 0.703339, acc.: 60.00%] [G loss: 3.844601]\n",
      "985 [D loss: 0.707943, acc.: 60.00%] [G loss: 5.044290]\n",
      "986 [D loss: 0.607429, acc.: 67.50%] [G loss: 7.113954]\n",
      "987 [D loss: 0.755924, acc.: 55.00%] [G loss: 4.351701]\n",
      "988 [D loss: 0.711073, acc.: 62.50%] [G loss: 5.610738]\n",
      "989 [D loss: 0.716808, acc.: 52.50%] [G loss: 7.358596]\n",
      "990 [D loss: 0.809228, acc.: 47.50%] [G loss: 5.849492]\n",
      "991 [D loss: 0.693492, acc.: 55.00%] [G loss: 6.268447]\n",
      "992 [D loss: 0.654947, acc.: 62.50%] [G loss: 6.789346]\n",
      "993 [D loss: 0.688392, acc.: 60.00%] [G loss: 6.483742]\n",
      "994 [D loss: 0.766121, acc.: 57.50%] [G loss: 7.271759]\n",
      "995 [D loss: 0.744074, acc.: 57.50%] [G loss: 6.269900]\n",
      "996 [D loss: 0.688152, acc.: 60.00%] [G loss: 7.380676]\n",
      "997 [D loss: 0.732011, acc.: 62.50%] [G loss: 5.022582]\n",
      "998 [D loss: 0.765492, acc.: 60.00%] [G loss: 6.032865]\n",
      "999 [D loss: 0.757660, acc.: 55.00%] [G loss: 7.147456]\n"
     ]
    }
   ],
   "source": [
    "lstmgan.train(epochs=1000, batch_size=20, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0h 6m 44s\n"
     ]
    }
   ],
   "source": [
    "end = datetime.datetime.now()\n",
    "\n",
    "diff = (end - start)\n",
    "\n",
    "datetime.timedelta(seconds=10, microseconds=885206)\n",
    "\n",
    "diff_seconds = int(diff.total_seconds())\n",
    "\n",
    "minute_seconds, seconds = divmod(diff_seconds, 60)\n",
    "hours, minutes = divmod(minute_seconds, 60)\n",
    "hms = f\"{hours}h {minutes}m {seconds}s\"\n",
    "\n",
    "'0h 0m 10s'\n",
    "print(hms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.34644547,  0.3232346 , -0.09182598,  0.21038857,  0.16755736,\n",
      "         0.12925172, -0.22083065, -0.06553719, -0.06917598,  0.02495553,\n",
      "        -0.31523803,  0.14694108, -0.3284582 ,  0.11354214,  0.33405226,\n",
      "        -0.21792482,  0.25912294, -0.24647751,  0.29054227,  0.26930898,\n",
      "        -0.16998976,  0.05294639,  0.08474829, -0.1248821 , -0.1012631 ,\n",
      "        -0.04891429,  0.06839203, -0.31024393, -0.19715972, -0.18473455,\n",
      "        -0.18837819,  0.04380918, -0.37868482,  0.12583238,  0.12585643,\n",
      "        -0.33095884, -0.31553274, -0.10647061, -0.13748038, -0.24025969,\n",
      "        -0.2641708 ,  0.31665835,  0.17512165, -0.29450563,  0.12276299,\n",
      "         0.12555745,  0.09658702,  0.1457905 , -0.26247212, -0.3424894 ,\n",
      "        -0.16801439,  0.14585987,  0.1487592 , -0.05755034, -0.13580103,\n",
      "         0.19954696, -0.14324228, -0.1964317 , -0.23880158, -0.1921513 ,\n",
      "        -0.35020667,  0.05475725, -0.36547914, -0.15800108]],\n",
      "      dtype=float32), array([[[-0.1301151 , -0.09251974,  0.00766946, ...,  0.05033582,\n",
      "         -0.08995011,  0.0922545 ],\n",
      "        [ 0.10684913, -0.06773368, -0.02626266, ..., -0.11489373,\n",
      "         -0.10083054, -0.03538565],\n",
      "        [-0.05852935, -0.13305214, -0.0570346 , ..., -0.02750913,\n",
      "         -0.00045805,  0.02474446],\n",
      "        ...,\n",
      "        [-0.05444095,  0.01336722,  0.11151189, ...,  0.05630303,\n",
      "         -0.06491657,  0.11269725],\n",
      "        [ 0.04404929,  0.13612436,  0.13396461, ...,  0.05163274,\n",
      "         -0.0105278 , -0.0084793 ],\n",
      "        [ 0.0556939 ,  0.03818534,  0.09505824, ..., -0.11363452,\n",
      "          0.10835919, -0.15575717]],\n",
      "\n",
      "       [[ 0.03282361,  0.11883805,  0.06079097, ..., -0.02508935,\n",
      "          0.06899612, -0.1367418 ],\n",
      "        [ 0.00876848,  0.03037381,  0.04053435, ...,  0.10915563,\n",
      "          0.10327917, -0.07978908],\n",
      "        [-0.04002538, -0.00570084, -0.12651318, ...,  0.05695588,\n",
      "          0.04267381, -0.1122928 ],\n",
      "        ...,\n",
      "        [-0.10102689, -0.09712411,  0.05435856, ..., -0.0422874 ,\n",
      "         -0.14518812, -0.00798952],\n",
      "        [-0.08458532,  0.09629739,  0.11348189, ..., -0.08158623,\n",
      "         -0.078593  , -0.04627063],\n",
      "        [ 0.00907719, -0.0250429 ,  0.07961427, ..., -0.03240559,\n",
      "          0.07744371, -0.1417851 ]],\n",
      "\n",
      "       [[ 0.18853906, -0.1502256 , -0.01038939, ...,  0.057096  ,\n",
      "         -0.01203443, -0.05005874],\n",
      "        [ 0.05230597, -0.05553912,  0.10749233, ...,  0.02274075,\n",
      "          0.02027921, -0.14376122],\n",
      "        [ 0.07177805,  0.03400826,  0.06471904, ..., -0.04371317,\n",
      "         -0.14622658, -0.08358172],\n",
      "        ...,\n",
      "        [-0.09101677, -0.01059095,  0.00236454, ..., -0.10295928,\n",
      "         -0.12258404, -0.047893  ],\n",
      "        [ 0.11591002,  0.06415405,  0.1370301 , ..., -0.04911977,\n",
      "         -0.07933369, -0.07741271],\n",
      "        [-0.05072742, -0.01157821,  0.16038807, ...,  0.15443079,\n",
      "          0.01256516, -0.09519762]]], dtype=float32), array([[[-0.24992585],\n",
      "        [ 0.00081499],\n",
      "        [-0.24031568],\n",
      "        [-0.1489243 ],\n",
      "        [-0.05973188],\n",
      "        [ 0.15688929],\n",
      "        [-0.16010298],\n",
      "        [ 0.0301971 ],\n",
      "        [-0.07692846],\n",
      "        [-0.08549457],\n",
      "        [-0.05372098],\n",
      "        [-0.1359093 ],\n",
      "        [-0.10714784],\n",
      "        [-0.19152586],\n",
      "        [ 0.26783407],\n",
      "        [ 0.13860507],\n",
      "        [ 0.14700592],\n",
      "        [ 0.15454881],\n",
      "        [ 0.18712106],\n",
      "        [-0.1742494 ],\n",
      "        [ 0.20010069],\n",
      "        [ 0.23190863],\n",
      "        [ 0.25945553],\n",
      "        [ 0.11873161],\n",
      "        [-0.1681372 ],\n",
      "        [ 0.25294605],\n",
      "        [-0.19381216],\n",
      "        [ 0.08762179],\n",
      "        [ 0.16583174],\n",
      "        [-0.20782748],\n",
      "        [ 0.16306295],\n",
      "        [-0.18835315]],\n",
      "\n",
      "       [[-0.01198762],\n",
      "        [ 0.145852  ],\n",
      "        [-0.09934749],\n",
      "        [-0.3071791 ],\n",
      "        [ 0.1460591 ],\n",
      "        [ 0.02868498],\n",
      "        [-0.03332251],\n",
      "        [-0.25391927],\n",
      "        [ 0.03875964],\n",
      "        [ 0.13039134],\n",
      "        [-0.04973942],\n",
      "        [ 0.2413297 ],\n",
      "        [ 0.23291463],\n",
      "        [ 0.02713392],\n",
      "        [-0.3277028 ],\n",
      "        [ 0.24728337],\n",
      "        [-0.20551254],\n",
      "        [ 0.05568576],\n",
      "        [ 0.21474145],\n",
      "        [-0.16419692],\n",
      "        [-0.16526127],\n",
      "        [-0.24511763],\n",
      "        [-0.35730216],\n",
      "        [ 0.0737057 ],\n",
      "        [-0.20322111],\n",
      "        [-0.22629666],\n",
      "        [ 0.18659003],\n",
      "        [ 0.08573357],\n",
      "        [-0.27734217],\n",
      "        [ 0.05172577],\n",
      "        [ 0.03684831],\n",
      "        [-0.18003201]],\n",
      "\n",
      "       [[ 0.04998072],\n",
      "        [ 0.00267001],\n",
      "        [ 0.09660465],\n",
      "        [-0.07918776],\n",
      "        [-0.06713474],\n",
      "        [ 0.15360269],\n",
      "        [ 0.21648161],\n",
      "        [-0.01977213],\n",
      "        [-0.22408351],\n",
      "        [-0.20413522],\n",
      "        [-0.13336283],\n",
      "        [ 0.0711722 ],\n",
      "        [-0.15463264],\n",
      "        [ 0.18436071],\n",
      "        [-0.15735944],\n",
      "        [ 0.01833931],\n",
      "        [ 0.2700811 ],\n",
      "        [ 0.20990749],\n",
      "        [ 0.02125277],\n",
      "        [-0.23835272],\n",
      "        [-0.31866753],\n",
      "        [ 0.04998383],\n",
      "        [-0.20458382],\n",
      "        [-0.0941212 ],\n",
      "        [-0.27507573],\n",
      "        [ 0.14613111],\n",
      "        [ 0.19247797],\n",
      "        [-0.14839053],\n",
      "        [ 0.25056008],\n",
      "        [ 0.26452747],\n",
      "        [ 0.2296548 ],\n",
      "        [-0.24441788]]], dtype=float32), array([[-0.14055645,  0.20709333,  0.23232524, -0.15305355,  0.23710181,\n",
      "        -0.12060128, -0.2843722 , -0.05110889, -0.12040715,  0.06915838,\n",
      "         0.1445793 ,  0.04741874,  0.03503725,  0.40683606,  0.38192537,\n",
      "        -0.05392134, -0.2826439 ,  0.11419173,  0.0478872 , -0.01530756,\n",
      "         0.15102234, -0.15380332, -0.2867944 , -0.10190298, -0.23550124,\n",
      "        -0.23357739, -0.08314858, -0.090443  ,  0.03369324,  0.15480037,\n",
      "        -0.03249454, -0.35012883],\n",
      "       [ 0.18257354, -0.3482778 ,  0.3440465 ,  0.29605538, -0.02161227,\n",
      "        -0.08929699,  0.27937862, -0.22151376, -0.13866886,  0.24717595,\n",
      "        -0.5195784 ,  0.30365327, -0.10378762,  0.16257285, -0.01790661,\n",
      "         0.318314  , -0.38232318,  0.28641248, -0.10848413,  0.2875044 ,\n",
      "        -0.32054862, -0.08320607, -0.04349156,  0.3365033 ,  0.34518614,\n",
      "         0.18398546, -0.10693916,  0.22800513,  0.31686068, -0.39159536,\n",
      "         0.02829962, -0.02732962],\n",
      "       [ 0.36484367, -0.14443108,  0.3561528 ,  0.00828074, -0.05684856,\n",
      "        -0.1391319 ,  0.26173854,  0.01381344,  0.08750645, -0.19650498,\n",
      "        -0.23495927, -0.23847415,  0.17294756, -0.17844401,  0.09740693,\n",
      "         0.2956654 ,  0.27481994, -0.16454063, -0.18044464, -0.3562212 ,\n",
      "        -0.2863791 , -0.21509045,  0.13398843, -0.19120802,  0.04028654,\n",
      "        -0.03578592, -0.13264792,  0.2872349 ,  0.21397054, -0.3649556 ,\n",
      "        -0.15173098, -0.09158838],\n",
      "       [-0.3101911 ,  0.38724026,  0.3642447 ,  0.02743372,  0.14830615,\n",
      "         0.26717153,  0.3472331 ,  0.07629777, -0.14901654, -0.1291098 ,\n",
      "         0.25253856, -0.18060495,  0.4191851 , -0.29970995, -0.21852371,\n",
      "        -0.15252255,  0.2173421 ,  0.25242847,  0.04166511, -0.01649235,\n",
      "         0.10083441,  0.39279336,  0.13683578,  0.16383088,  0.23199165,\n",
      "         0.4517865 ,  0.24744223, -0.17231463, -0.18036883,  0.3050121 ,\n",
      "         0.04942301, -0.3643999 ],\n",
      "       [-0.36258498,  0.32247928,  0.06774487,  0.13468955, -0.326905  ,\n",
      "         0.08574484,  0.18217027,  0.24783882, -0.02358945,  0.44888598,\n",
      "        -0.18548116,  0.20338261,  0.06443675,  0.29795334,  0.3220254 ,\n",
      "         0.23228116,  0.18401201,  0.22401197,  0.0663694 ,  0.09671439,\n",
      "        -0.22162245, -0.0157272 ,  0.3311468 , -0.45169136, -0.39987367,\n",
      "        -0.09915398,  0.06612132,  0.0747419 ,  0.27617875, -0.33428004,\n",
      "        -0.3183493 ,  0.397149  ]], dtype=float32), array([-0.00319529,  0.02849557,  0.01957621,  0.0242953 ,  0.05243331,\n",
      "        0.03309862,  0.02853761,  0.0386097 ,  0.04613449, -0.00065691,\n",
      "        0.03267705,  0.01707822, -0.0205232 , -0.00303248, -0.01655233,\n",
      "       -0.00131051,  0.04767417,  0.02608252, -0.00630598, -0.00215499,\n",
      "        0.02145665,  0.03662374, -0.00629025, -0.00541174, -0.00663527,\n",
      "        0.01518031, -0.01845673, -0.00283805, -0.00083044,  0.03120528,\n",
      "       -0.00339317, -0.00845253], dtype=float32), array([[-0.29239947],\n",
      "       [ 0.46228656],\n",
      "       [ 0.19234656],\n",
      "       [ 0.46022096],\n",
      "       [ 0.11684152],\n",
      "       [ 0.16521657],\n",
      "       [ 0.40723255],\n",
      "       [ 0.4895394 ],\n",
      "       [ 0.5535146 ],\n",
      "       [-0.20996305],\n",
      "       [ 0.28067118],\n",
      "       [ 0.09840405],\n",
      "       [-0.13025689],\n",
      "       [-0.40077487],\n",
      "       [-0.23695849],\n",
      "       [-0.21350138],\n",
      "       [ 0.2505213 ],\n",
      "       [ 0.07094184],\n",
      "       [ 0.01929682],\n",
      "       [-0.25326353],\n",
      "       [ 0.45909885],\n",
      "       [ 0.28866097],\n",
      "       [-0.17292574],\n",
      "       [-0.07465115],\n",
      "       [-0.39308423],\n",
      "       [ 0.2722564 ],\n",
      "       [-0.189407  ],\n",
      "       [-0.33226854],\n",
      "       [-0.39824817],\n",
      "       [ 0.1810686 ],\n",
      "       [-0.4142268 ],\n",
      "       [-0.25918508]], dtype=float32), array([0.02211175], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(lstmgan.discriminator.layers[1].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)]\n",
      "(1, 40000, 1, 20)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "hidden_layers = keras.backend.function(\n",
    "[lstmgan.discriminator.layers[0].input],  # we will feed the function with the input of the first layer  \n",
    "[lstmgan.discriminator.layers[0].output,] # we want to get the output of the first layer\n",
    ")\n",
    "h=hidden_layers([df_train])\n",
    "print(h)\n",
    "print(np.shape(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=np.array(h)  \n",
    "\n",
    "arr_reshaped = np.array(h).reshape(40000, 20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 20)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(arr_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('abnormal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.         0.         0.         0.         0.         0.1171062\n",
      "   0.         0.         0.31030872 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "#To check 1st row\n",
    "print(h[:,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_1 = pd.read_csv('abnormal.csv')\n",
    "csv_2 = pd.read_csv('abnormallabel.csv')\n",
    "\n",
    "result = pd.concat([csv_1, csv_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"lgcabnormalresult.csv\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1145401c5c8>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['gen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11464936e88>,\n",
       " <matplotlib.lines.Line2D at 0x1146493e048>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['dis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature12</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.049896017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074576885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0035594553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012225969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019287573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06614828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05703238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052905086</td>\n",
       "      <td>0.043763362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031135669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006044184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.05211395</td>\n",
       "      <td>0.0006721932400000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01591806</td>\n",
       "      <td>0.02435071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0016837418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02647767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.04974308</td>\n",
       "      <td>0.003009851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0029502455</td>\n",
       "      <td>0.017709356000000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0066348887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025008397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04832898</td>\n",
       "      <td>0.0042979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014554087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009138886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024143761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature1               feature2 feature3     feature4      feature5  \\\n",
       "0  0.049896017                    0.0      0.0  0.074576885           0.0   \n",
       "1   0.05703238                    0.0      0.0          0.0   0.052905086   \n",
       "2   0.05211395  0.0006721932400000001      0.0          0.0    0.01591806   \n",
       "3   0.04974308            0.003009851      0.0          0.0  0.0029502455   \n",
       "4   0.04832898              0.0042979      0.0          0.0           0.0   \n",
       "\n",
       "               feature6 feature7      feature8 feature9    feature10  ...  \\\n",
       "0          0.0035594553      0.0   0.012225969      0.0  0.019287573  ...   \n",
       "1           0.043763362      0.0           0.0      0.0  0.031135669  ...   \n",
       "2            0.02435071      0.0  0.0016837418      0.0   0.02647767  ...   \n",
       "3  0.017709356000000002      0.0  0.0066348887      0.0  0.025008397  ...   \n",
       "4           0.014554087      0.0   0.009138886      0.0  0.024143761  ...   \n",
       "\n",
       "  feature12 feature13 feature14    feature15 feature16   feature17 feature18  \\\n",
       "0       0.0       0.0       0.0          0.0       0.0  0.06614828       0.0   \n",
       "1       0.0       0.0       0.0  0.006044184       0.0         0.0       0.0   \n",
       "2       0.0       0.0       0.0          0.0       0.0         0.0       0.0   \n",
       "3       0.0       0.0       0.0          0.0       0.0         0.0       0.0   \n",
       "4       0.0       0.0       0.0          0.0       0.0         0.0       0.0   \n",
       "\n",
       "  feature19 feature20 label  \n",
       "0       0.0       0.0     0  \n",
       "1       0.0       0.0     0  \n",
       "2       0.0       0.0     0  \n",
       "3       0.0       0.0     0  \n",
       "4       0.0       0.0     0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test=df1.label\n",
    "\n",
    "x_test=df1.drop('label',axis=1).values.reshape(-1,20,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0\n",
       "          ..\n",
       "1048571    0\n",
       "1048572    0\n",
       "1048573    0\n",
       "1048574    0\n",
       "1048575    0\n",
       "Name: label, Length: 1048576, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.07970018]]\n",
      "\n",
      " [[0.18410666]]\n",
      "\n",
      " [[0.17668222]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.18324757]]\n",
      "\n",
      " [[0.18296733]]\n",
      "\n",
      " [[0.18284294]]]\n"
     ]
    }
   ],
   "source": [
    "#y_pred=lstmgan.generator.predict(x_test)\n",
    "y_pred=lstmgan.discriminator.predict(x_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, accuracy_score ,recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'y_pred')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#range of output value\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(range(len(y_pred)),y_pred.reshape(-1,1))\n",
    "plt.ylabel('y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob =-0.2 and f1 score =0.0 : accuracy=0.5681972503662109 : precision=0.43180274963378906 : recall=0.0\n",
      "1048576 0\n",
      "prob =-0.1 and f1 score =0.0 : accuracy=0.5681972503662109 : precision=0.43180274963378906 : recall=0.0\n",
      "1048576 0\n",
      "prob =-0.05 and f1 score =0.0 : accuracy=0.5681972503662109 : precision=0.43180274963378906 : recall=0.0\n",
      "1048576 0\n",
      "prob =0 and f1 score =0.0 : accuracy=0.5681972503662109 : precision=0.43180274963378906 : recall=0.0\n",
      "1048576 0\n",
      "prob =0.005 and f1 score =0.0 : accuracy=0.5681972503662109 : precision=0.43180274963378906 : recall=0.0\n",
      "1048576 0\n",
      "prob =0.1 and f1 score =0.00014575134819997084 : accuracy=0.5682144165039062 : precision=0.4318213857187183 : recall=7.288339981182831e-05\n",
      "1048528 48\n",
      "prob =0.5 and f1 score =0.6031586175935972 : accuracy=0.43180179595947266 : precision=0.4318022077593448 : recall=0.999997791412127\n",
      "1 1048575\n",
      "prob =1 and f1 score =0.6031586175935972 : accuracy=0.43180179595947266 : precision=0.4318022077593448 : recall=0.999997791412127\n",
      "1 1048575\n",
      "prob =2 and f1 score =0.6031595479813555 : accuracy=0.43180274963378906 : precision=0.43180274963378906 : recall=1.0\n",
      "0 1048576\n"
     ]
    }
   ],
   "source": [
    "#looking for the optimal probability threshold\n",
    "#prob=[0.5,0.55,0.6,0.65,0.75,0.8,0.85,0.9,0.95,1,2,2.5,5]\n",
    "prob=[-0.2,-0.1,-0.05,0,0.005,0.1,.5,1,2]\n",
    "for p in prob:\n",
    "    pred_value =[1 if i<p   else 0 for i in y_pred]\n",
    "    f1=f1_score(y_test,pred_value)\n",
    "    acc=accuracy_score(y_test,pred_value)\n",
    "    precision=average_precision_score(y_test,pred_value)\n",
    "    recall= recall_score(y_test,pred_value)\n",
    "    print(f'prob ={p} and f1 score ={f1} : accuracy={acc} : precision={precision} : recall={recall}')\n",
    "    print(pred_value.count(0),pred_value.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=dict()\n",
    "history['gen']=[]\n",
    "history['dis']=[]\n",
    "def train1(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for batch in dataset:\n",
    "\n",
    "           gen_loss,dis_loss= lstmgan.train(batch)\n",
    "        history['gen'].append(gen_loss)\n",
    "        history['dis'].append(dis_loss)\n",
    "        print ('Time for epoch {} is {} sec '.format(epoch + 1, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
