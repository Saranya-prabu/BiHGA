{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/function9/bidirectional-lstm-gan-music-generation\n",
    "#https://www.kaggle.com/abhisheksinha28/bidirectional-lstm/data\n",
    "#https://www.kaggle.com/sekfook97/gan-for-anomaly-detection\n",
    "# importing libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Bidirectional, LSTM, Reshape, RepeatVector, TimeDistributed\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "\"feature1\": np.float16,\n",
    "\"feature2\": np.float16,\n",
    "\"feature3\": np.float16,\n",
    "\"feature4\": np.float16,\n",
    "\"feature5\": np.float16,\n",
    "\"feature6\": np.float16,\n",
    "\"feature7\": np.float16,\n",
    "\"feature8\": np.float16,\n",
    "\"feature9\": np.float16,\n",
    "\"feature10\": np.float16,\n",
    "\"feature11\": np.float16,\n",
    "\"feature12\": np.float16,\n",
    "\"feature13\": np.float16,\n",
    "\"feature14\": np.float16,\n",
    "\"feature15\": np.float16,\n",
    "\"feature16\": np.float16,\n",
    "\"feature17\": np.float16,\n",
    "\"feature18\": np.float16,\n",
    "\"feature19\": np.float16,\n",
    "\"feature20\": np.float16,\n",
    "\"label\": np.object}    \n",
    "columns = [\"feature1\",\"feature2\",\"feature3\",\"feature4\",\"feature5\",\"feature6\",\"feature7\",\"feature8\",\"feature9\",\"feature10\",\"feature11\",\"feature12\",\"feature13\",\"feature14\",\"feature15\",\"feature16\",\"feature17\",\"feature18\",\"feature19\",\"feature20\",\"label\"]\n",
    "#df = pd.read_csv(\"/kaggle/input/kdd-cup-1999-data/kddcup.data_10_percent_corrected\", sep=\",\", names=columns, dtype=dtypes, index_col=None)\n",
    "df = pd.read_csv(r\"C:\\Users\\admin\\ablation study - 20 neurons\\afterdimensionalityreduction\",sep=\",\", names=columns, dtype=str, index_col=None)\n",
    "#df_read = pd.read_csv(savefile, dtype=str, index_col=0)\n",
    "#print(type(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal      595798\n",
       "Abnormal    452778\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             feature1               feature2 feature3     feature4  \\\n",
       "0        0.049896017                    0.0      0.0  0.074576885   \n",
       "1         0.05703238                    0.0      0.0          0.0   \n",
       "2         0.05211395  0.0006721932400000001      0.0          0.0   \n",
       "3         0.04974308            0.003009851      0.0          0.0   \n",
       "4         0.04832898              0.0042979      0.0          0.0   \n",
       "...              ...                    ...      ...          ...   \n",
       "1048571  0.023582537            0.011685466      0.0          0.0   \n",
       "1048572  0.023108114             0.01054538      0.0          0.0   \n",
       "1048573  0.022631112            0.008008906      0.0          0.0   \n",
       "1048574  0.022127744            0.008818104      0.0          0.0   \n",
       "1048575   0.02162299   0.008149143000000001      0.0          0.0   \n",
       "\n",
       "             feature5              feature6 feature7              feature8  \\\n",
       "0                 0.0          0.0035594553      0.0           0.012225969   \n",
       "1         0.052905086           0.043763362      0.0                   0.0   \n",
       "2          0.01591806            0.02435071      0.0          0.0016837418   \n",
       "3        0.0029502455  0.017709356000000002      0.0          0.0066348887   \n",
       "4                 0.0           0.014554087      0.0           0.009138886   \n",
       "...               ...                   ...      ...                   ...   \n",
       "1048571           0.0           0.005893277      0.0           0.046702452   \n",
       "1048572           0.0          0.0058220904      0.0            0.04695243   \n",
       "1048573           0.0           0.005821051      0.0  0.047111400000000005   \n",
       "1048574           0.0           0.005707981      0.0           0.047399394   \n",
       "1048575           0.0  0.005666818499999999      0.0  0.047594800000000013   \n",
       "\n",
       "        feature9    feature10  ... feature12 feature13 feature14    feature15  \\\n",
       "0            0.0  0.019287573  ...       0.0       0.0       0.0          0.0   \n",
       "1            0.0  0.031135669  ...       0.0       0.0       0.0  0.006044184   \n",
       "2            0.0   0.02647767  ...       0.0       0.0       0.0          0.0   \n",
       "3            0.0  0.025008397  ...       0.0       0.0       0.0          0.0   \n",
       "4            0.0  0.024143761  ...       0.0       0.0       0.0          0.0   \n",
       "...          ...          ...  ...       ...       ...       ...          ...   \n",
       "1048571      0.0          0.0  ...       0.0       0.0       0.0  0.015696432   \n",
       "1048572      0.0          0.0  ...       0.0       0.0       0.0  0.015654337   \n",
       "1048573      0.0          0.0  ...       0.0       0.0       0.0  0.015895598   \n",
       "1048574      0.0          0.0  ...       0.0       0.0       0.0  0.015462209   \n",
       "1048575      0.0          0.0  ...       0.0       0.0       0.0  0.015328575   \n",
       "\n",
       "        feature16   feature17 feature18 feature19 feature20   label  \n",
       "0             0.0  0.06614828       0.0       0.0       0.0  Normal  \n",
       "1             0.0         0.0       0.0       0.0       0.0  Normal  \n",
       "2             0.0         0.0       0.0       0.0       0.0  Normal  \n",
       "3             0.0         0.0       0.0       0.0       0.0  Normal  \n",
       "4             0.0         0.0       0.0       0.0       0.0  Normal  \n",
       "...           ...         ...       ...       ...       ...     ...  \n",
       "1048571       0.0         0.0       0.0       0.0       0.0  Normal  \n",
       "1048572       0.0         0.0       0.0       0.0       0.0  Normal  \n",
       "1048573       0.0         0.0       0.0       0.0       0.0  Normal  \n",
       "1048574       0.0         0.0       0.0       0.0       0.0  Normal  \n",
       "1048575       0.0         0.0       0.0       0.0       0.0  Normal  \n",
       "\n",
       "[1048576 rows x 21 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 for normal data and 1 for abnormalities\n",
    "df.label=df.label.apply(lambda x: 0 if x == 'Normal' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048576, 21)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             feature1               feature2 feature3     feature4  \\\n",
       "0        0.049896017                    0.0      0.0  0.074576885   \n",
       "1         0.05703238                    0.0      0.0          0.0   \n",
       "2         0.05211395  0.0006721932400000001      0.0          0.0   \n",
       "3         0.04974308            0.003009851      0.0          0.0   \n",
       "4         0.04832898              0.0042979      0.0          0.0   \n",
       "...              ...                    ...      ...          ...   \n",
       "1048571  0.023582537            0.011685466      0.0          0.0   \n",
       "1048572  0.023108114             0.01054538      0.0          0.0   \n",
       "1048573  0.022631112            0.008008906      0.0          0.0   \n",
       "1048574  0.022127744            0.008818104      0.0          0.0   \n",
       "1048575   0.02162299   0.008149143000000001      0.0          0.0   \n",
       "\n",
       "             feature5              feature6 feature7              feature8  \\\n",
       "0                 0.0          0.0035594553      0.0           0.012225969   \n",
       "1         0.052905086           0.043763362      0.0                   0.0   \n",
       "2          0.01591806            0.02435071      0.0          0.0016837418   \n",
       "3        0.0029502455  0.017709356000000002      0.0          0.0066348887   \n",
       "4                 0.0           0.014554087      0.0           0.009138886   \n",
       "...               ...                   ...      ...                   ...   \n",
       "1048571           0.0           0.005893277      0.0           0.046702452   \n",
       "1048572           0.0          0.0058220904      0.0            0.04695243   \n",
       "1048573           0.0           0.005821051      0.0  0.047111400000000005   \n",
       "1048574           0.0           0.005707981      0.0           0.047399394   \n",
       "1048575           0.0  0.005666818499999999      0.0  0.047594800000000013   \n",
       "\n",
       "        feature9    feature10  ... feature12 feature13 feature14    feature15  \\\n",
       "0            0.0  0.019287573  ...       0.0       0.0       0.0          0.0   \n",
       "1            0.0  0.031135669  ...       0.0       0.0       0.0  0.006044184   \n",
       "2            0.0   0.02647767  ...       0.0       0.0       0.0          0.0   \n",
       "3            0.0  0.025008397  ...       0.0       0.0       0.0          0.0   \n",
       "4            0.0  0.024143761  ...       0.0       0.0       0.0          0.0   \n",
       "...          ...          ...  ...       ...       ...       ...          ...   \n",
       "1048571      0.0          0.0  ...       0.0       0.0       0.0  0.015696432   \n",
       "1048572      0.0          0.0  ...       0.0       0.0       0.0  0.015654337   \n",
       "1048573      0.0          0.0  ...       0.0       0.0       0.0  0.015895598   \n",
       "1048574      0.0          0.0  ...       0.0       0.0       0.0  0.015462209   \n",
       "1048575      0.0          0.0  ...       0.0       0.0       0.0  0.015328575   \n",
       "\n",
       "        feature16   feature17 feature18 feature19 feature20 label  \n",
       "0             0.0  0.06614828       0.0       0.0       0.0     0  \n",
       "1             0.0         0.0       0.0       0.0       0.0     0  \n",
       "2             0.0         0.0       0.0       0.0       0.0     0  \n",
       "3             0.0         0.0       0.0       0.0       0.0     0  \n",
       "4             0.0         0.0       0.0       0.0       0.0     0  \n",
       "...           ...         ...       ...       ...       ...   ...  \n",
       "1048571       0.0         0.0       0.0       0.0       0.0     0  \n",
       "1048572       0.0         0.0       0.0       0.0       0.0     0  \n",
       "1048573       0.0         0.0       0.0       0.0       0.0     0  \n",
       "1048574       0.0         0.0       0.0       0.0       0.0     0  \n",
       "1048575       0.0         0.0       0.0       0.0       0.0     0  \n",
       "\n",
       "[1048576 rows x 21 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     object\n",
       "feature2     object\n",
       "feature3     object\n",
       "feature4     object\n",
       "feature5     object\n",
       "feature6     object\n",
       "feature7     object\n",
       "feature8     object\n",
       "feature9     object\n",
       "feature10    object\n",
       "feature11    object\n",
       "feature12    object\n",
       "feature13    object\n",
       "feature14    object\n",
       "feature15    object\n",
       "feature16    object\n",
       "feature17    object\n",
       "feature18    object\n",
       "feature19    object\n",
       "feature20    object\n",
       "label         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df)   \n",
    " \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['feature1'] = df['feature1'].astype(float) \n",
    "df['feature2'] = df['feature2'].astype(float) \n",
    "df['feature3'] = df['feature3'].astype(float) \n",
    "df['feature4'] = df['feature4'].astype(float) \n",
    "df['feature5'] = df['feature5'].astype(float) \n",
    "df['feature6'] = df['feature6'].astype(float) \n",
    "df['feature7'] = df['feature7'].astype(float) \n",
    "df['feature8'] = df['feature8'].astype(float) \n",
    "df['feature9'] = df['feature9'].astype(float) \n",
    "df['feature10'] = df['feature10'].astype(float) \n",
    "df['feature11'] = df['feature11'].astype(float) \n",
    "df['feature12'] = df['feature12'].astype(float) \n",
    "df['feature13'] = df['feature13'].astype(float) \n",
    "df['feature14'] = df['feature14'].astype(float) \n",
    "df['feature15'] = df['feature15'].astype(float) \n",
    "df['feature16'] = df['feature16'].astype(float) \n",
    "df['feature17'] = df['feature17'].astype(float) \n",
    "df['feature18'] = df['feature18'].astype(float) \n",
    "df['feature19'] = df['feature19'].astype(float) \n",
    "df['feature20'] = df['feature20'].astype(float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly sample 500 data point for training\n",
    "df_train=df[df.label==0].sample(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 21)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     float64\n",
       "feature2     float64\n",
       "feature3     float64\n",
       "feature4     float64\n",
       "feature5     float64\n",
       "feature6     float64\n",
       "feature7     float64\n",
       "feature8     float64\n",
       "feature9     float64\n",
       "feature10    float64\n",
       "feature11    float64\n",
       "feature12    float64\n",
       "feature13    float64\n",
       "feature14    float64\n",
       "feature15    float64\n",
       "feature16    float64\n",
       "feature17    float64\n",
       "feature18    float64\n",
       "feature19    float64\n",
       "feature20    float64\n",
       "label          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the item sampled from our dataset\n",
    "index_list=df_train.index\n",
    "df=df.drop(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     0.036062\n",
       "feature2     0.002170\n",
       "feature3     0.000000\n",
       "feature4     0.000000\n",
       "feature5     0.000000\n",
       "feature6     0.010692\n",
       "feature7     0.000000\n",
       "feature8     0.022869\n",
       "feature9     0.000000\n",
       "feature10    0.028666\n",
       "feature11    0.000000\n",
       "feature12    0.000000\n",
       "feature13    0.000000\n",
       "feature14    0.000000\n",
       "feature15    0.000000\n",
       "feature16    0.000000\n",
       "feature17    0.000000\n",
       "feature18    0.000000\n",
       "feature19    0.000000\n",
       "feature20    0.000000\n",
       "label        0.000000\n",
       "Name: 42677, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature12</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.049896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.057032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052905</td>\n",
       "      <td>0.043763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.052114</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.024351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.048329</td>\n",
       "      <td>0.004298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.047246</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0  0.049896  0.000000       0.0  0.074577  0.000000  0.003559       0.0   \n",
       "1  0.057032  0.000000       0.0  0.000000  0.052905  0.043763       0.0   \n",
       "2  0.052114  0.000672       0.0  0.000000  0.015918  0.024351       0.0   \n",
       "4  0.048329  0.004298       0.0  0.000000  0.000000  0.014554       0.0   \n",
       "5  0.047246  0.004409       0.0  0.000000  0.000000  0.012579       0.0   \n",
       "\n",
       "   feature8  feature9  feature10  ...  feature12  feature13  feature14  \\\n",
       "0  0.012226       0.0   0.019288  ...        0.0        0.0        0.0   \n",
       "1  0.000000       0.0   0.031136  ...        0.0        0.0        0.0   \n",
       "2  0.001684       0.0   0.026478  ...        0.0        0.0        0.0   \n",
       "4  0.009139       0.0   0.024144  ...        0.0        0.0        0.0   \n",
       "5  0.010811       0.0   0.023702  ...        0.0        0.0        0.0   \n",
       "\n",
       "   feature15  feature16  feature17  feature18  feature19  feature20  label  \n",
       "0   0.000000        0.0   0.066148        0.0        0.0        0.0      0  \n",
       "1   0.006044        0.0   0.000000        0.0        0.0        0.0      0  \n",
       "2   0.000000        0.0   0.000000        0.0        0.0        0.0      0  \n",
       "4   0.000000        0.0   0.000000        0.0        0.0        0.0      0  \n",
       "5   0.000000        0.0   0.000000        0.0        0.0        0.0      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the label columns\n",
    "df_label=df_train.label\n",
    "df_train=df_train.drop('label',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label\n",
    "df_label.to_csv('normallabel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 20)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     0.036062\n",
       "feature2     0.002170\n",
       "feature3     0.000000\n",
       "feature4     0.000000\n",
       "feature5     0.000000\n",
       "feature6     0.010692\n",
       "feature7     0.000000\n",
       "feature8     0.022869\n",
       "feature9     0.000000\n",
       "feature10    0.028666\n",
       "feature11    0.000000\n",
       "feature12    0.000000\n",
       "feature13    0.000000\n",
       "feature14    0.000000\n",
       "feature15    0.000000\n",
       "feature16    0.000000\n",
       "feature17    0.000000\n",
       "feature18    0.000000\n",
       "feature19    0.000000\n",
       "feature20    0.000000\n",
       "Name: 42677, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 20)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.values.reshape((df_train.shape[0], 1, df_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1, 20)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature12</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.049896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.057032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052905</td>\n",
       "      <td>0.043763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.052114</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.024351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.048329</td>\n",
       "      <td>0.004298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.047246</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0  0.049896  0.000000       0.0  0.074577  0.000000  0.003559       0.0   \n",
       "1  0.057032  0.000000       0.0  0.000000  0.052905  0.043763       0.0   \n",
       "2  0.052114  0.000672       0.0  0.000000  0.015918  0.024351       0.0   \n",
       "4  0.048329  0.004298       0.0  0.000000  0.000000  0.014554       0.0   \n",
       "5  0.047246  0.004409       0.0  0.000000  0.000000  0.012579       0.0   \n",
       "\n",
       "   feature8  feature9  feature10  ...  feature12  feature13  feature14  \\\n",
       "0  0.012226       0.0   0.019288  ...        0.0        0.0        0.0   \n",
       "1  0.000000       0.0   0.031136  ...        0.0        0.0        0.0   \n",
       "2  0.001684       0.0   0.026478  ...        0.0        0.0        0.0   \n",
       "4  0.009139       0.0   0.024144  ...        0.0        0.0        0.0   \n",
       "5  0.010811       0.0   0.023702  ...        0.0        0.0        0.0   \n",
       "\n",
       "   feature15  feature16  feature17  feature18  feature19  feature20  label  \n",
       "0   0.000000        0.0   0.066148        0.0        0.0        0.0      0  \n",
       "1   0.006044        0.0   0.000000        0.0        0.0        0.0      0  \n",
       "2   0.000000        0.0   0.000000        0.0        0.0        0.0      0  \n",
       "4   0.000000        0.0   0.000000        0.0        0.0        0.0      0  \n",
       "5   0.000000        0.0   0.000000        0.0        0.0        0.0      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "timesteps =  df_train.shape[1] # equal to the lookback\n",
    "n_features =  df_train.shape[2] # 59\n",
    "\n",
    "epochs = 15\n",
    "batch = 64\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=dict()\n",
    "history['gen']=[]\n",
    "history['dis']=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error\n",
    "                             ,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 1\n",
    "        self.img_cols = 20\n",
    "        self.img_shape = (self.img_rows, self.img_cols)\n",
    "        self.latent_dim = 20\n",
    "\n",
    "        optimizer = Adam(0.0001, 0.4)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates song\n",
    "        z = Input(shape=(1,20))\n",
    "    \n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(1, 20)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #specifying output to have 40 timesteps\n",
    "        model.add(RepeatVector(20))\n",
    "        #specifying 1 feature as the output\n",
    "        \n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.3))   \n",
    "        model.add(TimeDistributed(Dense(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(1,20))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "        #return Model(img)\n",
    "    \n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(64, use_bias=False, input_shape=(20, 1)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Convolution1D(32, 3, strides=2, padding='same', use_bias=False))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Convolution1D(1, 3, strides=2, padding='same', use_bias=False))\n",
    "        model.add(Flatten())        \n",
    "        model.add(RepeatVector(1))        \n",
    "        model.add(TimeDistributed(Dense(32, activation = 'relu')))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(1, activation = 'relu')))\n",
    "        #model.add(TimeDistributed(Dense(1)))\n",
    "        #model.add(Dense(1, activation='tanh'))\n",
    "        #model.add(Dense(1))\n",
    "        #model.add(Activation('sigmoid'))\n",
    "        model.summary()\n",
    "        #model.add(layers.BatchNormalization())\n",
    "        #model.add(sigmoid())\n",
    "       \n",
    "        img = Input(shape=(20,1))\n",
    "        validity = model(img)\n",
    "        print(\"exit\")\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs=15, batch_size=64, save_interval=50):\n",
    "        print(\"enter\")  \n",
    "        # Load the dataset\n",
    "        X_train = df_train\n",
    "       \n",
    "        # Rescale 0 to 1\n",
    "      #  X_train = X_train / 128\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size,1,1))\n",
    "        fake = np.zeros((batch_size,1,1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of songs\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            imgs = np.array(imgs)\n",
    "            imgs = imgs.reshape(len(imgs),20,1)\n",
    "\n",
    "            # Sample noise and generate a batch of new songs\n",
    "            noise = np.random.normal(0, 1, (batch_size,1,20))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake songs as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "            history['gen'].append(g_loss)\n",
    "            \n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            history['dis'].append(d_loss) \n",
    "            #return (g_loss,d_loss)\n",
    "            # If at save interval => save model\n",
    "#            if epoch % save_interval == 0:\n",
    " #               self.generator.save(\"LSTM_generator.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 20, 64)            64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 20, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 10, 32)            6144      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5, 1)              96        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 1, 5)              0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 32)             192       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 1, 1)              33        \n",
      "=================================================================\n",
      "Total params: 6,529\n",
      "Trainable params: 6,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "exit\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 1, 256)            152576    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 1, 256)            394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 20, 256)           394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 20, 256)           394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 20, 256)           394240    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 20, 128)           32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 20, 128)           16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 20, 1)             129       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 20, 1)             0         \n",
      "=================================================================\n",
      "Total params: 2,173,313\n",
      "Trainable params: 2,173,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstmgan = LSTMGAN()\n",
    "#lstmgan.train(epochs=1000, batch_size=20, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 5.231367, acc.: 50.00%] [G loss: 7.462737]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 3.573940, acc.: 50.00%] [G loss: 6.876701]\n",
      "2 [D loss: 3.370727, acc.: 50.00%] [G loss: 6.577449]\n",
      "3 [D loss: 3.221380, acc.: 50.00%] [G loss: 6.326071]\n",
      "4 [D loss: 3.252480, acc.: 50.00%] [G loss: 6.231220]\n",
      "5 [D loss: 3.145452, acc.: 50.00%] [G loss: 6.237228]\n",
      "6 [D loss: 3.112618, acc.: 50.00%] [G loss: 6.005770]\n",
      "7 [D loss: 2.953639, acc.: 50.00%] [G loss: 5.972490]\n",
      "8 [D loss: 2.979051, acc.: 50.00%] [G loss: 5.809557]\n",
      "9 [D loss: 3.014195, acc.: 50.00%] [G loss: 5.892426]\n",
      "10 [D loss: 2.916846, acc.: 50.00%] [G loss: 5.730575]\n",
      "11 [D loss: 2.875999, acc.: 50.00%] [G loss: 5.672523]\n",
      "12 [D loss: 2.838346, acc.: 50.00%] [G loss: 5.509557]\n",
      "13 [D loss: 2.800802, acc.: 50.00%] [G loss: 5.463004]\n",
      "14 [D loss: 2.747982, acc.: 50.00%] [G loss: 5.466753]\n",
      "15 [D loss: 2.732333, acc.: 50.00%] [G loss: 5.431560]\n",
      "16 [D loss: 2.850679, acc.: 50.00%] [G loss: 5.277284]\n",
      "17 [D loss: 2.689746, acc.: 50.00%] [G loss: 5.193185]\n",
      "18 [D loss: 2.635931, acc.: 50.00%] [G loss: 5.183199]\n",
      "19 [D loss: 2.594566, acc.: 50.00%] [G loss: 5.163666]\n",
      "20 [D loss: 2.599521, acc.: 50.00%] [G loss: 5.150316]\n",
      "21 [D loss: 2.532203, acc.: 50.00%] [G loss: 5.327330]\n",
      "22 [D loss: 2.640000, acc.: 50.00%] [G loss: 5.137845]\n",
      "23 [D loss: 2.574692, acc.: 50.00%] [G loss: 5.168010]\n",
      "24 [D loss: 2.536816, acc.: 50.00%] [G loss: 5.044497]\n",
      "25 [D loss: 2.516441, acc.: 50.00%] [G loss: 4.981954]\n",
      "26 [D loss: 2.525928, acc.: 50.00%] [G loss: 5.015405]\n",
      "27 [D loss: 2.525970, acc.: 50.00%] [G loss: 5.047859]\n",
      "28 [D loss: 2.528811, acc.: 50.00%] [G loss: 4.910866]\n",
      "29 [D loss: 2.507473, acc.: 50.00%] [G loss: 4.897196]\n",
      "30 [D loss: 2.760810, acc.: 50.00%] [G loss: 4.942416]\n",
      "31 [D loss: 2.471612, acc.: 50.00%] [G loss: 4.857797]\n",
      "32 [D loss: 2.489181, acc.: 50.00%] [G loss: 4.886513]\n",
      "33 [D loss: 2.431989, acc.: 50.00%] [G loss: 4.913711]\n",
      "34 [D loss: 2.430011, acc.: 50.00%] [G loss: 4.746484]\n",
      "35 [D loss: 2.405240, acc.: 50.00%] [G loss: 4.880266]\n",
      "36 [D loss: 2.434702, acc.: 50.00%] [G loss: 4.886992]\n",
      "37 [D loss: 2.414974, acc.: 50.00%] [G loss: 4.771383]\n",
      "38 [D loss: 2.451073, acc.: 50.00%] [G loss: 4.747451]\n",
      "39 [D loss: 2.373500, acc.: 50.00%] [G loss: 4.727413]\n",
      "40 [D loss: 2.434120, acc.: 50.00%] [G loss: 4.777332]\n",
      "41 [D loss: 2.416392, acc.: 50.00%] [G loss: 4.717391]\n",
      "42 [D loss: 2.377485, acc.: 50.00%] [G loss: 4.686575]\n",
      "43 [D loss: 2.317641, acc.: 50.00%] [G loss: 4.629237]\n",
      "44 [D loss: 2.397700, acc.: 50.00%] [G loss: 4.738611]\n",
      "45 [D loss: 2.404238, acc.: 50.00%] [G loss: 4.731856]\n",
      "46 [D loss: 2.369080, acc.: 50.00%] [G loss: 4.737821]\n",
      "47 [D loss: 2.367533, acc.: 50.00%] [G loss: 4.685918]\n",
      "48 [D loss: 2.325548, acc.: 50.00%] [G loss: 4.728896]\n",
      "49 [D loss: 2.300513, acc.: 50.00%] [G loss: 4.649069]\n",
      "50 [D loss: 2.319936, acc.: 50.00%] [G loss: 4.649971]\n",
      "51 [D loss: 2.323009, acc.: 50.00%] [G loss: 4.485327]\n",
      "52 [D loss: 2.336514, acc.: 50.00%] [G loss: 4.493140]\n",
      "53 [D loss: 2.280739, acc.: 50.00%] [G loss: 4.550237]\n",
      "54 [D loss: 2.326867, acc.: 50.00%] [G loss: 4.447113]\n",
      "55 [D loss: 2.320665, acc.: 50.00%] [G loss: 4.600411]\n",
      "56 [D loss: 2.281310, acc.: 50.00%] [G loss: 4.575562]\n",
      "57 [D loss: 2.249223, acc.: 50.00%] [G loss: 4.551818]\n",
      "58 [D loss: 2.239381, acc.: 50.00%] [G loss: 4.601179]\n",
      "59 [D loss: 2.264342, acc.: 50.00%] [G loss: 4.535604]\n",
      "60 [D loss: 2.275172, acc.: 50.00%] [G loss: 4.435287]\n",
      "61 [D loss: 2.278151, acc.: 50.00%] [G loss: 4.480585]\n",
      "62 [D loss: 2.265914, acc.: 50.00%] [G loss: 4.433391]\n",
      "63 [D loss: 2.277056, acc.: 50.00%] [G loss: 4.318730]\n",
      "64 [D loss: 2.282037, acc.: 50.00%] [G loss: 4.447795]\n",
      "65 [D loss: 2.265390, acc.: 50.00%] [G loss: 4.486863]\n",
      "66 [D loss: 2.239853, acc.: 50.00%] [G loss: 4.408749]\n",
      "67 [D loss: 2.220579, acc.: 50.00%] [G loss: 4.328652]\n",
      "68 [D loss: 2.244908, acc.: 50.00%] [G loss: 4.411425]\n",
      "69 [D loss: 2.222939, acc.: 50.00%] [G loss: 4.363674]\n",
      "70 [D loss: 2.235916, acc.: 50.00%] [G loss: 4.323951]\n",
      "71 [D loss: 2.232338, acc.: 50.00%] [G loss: 4.401733]\n",
      "72 [D loss: 2.244750, acc.: 50.00%] [G loss: 4.409011]\n",
      "73 [D loss: 2.238266, acc.: 50.00%] [G loss: 4.332738]\n",
      "74 [D loss: 2.201722, acc.: 50.00%] [G loss: 4.371304]\n",
      "75 [D loss: 2.190738, acc.: 50.00%] [G loss: 4.380176]\n",
      "76 [D loss: 2.185503, acc.: 50.00%] [G loss: 4.317636]\n",
      "77 [D loss: 2.198080, acc.: 50.00%] [G loss: 4.289918]\n",
      "78 [D loss: 2.212740, acc.: 50.00%] [G loss: 4.234713]\n",
      "79 [D loss: 2.188676, acc.: 50.00%] [G loss: 4.213463]\n",
      "80 [D loss: 2.173484, acc.: 50.00%] [G loss: 4.259466]\n",
      "81 [D loss: 2.175724, acc.: 50.00%] [G loss: 4.271262]\n",
      "82 [D loss: 2.171657, acc.: 50.00%] [G loss: 4.219151]\n",
      "83 [D loss: 2.203293, acc.: 50.00%] [G loss: 4.311686]\n",
      "84 [D loss: 2.150843, acc.: 50.00%] [G loss: 4.196982]\n",
      "85 [D loss: 2.180546, acc.: 50.00%] [G loss: 4.177852]\n",
      "86 [D loss: 2.188834, acc.: 50.00%] [G loss: 4.146192]\n",
      "87 [D loss: 2.144284, acc.: 50.00%] [G loss: 4.149414]\n",
      "88 [D loss: 2.136879, acc.: 50.00%] [G loss: 4.179811]\n",
      "89 [D loss: 2.087169, acc.: 50.00%] [G loss: 4.187771]\n",
      "90 [D loss: 2.158884, acc.: 50.00%] [G loss: 4.309319]\n",
      "91 [D loss: 2.143358, acc.: 50.00%] [G loss: 4.129432]\n",
      "92 [D loss: 2.119539, acc.: 50.00%] [G loss: 4.272787]\n",
      "93 [D loss: 2.174693, acc.: 50.00%] [G loss: 4.182819]\n",
      "94 [D loss: 2.070802, acc.: 50.00%] [G loss: 4.116359]\n",
      "95 [D loss: 2.103130, acc.: 50.00%] [G loss: 4.082978]\n",
      "96 [D loss: 2.158679, acc.: 50.00%] [G loss: 4.137688]\n",
      "97 [D loss: 2.104326, acc.: 50.00%] [G loss: 4.253920]\n",
      "98 [D loss: 2.102968, acc.: 50.00%] [G loss: 4.059055]\n",
      "99 [D loss: 2.080940, acc.: 50.00%] [G loss: 4.112139]\n",
      "100 [D loss: 2.057420, acc.: 50.00%] [G loss: 4.065455]\n",
      "101 [D loss: 2.062988, acc.: 50.00%] [G loss: 4.125874]\n",
      "102 [D loss: 2.065086, acc.: 50.00%] [G loss: 4.122725]\n",
      "103 [D loss: 2.109789, acc.: 50.00%] [G loss: 4.198810]\n",
      "104 [D loss: 2.118449, acc.: 50.00%] [G loss: 4.119073]\n",
      "105 [D loss: 2.042563, acc.: 50.00%] [G loss: 4.000835]\n",
      "106 [D loss: 2.038755, acc.: 50.00%] [G loss: 4.070200]\n",
      "107 [D loss: 2.025204, acc.: 50.00%] [G loss: 4.109336]\n",
      "108 [D loss: 2.023564, acc.: 50.00%] [G loss: 4.191393]\n",
      "109 [D loss: 2.116966, acc.: 50.00%] [G loss: 4.012183]\n",
      "110 [D loss: 2.020806, acc.: 50.00%] [G loss: 4.049341]\n",
      "111 [D loss: 2.068130, acc.: 50.00%] [G loss: 3.977345]\n",
      "112 [D loss: 2.032629, acc.: 50.00%] [G loss: 4.110146]\n",
      "113 [D loss: 2.000378, acc.: 50.00%] [G loss: 4.032495]\n",
      "114 [D loss: 1.998231, acc.: 50.00%] [G loss: 4.049183]\n",
      "115 [D loss: 2.087002, acc.: 50.00%] [G loss: 4.045180]\n",
      "116 [D loss: 2.010609, acc.: 50.00%] [G loss: 4.065241]\n",
      "117 [D loss: 1.986715, acc.: 50.00%] [G loss: 4.089878]\n",
      "118 [D loss: 1.956762, acc.: 50.00%] [G loss: 3.943140]\n",
      "119 [D loss: 2.026705, acc.: 50.00%] [G loss: 4.005418]\n",
      "120 [D loss: 2.034168, acc.: 50.00%] [G loss: 4.054025]\n",
      "121 [D loss: 2.013559, acc.: 50.00%] [G loss: 3.985131]\n",
      "122 [D loss: 1.999920, acc.: 50.00%] [G loss: 4.142294]\n",
      "123 [D loss: 2.035675, acc.: 50.00%] [G loss: 3.977676]\n",
      "124 [D loss: 1.987021, acc.: 50.00%] [G loss: 3.903937]\n",
      "125 [D loss: 1.974333, acc.: 50.00%] [G loss: 3.889126]\n",
      "126 [D loss: 2.067598, acc.: 50.00%] [G loss: 3.924112]\n",
      "127 [D loss: 2.009589, acc.: 50.00%] [G loss: 3.956118]\n",
      "128 [D loss: 1.991979, acc.: 50.00%] [G loss: 3.902017]\n",
      "129 [D loss: 1.999838, acc.: 50.00%] [G loss: 3.865049]\n",
      "130 [D loss: 1.968012, acc.: 50.00%] [G loss: 3.957572]\n",
      "131 [D loss: 1.948499, acc.: 50.00%] [G loss: 3.856029]\n",
      "132 [D loss: 2.017040, acc.: 50.00%] [G loss: 3.873133]\n",
      "133 [D loss: 1.954370, acc.: 50.00%] [G loss: 3.854860]\n",
      "134 [D loss: 1.944218, acc.: 50.00%] [G loss: 3.780054]\n",
      "135 [D loss: 1.980117, acc.: 50.00%] [G loss: 3.940351]\n",
      "136 [D loss: 1.995472, acc.: 50.00%] [G loss: 3.885945]\n",
      "137 [D loss: 1.951121, acc.: 50.00%] [G loss: 3.878268]\n",
      "138 [D loss: 1.991040, acc.: 50.00%] [G loss: 3.892667]\n",
      "139 [D loss: 2.010241, acc.: 50.00%] [G loss: 4.031184]\n",
      "140 [D loss: 1.917428, acc.: 50.00%] [G loss: 3.842407]\n",
      "141 [D loss: 1.983146, acc.: 50.00%] [G loss: 3.934277]\n",
      "142 [D loss: 1.938276, acc.: 50.00%] [G loss: 3.857362]\n",
      "143 [D loss: 1.974207, acc.: 50.00%] [G loss: 3.791604]\n",
      "144 [D loss: 2.012425, acc.: 50.00%] [G loss: 3.800735]\n",
      "145 [D loss: 1.972099, acc.: 50.00%] [G loss: 3.894864]\n",
      "146 [D loss: 1.969562, acc.: 50.00%] [G loss: 3.891429]\n",
      "147 [D loss: 1.957344, acc.: 50.00%] [G loss: 3.816885]\n",
      "148 [D loss: 1.991826, acc.: 50.00%] [G loss: 3.875830]\n",
      "149 [D loss: 1.916691, acc.: 50.00%] [G loss: 3.834769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 [D loss: 1.898584, acc.: 50.00%] [G loss: 3.821993]\n",
      "151 [D loss: 1.959240, acc.: 50.00%] [G loss: 3.861207]\n",
      "152 [D loss: 1.886092, acc.: 50.00%] [G loss: 3.786225]\n",
      "153 [D loss: 1.924123, acc.: 50.00%] [G loss: 3.797371]\n",
      "154 [D loss: 1.912568, acc.: 50.00%] [G loss: 3.841446]\n",
      "155 [D loss: 1.927430, acc.: 50.00%] [G loss: 3.818192]\n",
      "156 [D loss: 1.912413, acc.: 50.00%] [G loss: 3.868247]\n",
      "157 [D loss: 1.936023, acc.: 50.00%] [G loss: 3.769955]\n",
      "158 [D loss: 1.866775, acc.: 50.00%] [G loss: 3.829754]\n",
      "159 [D loss: 1.868586, acc.: 50.00%] [G loss: 3.705132]\n",
      "160 [D loss: 1.888790, acc.: 50.00%] [G loss: 3.711401]\n",
      "161 [D loss: 1.896255, acc.: 50.00%] [G loss: 3.759780]\n",
      "162 [D loss: 1.909138, acc.: 50.00%] [G loss: 3.772969]\n",
      "163 [D loss: 1.918604, acc.: 50.00%] [G loss: 3.850523]\n",
      "164 [D loss: 1.910239, acc.: 50.00%] [G loss: 3.679498]\n",
      "165 [D loss: 1.977096, acc.: 50.00%] [G loss: 3.648149]\n",
      "166 [D loss: 1.880525, acc.: 50.00%] [G loss: 3.737952]\n",
      "167 [D loss: 1.871827, acc.: 50.00%] [G loss: 3.737807]\n",
      "168 [D loss: 1.900417, acc.: 50.00%] [G loss: 3.747464]\n",
      "169 [D loss: 1.927935, acc.: 50.00%] [G loss: 3.721588]\n",
      "170 [D loss: 1.906472, acc.: 50.00%] [G loss: 3.656556]\n",
      "171 [D loss: 1.833265, acc.: 50.00%] [G loss: 3.686806]\n",
      "172 [D loss: 1.895783, acc.: 50.00%] [G loss: 3.689564]\n",
      "173 [D loss: 1.900162, acc.: 50.00%] [G loss: 3.708682]\n",
      "174 [D loss: 1.888197, acc.: 50.00%] [G loss: 3.676374]\n",
      "175 [D loss: 1.857586, acc.: 50.00%] [G loss: 3.667197]\n",
      "176 [D loss: 1.906159, acc.: 50.00%] [G loss: 3.722693]\n",
      "177 [D loss: 1.842564, acc.: 50.00%] [G loss: 3.673368]\n",
      "178 [D loss: 1.898124, acc.: 50.00%] [G loss: 3.640998]\n",
      "179 [D loss: 1.797235, acc.: 50.00%] [G loss: 3.632584]\n",
      "180 [D loss: 1.874341, acc.: 50.00%] [G loss: 3.654280]\n",
      "181 [D loss: 1.896112, acc.: 50.00%] [G loss: 3.654435]\n",
      "182 [D loss: 1.877379, acc.: 50.00%] [G loss: 3.669593]\n",
      "183 [D loss: 1.865090, acc.: 50.00%] [G loss: 3.670398]\n",
      "184 [D loss: 1.779542, acc.: 50.00%] [G loss: 3.720275]\n",
      "185 [D loss: 1.848747, acc.: 50.00%] [G loss: 3.751373]\n",
      "186 [D loss: 1.861611, acc.: 50.00%] [G loss: 3.661744]\n",
      "187 [D loss: 1.857174, acc.: 50.00%] [G loss: 3.665016]\n",
      "188 [D loss: 1.830341, acc.: 50.00%] [G loss: 3.685696]\n",
      "189 [D loss: 1.860767, acc.: 50.00%] [G loss: 3.703968]\n",
      "190 [D loss: 1.871428, acc.: 50.00%] [G loss: 3.653663]\n",
      "191 [D loss: 1.779710, acc.: 50.00%] [G loss: 3.599551]\n",
      "192 [D loss: 1.845014, acc.: 50.00%] [G loss: 3.588184]\n",
      "193 [D loss: 1.821257, acc.: 50.00%] [G loss: 3.609878]\n",
      "194 [D loss: 1.863672, acc.: 50.00%] [G loss: 3.659111]\n",
      "195 [D loss: 1.804392, acc.: 50.00%] [G loss: 3.577142]\n",
      "196 [D loss: 1.818485, acc.: 50.00%] [G loss: 3.638706]\n",
      "197 [D loss: 1.807616, acc.: 50.00%] [G loss: 3.573419]\n",
      "198 [D loss: 1.871627, acc.: 50.00%] [G loss: 3.606181]\n",
      "199 [D loss: 1.818539, acc.: 50.00%] [G loss: 3.617026]\n",
      "200 [D loss: 1.808480, acc.: 50.00%] [G loss: 3.561275]\n",
      "201 [D loss: 1.814503, acc.: 50.00%] [G loss: 3.527142]\n",
      "202 [D loss: 1.841559, acc.: 50.00%] [G loss: 3.483552]\n",
      "203 [D loss: 1.815506, acc.: 50.00%] [G loss: 3.556422]\n",
      "204 [D loss: 1.782414, acc.: 50.00%] [G loss: 3.534526]\n",
      "205 [D loss: 1.814572, acc.: 50.00%] [G loss: 3.670841]\n",
      "206 [D loss: 1.775124, acc.: 50.00%] [G loss: 3.702869]\n",
      "207 [D loss: 1.837128, acc.: 50.00%] [G loss: 3.696152]\n",
      "208 [D loss: 1.857147, acc.: 50.00%] [G loss: 3.711026]\n",
      "209 [D loss: 1.778408, acc.: 50.00%] [G loss: 3.630161]\n",
      "210 [D loss: 1.817406, acc.: 50.00%] [G loss: 3.485297]\n",
      "211 [D loss: 1.811082, acc.: 50.00%] [G loss: 3.524326]\n",
      "212 [D loss: 1.778616, acc.: 50.00%] [G loss: 3.622416]\n",
      "213 [D loss: 1.820131, acc.: 50.00%] [G loss: 3.543514]\n",
      "214 [D loss: 1.805912, acc.: 50.00%] [G loss: 3.569405]\n",
      "215 [D loss: 1.837471, acc.: 50.00%] [G loss: 3.515431]\n",
      "216 [D loss: 1.764068, acc.: 50.00%] [G loss: 3.534929]\n",
      "217 [D loss: 1.786978, acc.: 50.00%] [G loss: 3.555975]\n",
      "218 [D loss: 1.788942, acc.: 50.00%] [G loss: 3.514043]\n",
      "219 [D loss: 1.850637, acc.: 50.00%] [G loss: 3.578855]\n",
      "220 [D loss: 1.760964, acc.: 50.00%] [G loss: 3.422137]\n",
      "221 [D loss: 1.826643, acc.: 50.00%] [G loss: 3.571244]\n",
      "222 [D loss: 1.777693, acc.: 50.00%] [G loss: 3.523212]\n",
      "223 [D loss: 1.787639, acc.: 50.00%] [G loss: 3.693311]\n",
      "224 [D loss: 1.736735, acc.: 50.00%] [G loss: 3.513148]\n",
      "225 [D loss: 1.809650, acc.: 50.00%] [G loss: 3.521739]\n",
      "226 [D loss: 1.810583, acc.: 50.00%] [G loss: 3.392802]\n",
      "227 [D loss: 1.742964, acc.: 50.00%] [G loss: 3.439031]\n",
      "228 [D loss: 1.755684, acc.: 50.00%] [G loss: 3.503145]\n",
      "229 [D loss: 1.762960, acc.: 50.00%] [G loss: 3.559478]\n",
      "230 [D loss: 1.777154, acc.: 50.00%] [G loss: 3.605783]\n",
      "231 [D loss: 1.785516, acc.: 50.00%] [G loss: 3.408363]\n",
      "232 [D loss: 1.776892, acc.: 50.00%] [G loss: 3.441075]\n",
      "233 [D loss: 1.745425, acc.: 50.00%] [G loss: 3.517033]\n",
      "234 [D loss: 1.754234, acc.: 50.00%] [G loss: 3.370187]\n",
      "235 [D loss: 1.753214, acc.: 50.00%] [G loss: 3.453513]\n",
      "236 [D loss: 1.762875, acc.: 50.00%] [G loss: 3.423052]\n",
      "237 [D loss: 1.730708, acc.: 50.00%] [G loss: 3.351803]\n",
      "238 [D loss: 1.703363, acc.: 50.00%] [G loss: 3.422193]\n",
      "239 [D loss: 1.742866, acc.: 50.00%] [G loss: 3.606055]\n",
      "240 [D loss: 1.757647, acc.: 50.00%] [G loss: 3.410213]\n",
      "241 [D loss: 1.757935, acc.: 50.00%] [G loss: 3.448249]\n",
      "242 [D loss: 1.711228, acc.: 50.00%] [G loss: 3.456481]\n",
      "243 [D loss: 1.754852, acc.: 50.00%] [G loss: 3.445365]\n",
      "244 [D loss: 1.755529, acc.: 50.00%] [G loss: 3.439624]\n",
      "245 [D loss: 1.762663, acc.: 50.00%] [G loss: 3.357724]\n",
      "246 [D loss: 1.756216, acc.: 50.00%] [G loss: 3.484102]\n",
      "247 [D loss: 1.693232, acc.: 50.00%] [G loss: 3.415842]\n",
      "248 [D loss: 1.689027, acc.: 50.00%] [G loss: 3.429224]\n",
      "249 [D loss: 1.819817, acc.: 50.00%] [G loss: 3.443132]\n",
      "250 [D loss: 1.701835, acc.: 50.00%] [G loss: 3.462781]\n",
      "251 [D loss: 1.732695, acc.: 50.00%] [G loss: 3.372928]\n",
      "252 [D loss: 1.753340, acc.: 50.00%] [G loss: 3.388757]\n",
      "253 [D loss: 1.781345, acc.: 50.00%] [G loss: 3.364236]\n",
      "254 [D loss: 1.738435, acc.: 50.00%] [G loss: 3.338814]\n",
      "255 [D loss: 1.721141, acc.: 50.00%] [G loss: 3.309509]\n",
      "256 [D loss: 1.718527, acc.: 50.00%] [G loss: 3.430216]\n",
      "257 [D loss: 1.715158, acc.: 50.00%] [G loss: 3.420644]\n",
      "258 [D loss: 1.726073, acc.: 50.00%] [G loss: 3.468845]\n",
      "259 [D loss: 1.731471, acc.: 50.00%] [G loss: 3.428682]\n",
      "260 [D loss: 1.755424, acc.: 50.00%] [G loss: 3.498898]\n",
      "261 [D loss: 1.659789, acc.: 50.00%] [G loss: 3.312656]\n",
      "262 [D loss: 1.665972, acc.: 50.00%] [G loss: 3.407858]\n",
      "263 [D loss: 1.696765, acc.: 50.00%] [G loss: 3.311848]\n",
      "264 [D loss: 1.742486, acc.: 50.00%] [G loss: 3.387620]\n",
      "265 [D loss: 1.700686, acc.: 50.00%] [G loss: 3.520494]\n",
      "266 [D loss: 1.730509, acc.: 50.00%] [G loss: 3.274719]\n",
      "267 [D loss: 1.654513, acc.: 50.00%] [G loss: 3.356605]\n",
      "268 [D loss: 1.744381, acc.: 50.00%] [G loss: 3.424475]\n",
      "269 [D loss: 1.708628, acc.: 50.00%] [G loss: 3.394142]\n",
      "270 [D loss: 1.720133, acc.: 50.00%] [G loss: 3.347049]\n",
      "271 [D loss: 1.680832, acc.: 50.00%] [G loss: 3.313218]\n",
      "272 [D loss: 1.662049, acc.: 50.00%] [G loss: 3.318874]\n",
      "273 [D loss: 1.710955, acc.: 50.00%] [G loss: 3.354006]\n",
      "274 [D loss: 1.716124, acc.: 50.00%] [G loss: 3.355471]\n",
      "275 [D loss: 1.655784, acc.: 50.00%] [G loss: 3.245219]\n",
      "276 [D loss: 1.687220, acc.: 50.00%] [G loss: 3.367555]\n",
      "277 [D loss: 1.689354, acc.: 50.00%] [G loss: 3.276669]\n",
      "278 [D loss: 1.691591, acc.: 50.00%] [G loss: 3.300580]\n",
      "279 [D loss: 1.699333, acc.: 50.00%] [G loss: 3.252402]\n",
      "280 [D loss: 1.718841, acc.: 50.00%] [G loss: 3.303523]\n",
      "281 [D loss: 1.681267, acc.: 50.00%] [G loss: 3.261718]\n",
      "282 [D loss: 1.683473, acc.: 50.00%] [G loss: 3.351189]\n",
      "283 [D loss: 1.670361, acc.: 50.00%] [G loss: 3.310770]\n",
      "284 [D loss: 1.692264, acc.: 50.00%] [G loss: 3.340256]\n",
      "285 [D loss: 1.663038, acc.: 50.00%] [G loss: 3.309367]\n",
      "286 [D loss: 1.718056, acc.: 50.00%] [G loss: 3.307208]\n",
      "287 [D loss: 1.694290, acc.: 50.00%] [G loss: 3.205535]\n",
      "288 [D loss: 1.701680, acc.: 50.00%] [G loss: 3.255621]\n",
      "289 [D loss: 1.693234, acc.: 50.00%] [G loss: 3.288852]\n",
      "290 [D loss: 1.706836, acc.: 50.00%] [G loss: 3.264763]\n",
      "291 [D loss: 1.684196, acc.: 50.00%] [G loss: 3.246478]\n",
      "292 [D loss: 1.685669, acc.: 50.00%] [G loss: 3.172591]\n",
      "293 [D loss: 1.663837, acc.: 50.00%] [G loss: 3.252359]\n",
      "294 [D loss: 1.696021, acc.: 50.00%] [G loss: 3.229819]\n",
      "295 [D loss: 1.668641, acc.: 50.00%] [G loss: 3.231267]\n",
      "296 [D loss: 1.654943, acc.: 50.00%] [G loss: 3.218946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 [D loss: 1.646559, acc.: 50.00%] [G loss: 3.313105]\n",
      "298 [D loss: 1.647442, acc.: 50.00%] [G loss: 3.223816]\n",
      "299 [D loss: 1.703368, acc.: 50.00%] [G loss: 3.206417]\n",
      "300 [D loss: 1.647780, acc.: 50.00%] [G loss: 3.284096]\n",
      "301 [D loss: 1.616204, acc.: 50.00%] [G loss: 3.226170]\n",
      "302 [D loss: 1.699952, acc.: 50.00%] [G loss: 3.223728]\n",
      "303 [D loss: 1.655357, acc.: 50.00%] [G loss: 3.334964]\n",
      "304 [D loss: 1.664255, acc.: 50.00%] [G loss: 3.189387]\n",
      "305 [D loss: 1.657222, acc.: 50.00%] [G loss: 3.260194]\n",
      "306 [D loss: 1.657601, acc.: 50.00%] [G loss: 3.227630]\n",
      "307 [D loss: 1.635179, acc.: 50.00%] [G loss: 3.202898]\n",
      "308 [D loss: 1.659777, acc.: 50.00%] [G loss: 3.247351]\n",
      "309 [D loss: 1.656566, acc.: 50.00%] [G loss: 3.172390]\n",
      "310 [D loss: 1.660069, acc.: 50.00%] [G loss: 3.229448]\n",
      "311 [D loss: 1.654449, acc.: 50.00%] [G loss: 3.255919]\n",
      "312 [D loss: 1.596766, acc.: 50.00%] [G loss: 3.301817]\n",
      "313 [D loss: 1.640330, acc.: 50.00%] [G loss: 3.263239]\n",
      "314 [D loss: 1.628533, acc.: 50.00%] [G loss: 3.300278]\n",
      "315 [D loss: 1.622186, acc.: 50.00%] [G loss: 3.183211]\n",
      "316 [D loss: 1.665124, acc.: 50.00%] [G loss: 3.220538]\n",
      "317 [D loss: 1.670083, acc.: 50.00%] [G loss: 3.225295]\n",
      "318 [D loss: 1.597444, acc.: 50.00%] [G loss: 3.178450]\n",
      "319 [D loss: 1.650209, acc.: 50.00%] [G loss: 3.218721]\n",
      "320 [D loss: 1.624332, acc.: 50.00%] [G loss: 3.273879]\n",
      "321 [D loss: 1.615615, acc.: 50.00%] [G loss: 3.226970]\n",
      "322 [D loss: 1.626225, acc.: 50.00%] [G loss: 3.299323]\n",
      "323 [D loss: 1.600525, acc.: 50.00%] [G loss: 3.235928]\n",
      "324 [D loss: 1.558534, acc.: 50.00%] [G loss: 3.224228]\n",
      "325 [D loss: 1.645234, acc.: 50.00%] [G loss: 3.196157]\n",
      "326 [D loss: 1.609477, acc.: 50.00%] [G loss: 3.181725]\n",
      "327 [D loss: 1.588297, acc.: 50.00%] [G loss: 3.281867]\n",
      "328 [D loss: 1.613835, acc.: 50.00%] [G loss: 3.229700]\n",
      "329 [D loss: 1.630713, acc.: 50.00%] [G loss: 3.196463]\n",
      "330 [D loss: 1.592030, acc.: 50.00%] [G loss: 3.133137]\n",
      "331 [D loss: 1.587818, acc.: 50.00%] [G loss: 3.220531]\n",
      "332 [D loss: 1.570668, acc.: 50.00%] [G loss: 3.151025]\n",
      "333 [D loss: 1.612368, acc.: 50.00%] [G loss: 3.240009]\n",
      "334 [D loss: 1.583963, acc.: 50.00%] [G loss: 3.146839]\n",
      "335 [D loss: 1.611224, acc.: 50.00%] [G loss: 3.238807]\n",
      "336 [D loss: 1.626948, acc.: 50.00%] [G loss: 3.105274]\n",
      "337 [D loss: 1.568246, acc.: 50.00%] [G loss: 3.202882]\n",
      "338 [D loss: 1.590276, acc.: 50.00%] [G loss: 3.196689]\n",
      "339 [D loss: 1.654018, acc.: 50.00%] [G loss: 3.113487]\n",
      "340 [D loss: 1.571799, acc.: 50.00%] [G loss: 3.116457]\n",
      "341 [D loss: 1.615626, acc.: 50.00%] [G loss: 3.246483]\n",
      "342 [D loss: 1.614521, acc.: 50.00%] [G loss: 3.167679]\n",
      "343 [D loss: 1.549489, acc.: 50.00%] [G loss: 3.168489]\n",
      "344 [D loss: 1.599250, acc.: 50.00%] [G loss: 3.067421]\n",
      "345 [D loss: 1.578050, acc.: 50.00%] [G loss: 3.193069]\n",
      "346 [D loss: 1.573500, acc.: 50.00%] [G loss: 3.143646]\n",
      "347 [D loss: 1.605467, acc.: 50.00%] [G loss: 3.159953]\n",
      "348 [D loss: 1.643639, acc.: 50.00%] [G loss: 3.154907]\n",
      "349 [D loss: 1.599799, acc.: 50.00%] [G loss: 3.153062]\n",
      "350 [D loss: 1.585037, acc.: 50.00%] [G loss: 3.185402]\n",
      "351 [D loss: 1.583345, acc.: 50.00%] [G loss: 3.112966]\n",
      "352 [D loss: 1.601222, acc.: 50.00%] [G loss: 3.221656]\n",
      "353 [D loss: 1.544664, acc.: 50.00%] [G loss: 3.069249]\n",
      "354 [D loss: 1.584550, acc.: 50.00%] [G loss: 3.035606]\n",
      "355 [D loss: 1.591584, acc.: 50.00%] [G loss: 3.079674]\n",
      "356 [D loss: 1.598563, acc.: 50.00%] [G loss: 3.103497]\n",
      "357 [D loss: 1.603368, acc.: 50.00%] [G loss: 3.002936]\n",
      "358 [D loss: 1.616954, acc.: 50.00%] [G loss: 3.061188]\n",
      "359 [D loss: 1.631283, acc.: 50.00%] [G loss: 3.026268]\n",
      "360 [D loss: 1.555488, acc.: 50.00%] [G loss: 3.064071]\n",
      "361 [D loss: 1.608240, acc.: 50.00%] [G loss: 3.012663]\n",
      "362 [D loss: 1.615638, acc.: 50.00%] [G loss: 3.151889]\n",
      "363 [D loss: 1.600518, acc.: 50.00%] [G loss: 3.043710]\n",
      "364 [D loss: 1.562993, acc.: 50.00%] [G loss: 3.050838]\n",
      "365 [D loss: 1.583015, acc.: 50.00%] [G loss: 3.130682]\n",
      "366 [D loss: 1.548030, acc.: 50.00%] [G loss: 3.018653]\n",
      "367 [D loss: 1.607838, acc.: 50.00%] [G loss: 3.025863]\n",
      "368 [D loss: 1.605020, acc.: 50.00%] [G loss: 3.150535]\n",
      "369 [D loss: 1.584655, acc.: 50.00%] [G loss: 3.020072]\n",
      "370 [D loss: 1.536520, acc.: 50.00%] [G loss: 3.162158]\n",
      "371 [D loss: 1.544937, acc.: 50.00%] [G loss: 3.094123]\n",
      "372 [D loss: 1.577346, acc.: 50.00%] [G loss: 3.101363]\n",
      "373 [D loss: 1.523455, acc.: 50.00%] [G loss: 3.038190]\n",
      "374 [D loss: 1.560687, acc.: 50.00%] [G loss: 3.198624]\n",
      "375 [D loss: 1.531944, acc.: 50.00%] [G loss: 2.973483]\n",
      "376 [D loss: 1.584589, acc.: 50.00%] [G loss: 3.160311]\n",
      "377 [D loss: 1.539599, acc.: 50.00%] [G loss: 3.162227]\n",
      "378 [D loss: 1.605296, acc.: 50.00%] [G loss: 3.058188]\n",
      "379 [D loss: 1.537053, acc.: 50.00%] [G loss: 3.107869]\n",
      "380 [D loss: 1.529082, acc.: 50.00%] [G loss: 3.041120]\n",
      "381 [D loss: 1.559351, acc.: 50.00%] [G loss: 3.063742]\n",
      "382 [D loss: 1.579420, acc.: 50.00%] [G loss: 3.004778]\n",
      "383 [D loss: 1.580721, acc.: 50.00%] [G loss: 3.048940]\n",
      "384 [D loss: 1.545306, acc.: 50.00%] [G loss: 3.072050]\n",
      "385 [D loss: 1.507990, acc.: 50.00%] [G loss: 2.941141]\n",
      "386 [D loss: 1.614981, acc.: 50.00%] [G loss: 3.025958]\n",
      "387 [D loss: 1.554059, acc.: 50.00%] [G loss: 3.190508]\n",
      "388 [D loss: 1.549321, acc.: 50.00%] [G loss: 2.987149]\n",
      "389 [D loss: 1.576500, acc.: 50.00%] [G loss: 3.031995]\n",
      "390 [D loss: 1.516083, acc.: 50.00%] [G loss: 3.034983]\n",
      "391 [D loss: 1.548388, acc.: 50.00%] [G loss: 2.975740]\n",
      "392 [D loss: 1.557118, acc.: 50.00%] [G loss: 3.023870]\n",
      "393 [D loss: 1.548060, acc.: 50.00%] [G loss: 3.042268]\n",
      "394 [D loss: 1.499425, acc.: 50.00%] [G loss: 3.046457]\n",
      "395 [D loss: 1.526776, acc.: 50.00%] [G loss: 3.027230]\n",
      "396 [D loss: 1.523067, acc.: 50.00%] [G loss: 3.024144]\n",
      "397 [D loss: 1.498463, acc.: 50.00%] [G loss: 2.980757]\n",
      "398 [D loss: 1.510737, acc.: 50.00%] [G loss: 3.019667]\n",
      "399 [D loss: 1.584099, acc.: 50.00%] [G loss: 2.957375]\n",
      "400 [D loss: 1.512672, acc.: 50.00%] [G loss: 3.093168]\n",
      "401 [D loss: 1.527388, acc.: 50.00%] [G loss: 2.957466]\n",
      "402 [D loss: 1.505938, acc.: 50.00%] [G loss: 3.023623]\n",
      "403 [D loss: 1.568107, acc.: 50.00%] [G loss: 2.874068]\n",
      "404 [D loss: 1.505185, acc.: 50.00%] [G loss: 3.008583]\n",
      "405 [D loss: 1.539003, acc.: 50.00%] [G loss: 2.941833]\n",
      "406 [D loss: 1.581030, acc.: 50.00%] [G loss: 3.048157]\n",
      "407 [D loss: 1.535800, acc.: 50.00%] [G loss: 2.922756]\n",
      "408 [D loss: 1.536510, acc.: 50.00%] [G loss: 3.036180]\n",
      "409 [D loss: 1.538420, acc.: 50.00%] [G loss: 2.960064]\n",
      "410 [D loss: 1.549426, acc.: 50.00%] [G loss: 3.072214]\n",
      "411 [D loss: 1.496581, acc.: 50.00%] [G loss: 2.865224]\n",
      "412 [D loss: 1.477946, acc.: 50.00%] [G loss: 2.914872]\n",
      "413 [D loss: 1.527746, acc.: 50.00%] [G loss: 2.862056]\n",
      "414 [D loss: 1.534394, acc.: 50.00%] [G loss: 2.991876]\n",
      "415 [D loss: 1.490565, acc.: 50.00%] [G loss: 3.000473]\n",
      "416 [D loss: 1.494260, acc.: 50.00%] [G loss: 2.910592]\n",
      "417 [D loss: 1.485539, acc.: 50.00%] [G loss: 2.905381]\n",
      "418 [D loss: 1.549152, acc.: 50.00%] [G loss: 2.809804]\n",
      "419 [D loss: 1.503016, acc.: 50.00%] [G loss: 3.001611]\n",
      "420 [D loss: 1.540822, acc.: 50.00%] [G loss: 2.956922]\n",
      "421 [D loss: 1.560185, acc.: 50.00%] [G loss: 2.972182]\n",
      "422 [D loss: 1.513442, acc.: 50.00%] [G loss: 2.955007]\n",
      "423 [D loss: 1.519438, acc.: 50.00%] [G loss: 3.105771]\n",
      "424 [D loss: 1.519091, acc.: 50.00%] [G loss: 3.076452]\n",
      "425 [D loss: 1.514086, acc.: 50.00%] [G loss: 2.958038]\n",
      "426 [D loss: 1.498428, acc.: 50.00%] [G loss: 2.986316]\n",
      "427 [D loss: 1.529737, acc.: 50.00%] [G loss: 2.881883]\n",
      "428 [D loss: 1.480655, acc.: 50.00%] [G loss: 3.008126]\n",
      "429 [D loss: 1.514518, acc.: 50.00%] [G loss: 2.921693]\n",
      "430 [D loss: 1.508014, acc.: 50.00%] [G loss: 3.011221]\n",
      "431 [D loss: 1.576652, acc.: 50.00%] [G loss: 2.946162]\n",
      "432 [D loss: 1.481529, acc.: 50.00%] [G loss: 3.096806]\n",
      "433 [D loss: 1.501853, acc.: 50.00%] [G loss: 2.905404]\n",
      "434 [D loss: 1.559747, acc.: 50.00%] [G loss: 2.875774]\n",
      "435 [D loss: 1.537993, acc.: 50.00%] [G loss: 3.043206]\n",
      "436 [D loss: 1.474755, acc.: 50.00%] [G loss: 2.878285]\n",
      "437 [D loss: 1.504525, acc.: 50.00%] [G loss: 2.951458]\n",
      "438 [D loss: 1.510688, acc.: 50.00%] [G loss: 3.002217]\n",
      "439 [D loss: 1.498514, acc.: 50.00%] [G loss: 2.972979]\n",
      "440 [D loss: 1.473921, acc.: 50.00%] [G loss: 3.021006]\n",
      "441 [D loss: 1.509535, acc.: 50.00%] [G loss: 2.913945]\n",
      "442 [D loss: 1.530836, acc.: 50.00%] [G loss: 2.959610]\n",
      "443 [D loss: 1.493323, acc.: 50.00%] [G loss: 2.905195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444 [D loss: 1.479673, acc.: 50.00%] [G loss: 2.886795]\n",
      "445 [D loss: 1.443132, acc.: 50.00%] [G loss: 2.865104]\n",
      "446 [D loss: 1.513861, acc.: 50.00%] [G loss: 2.899246]\n",
      "447 [D loss: 1.473051, acc.: 50.00%] [G loss: 2.861706]\n",
      "448 [D loss: 1.456562, acc.: 50.00%] [G loss: 2.888266]\n",
      "449 [D loss: 1.509468, acc.: 50.00%] [G loss: 2.975444]\n",
      "450 [D loss: 1.492550, acc.: 50.00%] [G loss: 2.969745]\n",
      "451 [D loss: 1.465165, acc.: 50.00%] [G loss: 2.846294]\n",
      "452 [D loss: 1.516144, acc.: 50.00%] [G loss: 2.993287]\n",
      "453 [D loss: 1.408775, acc.: 50.00%] [G loss: 2.795974]\n",
      "454 [D loss: 1.501404, acc.: 50.00%] [G loss: 2.977551]\n",
      "455 [D loss: 1.436892, acc.: 50.00%] [G loss: 2.854881]\n",
      "456 [D loss: 1.481305, acc.: 50.00%] [G loss: 2.904063]\n",
      "457 [D loss: 1.563010, acc.: 50.00%] [G loss: 2.890164]\n",
      "458 [D loss: 1.459000, acc.: 50.00%] [G loss: 2.900573]\n",
      "459 [D loss: 1.479258, acc.: 50.00%] [G loss: 2.837099]\n",
      "460 [D loss: 1.430716, acc.: 50.00%] [G loss: 2.786281]\n",
      "461 [D loss: 1.480453, acc.: 50.00%] [G loss: 2.856998]\n",
      "462 [D loss: 1.477294, acc.: 50.00%] [G loss: 2.826200]\n",
      "463 [D loss: 1.427487, acc.: 50.00%] [G loss: 2.917868]\n",
      "464 [D loss: 1.487157, acc.: 50.00%] [G loss: 2.859311]\n",
      "465 [D loss: 1.469514, acc.: 50.00%] [G loss: 2.902170]\n",
      "466 [D loss: 1.483517, acc.: 50.00%] [G loss: 2.855727]\n",
      "467 [D loss: 1.469047, acc.: 50.00%] [G loss: 2.829737]\n",
      "468 [D loss: 1.458494, acc.: 50.00%] [G loss: 2.855719]\n",
      "469 [D loss: 1.468999, acc.: 50.00%] [G loss: 2.784374]\n",
      "470 [D loss: 1.466521, acc.: 50.00%] [G loss: 2.569914]\n",
      "471 [D loss: 1.471600, acc.: 50.00%] [G loss: 3.088592]\n",
      "472 [D loss: 1.468241, acc.: 50.00%] [G loss: 2.867754]\n",
      "473 [D loss: 1.492440, acc.: 50.00%] [G loss: 2.860677]\n",
      "474 [D loss: 1.445984, acc.: 50.00%] [G loss: 2.905552]\n",
      "475 [D loss: 1.426424, acc.: 50.00%] [G loss: 2.871906]\n",
      "476 [D loss: 1.444484, acc.: 50.00%] [G loss: 2.945992]\n",
      "477 [D loss: 1.432327, acc.: 50.00%] [G loss: 2.876285]\n",
      "478 [D loss: 1.398552, acc.: 50.00%] [G loss: 2.930181]\n",
      "479 [D loss: 1.482314, acc.: 50.00%] [G loss: 2.836555]\n",
      "480 [D loss: 1.431207, acc.: 50.00%] [G loss: 2.905570]\n",
      "481 [D loss: 1.428059, acc.: 50.00%] [G loss: 2.850265]\n",
      "482 [D loss: 1.453343, acc.: 50.00%] [G loss: 2.839086]\n",
      "483 [D loss: 1.541608, acc.: 50.00%] [G loss: 2.833226]\n",
      "484 [D loss: 1.485378, acc.: 50.00%] [G loss: 2.836673]\n",
      "485 [D loss: 1.475741, acc.: 50.00%] [G loss: 2.742753]\n",
      "486 [D loss: 1.466794, acc.: 50.00%] [G loss: 2.839629]\n",
      "487 [D loss: 1.414867, acc.: 50.00%] [G loss: 2.835063]\n",
      "488 [D loss: 1.460037, acc.: 50.00%] [G loss: 2.827684]\n",
      "489 [D loss: 1.425748, acc.: 50.00%] [G loss: 2.872241]\n",
      "490 [D loss: 1.436823, acc.: 50.00%] [G loss: 2.887379]\n",
      "491 [D loss: 1.433115, acc.: 50.00%] [G loss: 2.876269]\n",
      "492 [D loss: 1.418252, acc.: 50.00%] [G loss: 2.882617]\n",
      "493 [D loss: 1.426357, acc.: 50.00%] [G loss: 2.880728]\n",
      "494 [D loss: 1.424768, acc.: 50.00%] [G loss: 2.832402]\n",
      "495 [D loss: 1.447054, acc.: 50.00%] [G loss: 2.843033]\n",
      "496 [D loss: 1.410583, acc.: 50.00%] [G loss: 2.732327]\n",
      "497 [D loss: 1.423311, acc.: 50.00%] [G loss: 2.828374]\n",
      "498 [D loss: 1.466611, acc.: 50.00%] [G loss: 2.858099]\n",
      "499 [D loss: 1.418936, acc.: 50.00%] [G loss: 2.785513]\n",
      "500 [D loss: 1.405675, acc.: 50.00%] [G loss: 2.794330]\n",
      "501 [D loss: 1.425027, acc.: 50.00%] [G loss: 2.729465]\n",
      "502 [D loss: 1.441445, acc.: 50.00%] [G loss: 2.828197]\n",
      "503 [D loss: 1.442210, acc.: 50.00%] [G loss: 2.759223]\n",
      "504 [D loss: 1.447367, acc.: 50.00%] [G loss: 2.906673]\n",
      "505 [D loss: 1.424428, acc.: 50.00%] [G loss: 2.800941]\n",
      "506 [D loss: 1.429482, acc.: 50.00%] [G loss: 2.852048]\n",
      "507 [D loss: 1.456478, acc.: 50.00%] [G loss: 2.805760]\n",
      "508 [D loss: 1.394464, acc.: 50.00%] [G loss: 2.864200]\n",
      "509 [D loss: 1.409534, acc.: 50.00%] [G loss: 2.775093]\n",
      "510 [D loss: 1.424682, acc.: 50.00%] [G loss: 2.773895]\n",
      "511 [D loss: 1.395510, acc.: 50.00%] [G loss: 2.725396]\n",
      "512 [D loss: 1.444154, acc.: 50.00%] [G loss: 2.754685]\n",
      "513 [D loss: 1.438251, acc.: 50.00%] [G loss: 2.812859]\n",
      "514 [D loss: 1.464339, acc.: 50.00%] [G loss: 2.829604]\n",
      "515 [D loss: 1.402322, acc.: 50.00%] [G loss: 2.728596]\n",
      "516 [D loss: 1.417474, acc.: 50.00%] [G loss: 2.844575]\n",
      "517 [D loss: 1.382113, acc.: 50.00%] [G loss: 2.667470]\n",
      "518 [D loss: 1.437684, acc.: 50.00%] [G loss: 2.754658]\n",
      "519 [D loss: 1.429664, acc.: 50.00%] [G loss: 2.735889]\n",
      "520 [D loss: 1.402052, acc.: 50.00%] [G loss: 2.754873]\n",
      "521 [D loss: 1.448110, acc.: 50.00%] [G loss: 2.796563]\n",
      "522 [D loss: 1.437150, acc.: 50.00%] [G loss: 2.790296]\n",
      "523 [D loss: 1.391175, acc.: 50.00%] [G loss: 2.797368]\n",
      "524 [D loss: 1.413494, acc.: 50.00%] [G loss: 2.693029]\n",
      "525 [D loss: 1.400578, acc.: 50.00%] [G loss: 2.716121]\n",
      "526 [D loss: 1.401468, acc.: 50.00%] [G loss: 2.732647]\n",
      "527 [D loss: 1.400227, acc.: 50.00%] [G loss: 2.722087]\n",
      "528 [D loss: 1.413820, acc.: 50.00%] [G loss: 2.721034]\n",
      "529 [D loss: 1.378102, acc.: 50.00%] [G loss: 2.735188]\n",
      "530 [D loss: 1.400214, acc.: 50.00%] [G loss: 2.722449]\n",
      "531 [D loss: 1.442681, acc.: 50.00%] [G loss: 2.730394]\n",
      "532 [D loss: 1.358802, acc.: 50.00%] [G loss: 2.767080]\n",
      "533 [D loss: 1.421887, acc.: 50.00%] [G loss: 2.716745]\n",
      "534 [D loss: 1.391636, acc.: 50.00%] [G loss: 2.694934]\n",
      "535 [D loss: 1.405821, acc.: 50.00%] [G loss: 2.653184]\n",
      "536 [D loss: 1.392576, acc.: 50.00%] [G loss: 2.711319]\n",
      "537 [D loss: 1.408581, acc.: 50.00%] [G loss: 2.786090]\n",
      "538 [D loss: 1.454593, acc.: 50.00%] [G loss: 2.617772]\n",
      "539 [D loss: 1.431892, acc.: 50.00%] [G loss: 2.722215]\n",
      "540 [D loss: 1.402848, acc.: 50.00%] [G loss: 2.635748]\n",
      "541 [D loss: 1.359440, acc.: 50.00%] [G loss: 2.771212]\n",
      "542 [D loss: 1.427864, acc.: 50.00%] [G loss: 2.754773]\n",
      "543 [D loss: 1.381549, acc.: 50.00%] [G loss: 2.747627]\n",
      "544 [D loss: 1.384165, acc.: 50.00%] [G loss: 2.835290]\n",
      "545 [D loss: 1.415422, acc.: 50.00%] [G loss: 2.678425]\n",
      "546 [D loss: 1.388075, acc.: 50.00%] [G loss: 2.814332]\n",
      "547 [D loss: 1.393849, acc.: 50.00%] [G loss: 2.698344]\n",
      "548 [D loss: 1.467987, acc.: 50.00%] [G loss: 2.732278]\n",
      "549 [D loss: 1.383177, acc.: 50.00%] [G loss: 2.697477]\n",
      "550 [D loss: 1.389876, acc.: 50.00%] [G loss: 2.775473]\n",
      "551 [D loss: 1.406852, acc.: 50.00%] [G loss: 2.726790]\n",
      "552 [D loss: 1.394437, acc.: 50.00%] [G loss: 2.740777]\n",
      "553 [D loss: 1.403024, acc.: 50.00%] [G loss: 2.712753]\n",
      "554 [D loss: 1.431911, acc.: 50.00%] [G loss: 2.664881]\n",
      "555 [D loss: 1.321643, acc.: 50.00%] [G loss: 2.710442]\n",
      "556 [D loss: 1.386394, acc.: 50.00%] [G loss: 2.852067]\n",
      "557 [D loss: 1.380103, acc.: 50.00%] [G loss: 2.662884]\n",
      "558 [D loss: 1.405959, acc.: 50.00%] [G loss: 2.676631]\n",
      "559 [D loss: 1.415074, acc.: 50.00%] [G loss: 2.700747]\n",
      "560 [D loss: 1.389037, acc.: 50.00%] [G loss: 2.754547]\n",
      "561 [D loss: 1.370135, acc.: 50.00%] [G loss: 2.588806]\n",
      "562 [D loss: 1.320800, acc.: 50.00%] [G loss: 2.530394]\n",
      "563 [D loss: 1.426036, acc.: 50.00%] [G loss: 2.628892]\n",
      "564 [D loss: 1.357777, acc.: 50.00%] [G loss: 2.684342]\n",
      "565 [D loss: 1.379985, acc.: 50.00%] [G loss: 2.670522]\n",
      "566 [D loss: 1.377758, acc.: 50.00%] [G loss: 2.658738]\n",
      "567 [D loss: 1.413418, acc.: 50.00%] [G loss: 2.730647]\n",
      "568 [D loss: 1.444712, acc.: 50.00%] [G loss: 2.679473]\n",
      "569 [D loss: 1.396550, acc.: 50.00%] [G loss: 2.549078]\n",
      "570 [D loss: 1.390307, acc.: 50.00%] [G loss: 2.657324]\n",
      "571 [D loss: 1.348680, acc.: 50.00%] [G loss: 2.665684]\n",
      "572 [D loss: 1.401338, acc.: 50.00%] [G loss: 2.699549]\n",
      "573 [D loss: 1.411781, acc.: 50.00%] [G loss: 2.626559]\n",
      "574 [D loss: 1.391889, acc.: 50.00%] [G loss: 2.609677]\n",
      "575 [D loss: 1.378576, acc.: 50.00%] [G loss: 2.731694]\n",
      "576 [D loss: 1.398604, acc.: 50.00%] [G loss: 2.613303]\n",
      "577 [D loss: 1.348887, acc.: 50.00%] [G loss: 2.648641]\n",
      "578 [D loss: 1.404863, acc.: 50.00%] [G loss: 2.652639]\n",
      "579 [D loss: 1.350329, acc.: 50.00%] [G loss: 2.659563]\n",
      "580 [D loss: 1.333696, acc.: 50.00%] [G loss: 2.559652]\n",
      "581 [D loss: 1.410426, acc.: 50.00%] [G loss: 2.649716]\n",
      "582 [D loss: 1.448933, acc.: 50.00%] [G loss: 2.628265]\n",
      "583 [D loss: 1.347163, acc.: 50.00%] [G loss: 2.673982]\n",
      "584 [D loss: 1.400222, acc.: 50.00%] [G loss: 2.569188]\n",
      "585 [D loss: 1.364062, acc.: 50.00%] [G loss: 2.634236]\n",
      "586 [D loss: 1.353673, acc.: 50.00%] [G loss: 2.664577]\n",
      "587 [D loss: 1.384139, acc.: 50.00%] [G loss: 2.717854]\n",
      "588 [D loss: 1.389560, acc.: 50.00%] [G loss: 2.590044]\n",
      "589 [D loss: 1.411694, acc.: 50.00%] [G loss: 2.643090]\n",
      "590 [D loss: 1.426785, acc.: 50.00%] [G loss: 2.636216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591 [D loss: 1.343251, acc.: 50.00%] [G loss: 2.617828]\n",
      "592 [D loss: 1.378419, acc.: 50.00%] [G loss: 2.695639]\n",
      "593 [D loss: 1.338451, acc.: 50.00%] [G loss: 2.659133]\n",
      "594 [D loss: 1.361656, acc.: 50.00%] [G loss: 2.634039]\n",
      "595 [D loss: 1.383519, acc.: 50.00%] [G loss: 2.601382]\n",
      "596 [D loss: 1.357399, acc.: 50.00%] [G loss: 2.612926]\n",
      "597 [D loss: 1.354334, acc.: 50.00%] [G loss: 2.701282]\n",
      "598 [D loss: 1.386091, acc.: 50.00%] [G loss: 2.535087]\n",
      "599 [D loss: 1.354662, acc.: 50.00%] [G loss: 2.525061]\n",
      "600 [D loss: 1.354308, acc.: 50.00%] [G loss: 2.591760]\n",
      "601 [D loss: 1.334350, acc.: 50.00%] [G loss: 2.612878]\n",
      "602 [D loss: 1.331649, acc.: 50.00%] [G loss: 2.386233]\n",
      "603 [D loss: 1.415372, acc.: 50.00%] [G loss: 2.345895]\n",
      "604 [D loss: 1.375196, acc.: 50.00%] [G loss: 2.362461]\n",
      "605 [D loss: 1.340444, acc.: 50.00%] [G loss: 2.379673]\n",
      "606 [D loss: 1.368127, acc.: 50.00%] [G loss: 3.156519]\n",
      "607 [D loss: 1.383306, acc.: 50.00%] [G loss: 2.205046]\n",
      "608 [D loss: 1.314205, acc.: 50.00%] [G loss: 3.081998]\n",
      "609 [D loss: 1.294201, acc.: 50.00%] [G loss: 2.661970]\n",
      "610 [D loss: 1.358916, acc.: 50.00%] [G loss: 2.664634]\n",
      "611 [D loss: 1.384990, acc.: 50.00%] [G loss: 2.354712]\n",
      "612 [D loss: 1.372104, acc.: 50.00%] [G loss: 2.411981]\n",
      "613 [D loss: 1.388465, acc.: 50.00%] [G loss: 2.291254]\n",
      "614 [D loss: 1.397946, acc.: 50.00%] [G loss: 2.374010]\n",
      "615 [D loss: 1.400072, acc.: 50.00%] [G loss: 3.199273]\n",
      "616 [D loss: 1.344571, acc.: 50.00%] [G loss: 2.370417]\n",
      "617 [D loss: 1.313592, acc.: 50.00%] [G loss: 2.308640]\n",
      "618 [D loss: 1.348463, acc.: 50.00%] [G loss: 2.486027]\n",
      "619 [D loss: 1.348436, acc.: 50.00%] [G loss: 2.251494]\n",
      "620 [D loss: 1.375840, acc.: 50.00%] [G loss: 2.345423]\n",
      "621 [D loss: 1.354356, acc.: 50.00%] [G loss: 2.288887]\n",
      "622 [D loss: 1.372771, acc.: 50.00%] [G loss: 2.370742]\n",
      "623 [D loss: 1.416354, acc.: 50.00%] [G loss: 2.868630]\n",
      "624 [D loss: 1.367926, acc.: 50.00%] [G loss: 3.671892]\n",
      "625 [D loss: 1.301678, acc.: 50.00%] [G loss: 2.252901]\n",
      "626 [D loss: 1.407059, acc.: 50.00%] [G loss: 2.463698]\n",
      "627 [D loss: 1.406191, acc.: 50.00%] [G loss: 2.139141]\n",
      "628 [D loss: 1.398845, acc.: 50.00%] [G loss: 3.025450]\n",
      "629 [D loss: 1.373343, acc.: 50.00%] [G loss: 3.056429]\n",
      "630 [D loss: 1.346640, acc.: 50.00%] [G loss: 2.246203]\n",
      "631 [D loss: 1.347534, acc.: 50.00%] [G loss: 4.570992]\n",
      "632 [D loss: 1.375058, acc.: 50.00%] [G loss: 2.568113]\n",
      "633 [D loss: 1.323161, acc.: 50.00%] [G loss: 3.022265]\n",
      "634 [D loss: 1.369481, acc.: 50.00%] [G loss: 2.352789]\n",
      "635 [D loss: 1.391863, acc.: 50.00%] [G loss: 2.449278]\n",
      "636 [D loss: 1.318370, acc.: 50.00%] [G loss: 2.701518]\n",
      "637 [D loss: 1.289116, acc.: 50.00%] [G loss: 2.683821]\n",
      "638 [D loss: 1.363524, acc.: 50.00%] [G loss: 2.653357]\n",
      "639 [D loss: 1.318226, acc.: 50.00%] [G loss: 2.755575]\n",
      "640 [D loss: 1.306365, acc.: 50.00%] [G loss: 3.161834]\n",
      "641 [D loss: 1.377426, acc.: 50.00%] [G loss: 2.727079]\n",
      "642 [D loss: 1.335259, acc.: 50.00%] [G loss: 3.056035]\n",
      "643 [D loss: 1.331952, acc.: 50.00%] [G loss: 2.990034]\n",
      "644 [D loss: 1.365472, acc.: 50.00%] [G loss: 2.400106]\n",
      "645 [D loss: 1.330877, acc.: 50.00%] [G loss: 2.573612]\n",
      "646 [D loss: 1.337306, acc.: 50.00%] [G loss: 4.627148]\n",
      "647 [D loss: 1.315710, acc.: 50.00%] [G loss: 3.156155]\n",
      "648 [D loss: 1.308356, acc.: 50.00%] [G loss: 2.464304]\n",
      "649 [D loss: 1.382401, acc.: 50.00%] [G loss: 2.478418]\n",
      "650 [D loss: 1.324521, acc.: 50.00%] [G loss: 2.420428]\n",
      "651 [D loss: 1.284776, acc.: 50.00%] [G loss: 2.563999]\n",
      "652 [D loss: 1.346165, acc.: 50.00%] [G loss: 3.112297]\n",
      "653 [D loss: 1.354566, acc.: 50.00%] [G loss: 3.268340]\n",
      "654 [D loss: 1.334892, acc.: 50.00%] [G loss: 2.722361]\n",
      "655 [D loss: 1.319230, acc.: 50.00%] [G loss: 4.611845]\n",
      "656 [D loss: 1.334232, acc.: 50.00%] [G loss: 3.266759]\n",
      "657 [D loss: 1.250913, acc.: 50.00%] [G loss: 2.745383]\n",
      "658 [D loss: 1.310568, acc.: 50.00%] [G loss: 2.796587]\n",
      "659 [D loss: 1.348639, acc.: 50.00%] [G loss: 2.635137]\n",
      "660 [D loss: 1.357472, acc.: 50.00%] [G loss: 2.653222]\n",
      "661 [D loss: 1.360418, acc.: 50.00%] [G loss: 2.626037]\n",
      "662 [D loss: 1.303079, acc.: 50.00%] [G loss: 2.617301]\n",
      "663 [D loss: 1.324549, acc.: 50.00%] [G loss: 2.580001]\n",
      "664 [D loss: 1.256678, acc.: 50.00%] [G loss: 2.637362]\n",
      "665 [D loss: 1.286887, acc.: 50.00%] [G loss: 2.517562]\n",
      "666 [D loss: 1.278024, acc.: 50.00%] [G loss: 2.598298]\n",
      "667 [D loss: 1.278731, acc.: 50.00%] [G loss: 2.648874]\n",
      "668 [D loss: 1.304444, acc.: 50.00%] [G loss: 2.613783]\n",
      "669 [D loss: 1.307935, acc.: 50.00%] [G loss: 2.694971]\n",
      "670 [D loss: 1.280687, acc.: 50.00%] [G loss: 2.581090]\n",
      "671 [D loss: 1.337489, acc.: 50.00%] [G loss: 2.483128]\n",
      "672 [D loss: 1.300926, acc.: 50.00%] [G loss: 2.569213]\n",
      "673 [D loss: 1.294294, acc.: 50.00%] [G loss: 2.534426]\n",
      "674 [D loss: 1.346540, acc.: 50.00%] [G loss: 2.615908]\n",
      "675 [D loss: 1.284622, acc.: 50.00%] [G loss: 2.541011]\n",
      "676 [D loss: 1.277766, acc.: 50.00%] [G loss: 2.489226]\n",
      "677 [D loss: 1.205992, acc.: 50.00%] [G loss: 2.434239]\n",
      "678 [D loss: 1.317966, acc.: 50.00%] [G loss: 2.455420]\n",
      "679 [D loss: 1.282485, acc.: 50.00%] [G loss: 2.511954]\n",
      "680 [D loss: 1.279334, acc.: 50.00%] [G loss: 2.455806]\n",
      "681 [D loss: 1.329460, acc.: 50.00%] [G loss: 2.475008]\n",
      "682 [D loss: 1.318404, acc.: 50.00%] [G loss: 2.535493]\n",
      "683 [D loss: 1.290863, acc.: 50.00%] [G loss: 2.474569]\n",
      "684 [D loss: 1.310747, acc.: 50.00%] [G loss: 2.530014]\n",
      "685 [D loss: 1.268729, acc.: 50.00%] [G loss: 2.474308]\n",
      "686 [D loss: 1.287162, acc.: 50.00%] [G loss: 2.553470]\n",
      "687 [D loss: 1.306457, acc.: 50.00%] [G loss: 2.490711]\n",
      "688 [D loss: 1.302074, acc.: 50.00%] [G loss: 2.499694]\n",
      "689 [D loss: 1.244139, acc.: 50.00%] [G loss: 2.562363]\n",
      "690 [D loss: 1.331955, acc.: 50.00%] [G loss: 2.521396]\n",
      "691 [D loss: 1.254331, acc.: 50.00%] [G loss: 2.435687]\n",
      "692 [D loss: 1.298119, acc.: 50.00%] [G loss: 2.545400]\n",
      "693 [D loss: 1.280776, acc.: 50.00%] [G loss: 2.561202]\n",
      "694 [D loss: 1.243902, acc.: 50.00%] [G loss: 2.519397]\n",
      "695 [D loss: 1.309651, acc.: 50.00%] [G loss: 2.508159]\n",
      "696 [D loss: 1.259122, acc.: 50.00%] [G loss: 2.466111]\n",
      "697 [D loss: 1.254142, acc.: 50.00%] [G loss: 2.524701]\n",
      "698 [D loss: 1.307583, acc.: 50.00%] [G loss: 2.562248]\n",
      "699 [D loss: 1.275784, acc.: 50.00%] [G loss: 2.426695]\n",
      "700 [D loss: 1.275391, acc.: 50.00%] [G loss: 2.480365]\n",
      "701 [D loss: 1.288093, acc.: 50.00%] [G loss: 2.452871]\n",
      "702 [D loss: 1.300159, acc.: 50.00%] [G loss: 2.461624]\n",
      "703 [D loss: 1.298725, acc.: 50.00%] [G loss: 2.461550]\n",
      "704 [D loss: 1.288823, acc.: 50.00%] [G loss: 2.562645]\n",
      "705 [D loss: 1.292204, acc.: 50.00%] [G loss: 2.546216]\n",
      "706 [D loss: 1.278553, acc.: 50.00%] [G loss: 2.456850]\n",
      "707 [D loss: 1.272830, acc.: 50.00%] [G loss: 2.508012]\n",
      "708 [D loss: 1.307001, acc.: 50.00%] [G loss: 2.376535]\n",
      "709 [D loss: 1.289957, acc.: 50.00%] [G loss: 2.506065]\n",
      "710 [D loss: 1.294752, acc.: 50.00%] [G loss: 2.495698]\n",
      "711 [D loss: 1.273680, acc.: 50.00%] [G loss: 2.450206]\n",
      "712 [D loss: 1.258405, acc.: 50.00%] [G loss: 2.433718]\n",
      "713 [D loss: 1.242800, acc.: 50.00%] [G loss: 2.426830]\n",
      "714 [D loss: 1.267440, acc.: 50.00%] [G loss: 2.472264]\n",
      "715 [D loss: 1.261262, acc.: 50.00%] [G loss: 2.419435]\n",
      "716 [D loss: 1.281737, acc.: 50.00%] [G loss: 2.500521]\n",
      "717 [D loss: 1.280052, acc.: 50.00%] [G loss: 2.503628]\n",
      "718 [D loss: 1.307384, acc.: 50.00%] [G loss: 2.491374]\n",
      "719 [D loss: 1.285485, acc.: 50.00%] [G loss: 2.456475]\n",
      "720 [D loss: 1.247287, acc.: 50.00%] [G loss: 2.456858]\n",
      "721 [D loss: 1.250122, acc.: 50.00%] [G loss: 2.435231]\n",
      "722 [D loss: 1.253992, acc.: 50.00%] [G loss: 2.494741]\n",
      "723 [D loss: 1.313716, acc.: 50.00%] [G loss: 2.419048]\n",
      "724 [D loss: 1.245596, acc.: 50.00%] [G loss: 2.480498]\n",
      "725 [D loss: 1.265418, acc.: 50.00%] [G loss: 2.484825]\n",
      "726 [D loss: 1.275508, acc.: 50.00%] [G loss: 2.456453]\n",
      "727 [D loss: 1.289289, acc.: 50.00%] [G loss: 2.441385]\n",
      "728 [D loss: 1.249771, acc.: 50.00%] [G loss: 2.383311]\n",
      "729 [D loss: 1.255213, acc.: 50.00%] [G loss: 2.476116]\n",
      "730 [D loss: 1.259772, acc.: 50.00%] [G loss: 2.498994]\n",
      "731 [D loss: 1.221371, acc.: 50.00%] [G loss: 2.347617]\n",
      "732 [D loss: 1.267836, acc.: 50.00%] [G loss: 2.449435]\n",
      "733 [D loss: 1.257388, acc.: 50.00%] [G loss: 2.464840]\n",
      "734 [D loss: 1.255544, acc.: 50.00%] [G loss: 2.389175]\n",
      "735 [D loss: 1.298849, acc.: 50.00%] [G loss: 2.471423]\n",
      "736 [D loss: 1.257157, acc.: 50.00%] [G loss: 2.420365]\n",
      "737 [D loss: 1.317854, acc.: 50.00%] [G loss: 2.453760]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738 [D loss: 1.264306, acc.: 50.00%] [G loss: 2.488098]\n",
      "739 [D loss: 1.257567, acc.: 50.00%] [G loss: 2.395289]\n",
      "740 [D loss: 1.224126, acc.: 50.00%] [G loss: 2.345053]\n",
      "741 [D loss: 1.237979, acc.: 50.00%] [G loss: 2.492861]\n",
      "742 [D loss: 1.244491, acc.: 50.00%] [G loss: 2.413664]\n",
      "743 [D loss: 1.291155, acc.: 50.00%] [G loss: 2.359713]\n",
      "744 [D loss: 1.285105, acc.: 50.00%] [G loss: 2.470769]\n",
      "745 [D loss: 1.235955, acc.: 50.00%] [G loss: 2.414611]\n",
      "746 [D loss: 1.317538, acc.: 50.00%] [G loss: 2.370533]\n",
      "747 [D loss: 1.243183, acc.: 50.00%] [G loss: 2.403903]\n",
      "748 [D loss: 1.288469, acc.: 50.00%] [G loss: 2.327843]\n",
      "749 [D loss: 1.257392, acc.: 50.00%] [G loss: 2.391223]\n",
      "750 [D loss: 1.266176, acc.: 50.00%] [G loss: 2.541719]\n",
      "751 [D loss: 1.250535, acc.: 50.00%] [G loss: 2.450081]\n",
      "752 [D loss: 1.212818, acc.: 50.00%] [G loss: 2.427105]\n",
      "753 [D loss: 1.277587, acc.: 50.00%] [G loss: 2.453372]\n",
      "754 [D loss: 1.262078, acc.: 50.00%] [G loss: 2.394966]\n",
      "755 [D loss: 1.224007, acc.: 50.00%] [G loss: 2.458275]\n",
      "756 [D loss: 1.253192, acc.: 50.00%] [G loss: 2.507937]\n",
      "757 [D loss: 1.235082, acc.: 50.00%] [G loss: 2.429518]\n",
      "758 [D loss: 1.283216, acc.: 50.00%] [G loss: 2.360110]\n",
      "759 [D loss: 1.229769, acc.: 50.00%] [G loss: 2.361946]\n",
      "760 [D loss: 1.238773, acc.: 50.00%] [G loss: 2.345485]\n",
      "761 [D loss: 1.246435, acc.: 50.00%] [G loss: 2.349275]\n",
      "762 [D loss: 1.266142, acc.: 50.00%] [G loss: 2.401741]\n",
      "763 [D loss: 1.207743, acc.: 50.00%] [G loss: 2.459865]\n",
      "764 [D loss: 1.264228, acc.: 50.00%] [G loss: 2.394382]\n",
      "765 [D loss: 1.203347, acc.: 50.00%] [G loss: 2.508110]\n",
      "766 [D loss: 1.234423, acc.: 50.00%] [G loss: 2.339738]\n",
      "767 [D loss: 1.281825, acc.: 50.00%] [G loss: 2.508242]\n",
      "768 [D loss: 1.189881, acc.: 50.00%] [G loss: 2.344799]\n",
      "769 [D loss: 1.210280, acc.: 50.00%] [G loss: 2.546995]\n",
      "770 [D loss: 1.195598, acc.: 50.00%] [G loss: 2.411858]\n",
      "771 [D loss: 1.266665, acc.: 50.00%] [G loss: 2.323737]\n",
      "772 [D loss: 1.251490, acc.: 50.00%] [G loss: 2.378872]\n",
      "773 [D loss: 1.259093, acc.: 50.00%] [G loss: 2.398588]\n",
      "774 [D loss: 1.173277, acc.: 50.00%] [G loss: 2.329902]\n",
      "775 [D loss: 1.275309, acc.: 50.00%] [G loss: 2.341126]\n",
      "776 [D loss: 1.295182, acc.: 50.00%] [G loss: 2.379886]\n",
      "777 [D loss: 1.269372, acc.: 50.00%] [G loss: 2.417802]\n",
      "778 [D loss: 1.251175, acc.: 50.00%] [G loss: 2.383789]\n",
      "779 [D loss: 1.265404, acc.: 50.00%] [G loss: 2.486177]\n",
      "780 [D loss: 1.234967, acc.: 50.00%] [G loss: 2.350984]\n",
      "781 [D loss: 1.244916, acc.: 50.00%] [G loss: 2.289096]\n",
      "782 [D loss: 1.264421, acc.: 50.00%] [G loss: 2.248732]\n",
      "783 [D loss: 1.205710, acc.: 50.00%] [G loss: 2.298220]\n",
      "784 [D loss: 1.208212, acc.: 50.00%] [G loss: 2.386430]\n",
      "785 [D loss: 1.232829, acc.: 50.00%] [G loss: 2.325424]\n",
      "786 [D loss: 1.222907, acc.: 50.00%] [G loss: 2.434003]\n",
      "787 [D loss: 1.233462, acc.: 50.00%] [G loss: 2.302763]\n",
      "788 [D loss: 1.211712, acc.: 50.00%] [G loss: 2.332519]\n",
      "789 [D loss: 1.246539, acc.: 50.00%] [G loss: 2.439621]\n",
      "790 [D loss: 1.239992, acc.: 50.00%] [G loss: 2.276639]\n",
      "791 [D loss: 1.242586, acc.: 50.00%] [G loss: 2.352289]\n",
      "792 [D loss: 1.260757, acc.: 50.00%] [G loss: 2.284766]\n",
      "793 [D loss: 1.260744, acc.: 50.00%] [G loss: 2.319568]\n",
      "794 [D loss: 1.245512, acc.: 50.00%] [G loss: 2.326713]\n",
      "795 [D loss: 1.204849, acc.: 50.00%] [G loss: 2.397267]\n",
      "796 [D loss: 1.187764, acc.: 50.00%] [G loss: 2.372862]\n",
      "797 [D loss: 1.258425, acc.: 50.00%] [G loss: 2.349330]\n",
      "798 [D loss: 1.254109, acc.: 50.00%] [G loss: 2.366347]\n",
      "799 [D loss: 1.251501, acc.: 50.00%] [G loss: 2.257119]\n",
      "800 [D loss: 1.207046, acc.: 50.00%] [G loss: 2.436867]\n",
      "801 [D loss: 1.208035, acc.: 50.00%] [G loss: 2.371747]\n",
      "802 [D loss: 1.208718, acc.: 50.00%] [G loss: 2.284414]\n",
      "803 [D loss: 1.199351, acc.: 50.00%] [G loss: 2.304597]\n",
      "804 [D loss: 1.237529, acc.: 50.00%] [G loss: 2.218562]\n",
      "805 [D loss: 1.242334, acc.: 50.00%] [G loss: 2.341531]\n",
      "806 [D loss: 1.221915, acc.: 50.00%] [G loss: 2.295733]\n",
      "807 [D loss: 1.187254, acc.: 50.00%] [G loss: 2.369535]\n",
      "808 [D loss: 1.216605, acc.: 50.00%] [G loss: 2.335641]\n",
      "809 [D loss: 1.232069, acc.: 50.00%] [G loss: 2.300381]\n",
      "810 [D loss: 1.235648, acc.: 50.00%] [G loss: 2.236296]\n",
      "811 [D loss: 1.191162, acc.: 50.00%] [G loss: 2.343992]\n",
      "812 [D loss: 1.160145, acc.: 50.00%] [G loss: 2.309562]\n",
      "813 [D loss: 1.243503, acc.: 50.00%] [G loss: 2.306346]\n",
      "814 [D loss: 1.229073, acc.: 50.00%] [G loss: 2.440781]\n",
      "815 [D loss: 1.185124, acc.: 50.00%] [G loss: 2.298310]\n",
      "816 [D loss: 1.204950, acc.: 50.00%] [G loss: 2.314427]\n",
      "817 [D loss: 1.215525, acc.: 50.00%] [G loss: 2.367203]\n",
      "818 [D loss: 1.216833, acc.: 50.00%] [G loss: 2.217241]\n",
      "819 [D loss: 1.195298, acc.: 50.00%] [G loss: 2.394044]\n",
      "820 [D loss: 1.166796, acc.: 50.00%] [G loss: 2.257489]\n",
      "821 [D loss: 1.178556, acc.: 50.00%] [G loss: 2.259579]\n",
      "822 [D loss: 1.186422, acc.: 50.00%] [G loss: 2.212824]\n",
      "823 [D loss: 1.178466, acc.: 50.00%] [G loss: 2.298611]\n",
      "824 [D loss: 1.174374, acc.: 50.00%] [G loss: 2.253987]\n",
      "825 [D loss: 1.211694, acc.: 50.00%] [G loss: 2.312011]\n",
      "826 [D loss: 1.158551, acc.: 50.00%] [G loss: 2.297935]\n",
      "827 [D loss: 1.205634, acc.: 50.00%] [G loss: 2.246506]\n",
      "828 [D loss: 1.196419, acc.: 50.00%] [G loss: 2.304993]\n",
      "829 [D loss: 1.211575, acc.: 50.00%] [G loss: 2.377466]\n",
      "830 [D loss: 1.190897, acc.: 50.00%] [G loss: 2.233166]\n",
      "831 [D loss: 1.238898, acc.: 50.00%] [G loss: 2.219713]\n",
      "832 [D loss: 1.230657, acc.: 50.00%] [G loss: 2.265770]\n",
      "833 [D loss: 1.201237, acc.: 50.00%] [G loss: 2.381518]\n",
      "834 [D loss: 1.224389, acc.: 50.00%] [G loss: 2.291518]\n",
      "835 [D loss: 1.227601, acc.: 50.00%] [G loss: 2.190603]\n",
      "836 [D loss: 1.195731, acc.: 50.00%] [G loss: 2.196981]\n",
      "837 [D loss: 1.202770, acc.: 50.00%] [G loss: 2.377714]\n",
      "838 [D loss: 1.234277, acc.: 50.00%] [G loss: 2.235717]\n",
      "839 [D loss: 1.213404, acc.: 50.00%] [G loss: 2.288018]\n",
      "840 [D loss: 1.174432, acc.: 50.00%] [G loss: 2.323360]\n",
      "841 [D loss: 1.162446, acc.: 50.00%] [G loss: 2.204619]\n",
      "842 [D loss: 1.191849, acc.: 50.00%] [G loss: 2.313422]\n",
      "843 [D loss: 1.148988, acc.: 50.00%] [G loss: 2.431301]\n",
      "844 [D loss: 1.196567, acc.: 50.00%] [G loss: 2.144788]\n",
      "845 [D loss: 1.206498, acc.: 50.00%] [G loss: 2.201904]\n",
      "846 [D loss: 1.205313, acc.: 50.00%] [G loss: 2.263734]\n",
      "847 [D loss: 1.224949, acc.: 50.00%] [G loss: 2.306026]\n",
      "848 [D loss: 1.193301, acc.: 50.00%] [G loss: 2.263302]\n",
      "849 [D loss: 1.190711, acc.: 50.00%] [G loss: 2.316857]\n",
      "850 [D loss: 1.163007, acc.: 50.00%] [G loss: 2.343034]\n",
      "851 [D loss: 1.166158, acc.: 50.00%] [G loss: 2.355756]\n",
      "852 [D loss: 1.194644, acc.: 50.00%] [G loss: 2.296234]\n",
      "853 [D loss: 1.166336, acc.: 50.00%] [G loss: 2.241779]\n",
      "854 [D loss: 1.150655, acc.: 50.00%] [G loss: 2.234845]\n",
      "855 [D loss: 1.146611, acc.: 50.00%] [G loss: 2.240813]\n",
      "856 [D loss: 1.202826, acc.: 50.00%] [G loss: 2.193740]\n",
      "857 [D loss: 1.161672, acc.: 50.00%] [G loss: 2.341172]\n",
      "858 [D loss: 1.121695, acc.: 50.00%] [G loss: 2.287078]\n",
      "859 [D loss: 1.231914, acc.: 50.00%] [G loss: 2.257886]\n",
      "860 [D loss: 1.184621, acc.: 50.00%] [G loss: 2.288166]\n",
      "861 [D loss: 1.198843, acc.: 50.00%] [G loss: 2.293954]\n",
      "862 [D loss: 1.177794, acc.: 50.00%] [G loss: 2.266237]\n",
      "863 [D loss: 1.233222, acc.: 50.00%] [G loss: 2.240995]\n",
      "864 [D loss: 1.172712, acc.: 50.00%] [G loss: 2.248499]\n",
      "865 [D loss: 1.170532, acc.: 50.00%] [G loss: 2.323572]\n",
      "866 [D loss: 1.181472, acc.: 50.00%] [G loss: 2.295700]\n",
      "867 [D loss: 1.186550, acc.: 50.00%] [G loss: 2.249915]\n",
      "868 [D loss: 1.222916, acc.: 50.00%] [G loss: 2.114702]\n",
      "869 [D loss: 1.168059, acc.: 50.00%] [G loss: 2.137069]\n",
      "870 [D loss: 1.193731, acc.: 50.00%] [G loss: 2.221932]\n",
      "871 [D loss: 1.110956, acc.: 50.00%] [G loss: 2.335803]\n",
      "872 [D loss: 1.188675, acc.: 50.00%] [G loss: 2.329059]\n",
      "873 [D loss: 1.126622, acc.: 50.00%] [G loss: 2.226786]\n",
      "874 [D loss: 1.172559, acc.: 50.00%] [G loss: 2.279131]\n",
      "875 [D loss: 1.226980, acc.: 50.00%] [G loss: 2.260417]\n",
      "876 [D loss: 1.147241, acc.: 50.00%] [G loss: 2.195999]\n",
      "877 [D loss: 1.175782, acc.: 50.00%] [G loss: 2.223015]\n",
      "878 [D loss: 1.151714, acc.: 50.00%] [G loss: 2.254941]\n",
      "879 [D loss: 1.205114, acc.: 50.00%] [G loss: 2.228554]\n",
      "880 [D loss: 1.156751, acc.: 50.00%] [G loss: 2.197835]\n",
      "881 [D loss: 1.159705, acc.: 50.00%] [G loss: 2.251337]\n",
      "882 [D loss: 1.184369, acc.: 50.00%] [G loss: 2.178061]\n",
      "883 [D loss: 1.096238, acc.: 50.00%] [G loss: 2.351697]\n",
      "884 [D loss: 1.093915, acc.: 50.00%] [G loss: 2.212515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885 [D loss: 1.171221, acc.: 50.00%] [G loss: 2.192238]\n",
      "886 [D loss: 1.227682, acc.: 50.00%] [G loss: 2.303680]\n",
      "887 [D loss: 1.168816, acc.: 50.00%] [G loss: 2.289119]\n",
      "888 [D loss: 1.138430, acc.: 50.00%] [G loss: 2.283549]\n",
      "889 [D loss: 1.163140, acc.: 50.00%] [G loss: 2.174062]\n",
      "890 [D loss: 1.179129, acc.: 50.00%] [G loss: 2.250897]\n",
      "891 [D loss: 1.180176, acc.: 50.00%] [G loss: 2.177069]\n",
      "892 [D loss: 1.198618, acc.: 50.00%] [G loss: 2.257277]\n",
      "893 [D loss: 1.141080, acc.: 50.00%] [G loss: 2.134614]\n",
      "894 [D loss: 1.177546, acc.: 50.00%] [G loss: 2.179485]\n",
      "895 [D loss: 1.181812, acc.: 50.00%] [G loss: 2.256814]\n",
      "896 [D loss: 1.160422, acc.: 50.00%] [G loss: 2.271183]\n",
      "897 [D loss: 1.148265, acc.: 50.00%] [G loss: 2.237625]\n",
      "898 [D loss: 1.168884, acc.: 50.00%] [G loss: 2.113324]\n",
      "899 [D loss: 1.171939, acc.: 50.00%] [G loss: 2.248259]\n",
      "900 [D loss: 1.167876, acc.: 50.00%] [G loss: 2.209716]\n",
      "901 [D loss: 1.103187, acc.: 50.00%] [G loss: 2.183870]\n",
      "902 [D loss: 1.179464, acc.: 50.00%] [G loss: 2.186285]\n",
      "903 [D loss: 1.193994, acc.: 50.00%] [G loss: 2.157284]\n",
      "904 [D loss: 1.161500, acc.: 50.00%] [G loss: 2.278854]\n",
      "905 [D loss: 1.143635, acc.: 50.00%] [G loss: 2.257200]\n",
      "906 [D loss: 1.167787, acc.: 50.00%] [G loss: 2.190889]\n",
      "907 [D loss: 1.130976, acc.: 50.00%] [G loss: 2.209509]\n",
      "908 [D loss: 1.200112, acc.: 50.00%] [G loss: 2.232957]\n",
      "909 [D loss: 1.121645, acc.: 50.00%] [G loss: 2.031272]\n",
      "910 [D loss: 1.145522, acc.: 50.00%] [G loss: 2.191762]\n",
      "911 [D loss: 1.172550, acc.: 50.00%] [G loss: 2.280237]\n",
      "912 [D loss: 1.162755, acc.: 50.00%] [G loss: 2.222158]\n",
      "913 [D loss: 1.161778, acc.: 50.00%] [G loss: 2.116023]\n",
      "914 [D loss: 1.169092, acc.: 50.00%] [G loss: 2.194971]\n",
      "915 [D loss: 1.198820, acc.: 50.00%] [G loss: 2.269366]\n",
      "916 [D loss: 1.126422, acc.: 50.00%] [G loss: 2.126540]\n",
      "917 [D loss: 1.170825, acc.: 50.00%] [G loss: 2.226772]\n",
      "918 [D loss: 1.209423, acc.: 50.00%] [G loss: 2.150014]\n",
      "919 [D loss: 1.161562, acc.: 50.00%] [G loss: 2.179174]\n",
      "920 [D loss: 1.131231, acc.: 50.00%] [G loss: 2.167747]\n",
      "921 [D loss: 1.118077, acc.: 50.00%] [G loss: 2.223314]\n",
      "922 [D loss: 1.185284, acc.: 50.00%] [G loss: 2.164849]\n",
      "923 [D loss: 1.146496, acc.: 50.00%] [G loss: 2.239635]\n",
      "924 [D loss: 1.155288, acc.: 50.00%] [G loss: 2.161374]\n",
      "925 [D loss: 1.171033, acc.: 50.00%] [G loss: 2.099527]\n",
      "926 [D loss: 1.106717, acc.: 50.00%] [G loss: 2.052698]\n",
      "927 [D loss: 1.117379, acc.: 50.00%] [G loss: 2.146782]\n",
      "928 [D loss: 1.149881, acc.: 50.00%] [G loss: 2.222086]\n",
      "929 [D loss: 1.110914, acc.: 50.00%] [G loss: 2.106295]\n",
      "930 [D loss: 1.207167, acc.: 50.00%] [G loss: 2.164763]\n",
      "931 [D loss: 1.146210, acc.: 50.00%] [G loss: 2.030621]\n",
      "932 [D loss: 1.128011, acc.: 50.00%] [G loss: 2.162040]\n",
      "933 [D loss: 1.105150, acc.: 50.00%] [G loss: 2.145704]\n",
      "934 [D loss: 1.110258, acc.: 50.00%] [G loss: 2.057140]\n",
      "935 [D loss: 1.151943, acc.: 50.00%] [G loss: 2.147980]\n",
      "936 [D loss: 1.181119, acc.: 50.00%] [G loss: 2.148150]\n",
      "937 [D loss: 1.112729, acc.: 50.00%] [G loss: 2.172454]\n",
      "938 [D loss: 1.160586, acc.: 50.00%] [G loss: 2.170058]\n",
      "939 [D loss: 1.120510, acc.: 50.00%] [G loss: 2.051664]\n",
      "940 [D loss: 1.136753, acc.: 50.00%] [G loss: 2.166935]\n",
      "941 [D loss: 1.150058, acc.: 50.00%] [G loss: 2.201268]\n",
      "942 [D loss: 1.134425, acc.: 50.00%] [G loss: 2.113904]\n",
      "943 [D loss: 1.125308, acc.: 50.00%] [G loss: 2.120544]\n",
      "944 [D loss: 1.125450, acc.: 50.00%] [G loss: 2.210408]\n",
      "945 [D loss: 1.102864, acc.: 50.00%] [G loss: 2.190555]\n",
      "946 [D loss: 1.180475, acc.: 50.00%] [G loss: 2.099065]\n",
      "947 [D loss: 1.137237, acc.: 50.00%] [G loss: 2.158000]\n",
      "948 [D loss: 1.111841, acc.: 50.00%] [G loss: 2.079574]\n",
      "949 [D loss: 1.131045, acc.: 50.00%] [G loss: 2.086827]\n",
      "950 [D loss: 1.115191, acc.: 50.00%] [G loss: 2.162714]\n",
      "951 [D loss: 1.157183, acc.: 50.00%] [G loss: 2.146616]\n",
      "952 [D loss: 1.118736, acc.: 50.00%] [G loss: 2.208067]\n",
      "953 [D loss: 1.108378, acc.: 50.00%] [G loss: 2.198674]\n",
      "954 [D loss: 1.113582, acc.: 50.00%] [G loss: 2.102390]\n",
      "955 [D loss: 1.140767, acc.: 50.00%] [G loss: 2.251714]\n",
      "956 [D loss: 1.166514, acc.: 50.00%] [G loss: 2.154513]\n",
      "957 [D loss: 1.153399, acc.: 50.00%] [G loss: 2.105765]\n",
      "958 [D loss: 1.141608, acc.: 50.00%] [G loss: 2.120923]\n",
      "959 [D loss: 1.167553, acc.: 50.00%] [G loss: 2.190455]\n",
      "960 [D loss: 1.127441, acc.: 50.00%] [G loss: 2.175779]\n",
      "961 [D loss: 1.137131, acc.: 50.00%] [G loss: 2.082009]\n",
      "962 [D loss: 1.098693, acc.: 50.00%] [G loss: 2.153159]\n",
      "963 [D loss: 1.161348, acc.: 50.00%] [G loss: 2.088432]\n",
      "964 [D loss: 1.096624, acc.: 50.00%] [G loss: 2.033024]\n",
      "965 [D loss: 1.127736, acc.: 50.00%] [G loss: 2.033092]\n",
      "966 [D loss: 1.129690, acc.: 50.00%] [G loss: 2.115761]\n",
      "967 [D loss: 1.124624, acc.: 50.00%] [G loss: 2.177699]\n",
      "968 [D loss: 1.118873, acc.: 50.00%] [G loss: 2.148802]\n",
      "969 [D loss: 1.155362, acc.: 50.00%] [G loss: 2.107510]\n",
      "970 [D loss: 1.148067, acc.: 50.00%] [G loss: 2.228263]\n",
      "971 [D loss: 1.136181, acc.: 50.00%] [G loss: 2.103691]\n",
      "972 [D loss: 1.137695, acc.: 50.00%] [G loss: 2.036459]\n",
      "973 [D loss: 1.127085, acc.: 50.00%] [G loss: 2.116639]\n",
      "974 [D loss: 1.198969, acc.: 50.00%] [G loss: 2.131349]\n",
      "975 [D loss: 1.121155, acc.: 50.00%] [G loss: 2.132738]\n",
      "976 [D loss: 1.121188, acc.: 50.00%] [G loss: 2.107673]\n",
      "977 [D loss: 1.144676, acc.: 50.00%] [G loss: 2.081220]\n",
      "978 [D loss: 1.081439, acc.: 50.00%] [G loss: 2.047319]\n",
      "979 [D loss: 1.095652, acc.: 50.00%] [G loss: 2.115176]\n",
      "980 [D loss: 1.150913, acc.: 50.00%] [G loss: 2.141630]\n",
      "981 [D loss: 1.066705, acc.: 50.00%] [G loss: 2.082730]\n",
      "982 [D loss: 1.144181, acc.: 50.00%] [G loss: 2.127048]\n",
      "983 [D loss: 1.112232, acc.: 50.00%] [G loss: 2.044024]\n",
      "984 [D loss: 1.066142, acc.: 50.00%] [G loss: 2.033628]\n",
      "985 [D loss: 1.150716, acc.: 50.00%] [G loss: 2.083672]\n",
      "986 [D loss: 1.096922, acc.: 50.00%] [G loss: 2.054604]\n",
      "987 [D loss: 1.091918, acc.: 50.00%] [G loss: 2.025384]\n",
      "988 [D loss: 1.184750, acc.: 50.00%] [G loss: 2.145139]\n",
      "989 [D loss: 1.110513, acc.: 50.00%] [G loss: 2.117215]\n",
      "990 [D loss: 1.132085, acc.: 50.00%] [G loss: 2.118165]\n",
      "991 [D loss: 1.110948, acc.: 50.00%] [G loss: 2.130536]\n",
      "992 [D loss: 1.085745, acc.: 50.00%] [G loss: 2.204248]\n",
      "993 [D loss: 1.060166, acc.: 50.00%] [G loss: 2.164078]\n",
      "994 [D loss: 1.109937, acc.: 50.00%] [G loss: 2.208966]\n",
      "995 [D loss: 1.148201, acc.: 50.00%] [G loss: 2.037685]\n",
      "996 [D loss: 1.148372, acc.: 50.00%] [G loss: 2.019550]\n",
      "997 [D loss: 1.139192, acc.: 50.00%] [G loss: 1.982695]\n",
      "998 [D loss: 1.157138, acc.: 50.00%] [G loss: 2.196988]\n",
      "999 [D loss: 1.091142, acc.: 50.00%] [G loss: 1.994145]\n"
     ]
    }
   ],
   "source": [
    "lstmgan.train(epochs=1000, batch_size=20, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0h 6m 16s\n"
     ]
    }
   ],
   "source": [
    "end = datetime.datetime.now()\n",
    "\n",
    "diff = (end - start)\n",
    "\n",
    "datetime.timedelta(seconds=10, microseconds=885206)\n",
    "\n",
    "diff_seconds = int(diff.total_seconds())\n",
    "\n",
    "minute_seconds, seconds = divmod(diff_seconds, 60)\n",
    "hours, minutes = divmod(minute_seconds, 60)\n",
    "hms = f\"{hours}h {minutes}m {seconds}s\"\n",
    "\n",
    "'0h 0m 10s'\n",
    "print(hms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.28431934,  0.28434303, -0.10698528,  0.15844306,  0.14912014,\n",
      "         0.08968819, -0.19791116, -0.04135382, -0.06384034,  0.01779097,\n",
      "        -0.2999072 ,  0.09051267, -0.28736454,  0.089148  ,  0.28851303,\n",
      "        -0.1864599 ,  0.23846999, -0.17799062,  0.26283908,  0.25384516,\n",
      "        -0.14127801,  0.04750336,  0.07895382, -0.07056253, -0.00377254,\n",
      "        -0.019689  ,  0.03557055, -0.24165171, -0.15958256, -0.16766009,\n",
      "        -0.20969011,  0.03052367, -0.27741835,  0.06248332,  0.10685652,\n",
      "        -0.2944726 , -0.28630424, -0.07811561, -0.11139081, -0.17031273,\n",
      "        -0.26815042,  0.2851172 ,  0.17443505, -0.2513597 ,  0.10430487,\n",
      "         0.03859655,  0.03558356,  0.12534112, -0.22620471, -0.26395017,\n",
      "        -0.13305499,  0.09711919,  0.15595867, -0.01730466, -0.08355609,\n",
      "         0.17342363, -0.08217075, -0.16081262, -0.23089965, -0.15415373,\n",
      "        -0.25684077,  0.02137782, -0.29980794, -0.12354012]],\n",
      "      dtype=float32), array([[[-0.13860252, -0.08469637, -0.0049761 , ...,  0.03956202,\n",
      "         -0.10720182,  0.12538508],\n",
      "        [ 0.12068118, -0.07466569, -0.01562285, ..., -0.10263845,\n",
      "         -0.08480874, -0.06670097],\n",
      "        [-0.06623264, -0.1255142 , -0.06895079, ..., -0.03749621,\n",
      "         -0.01606674,  0.05545098],\n",
      "        ...,\n",
      "        [-0.03769019,  0.00038786,  0.12626934, ...,  0.07254738,\n",
      "         -0.04155016,  0.07039204],\n",
      "        [ 0.035607  ,  0.14364345,  0.12143312, ...,  0.04083151,\n",
      "         -0.02752576,  0.02443083],\n",
      "        [ 0.04683343,  0.04666902,  0.08188017, ..., -0.124797  ,\n",
      "          0.09075921, -0.12184189]],\n",
      "\n",
      "       [[ 0.05053293,  0.11888076,  0.09855218, ..., -0.00069152,\n",
      "          0.06171911, -0.13474791],\n",
      "        [ 0.00107537,  0.0357056 ,  0.01148028, ...,  0.08690907,\n",
      "          0.10750425, -0.08049737],\n",
      "        [-0.02275196, -0.00525902, -0.0898113 , ...,  0.07974762,\n",
      "          0.03694242, -0.11013204],\n",
      "        ...,\n",
      "        [-0.11188558, -0.08990729,  0.02082831, ..., -0.07133128,\n",
      "         -0.13895185, -0.01468619],\n",
      "        [-0.06709217,  0.09613205,  0.15114234, ..., -0.05756269,\n",
      "         -0.08598422, -0.04465714],\n",
      "        [ 0.02691335, -0.02498927,  0.11762633, ..., -0.00766763,\n",
      "          0.07016163, -0.13933457]],\n",
      "\n",
      "       [[ 0.13794829, -0.14297684, -0.03156016, ...,  0.0413047 ,\n",
      "         -0.01008573, -0.07553644],\n",
      "        [ 0.09951285, -0.06260137,  0.13413839, ...,  0.04161323,\n",
      "          0.02018368, -0.12009822],\n",
      "        [ 0.02452612,  0.04042427,  0.0460058 , ..., -0.05804691,\n",
      "         -0.14350079, -0.10693848],\n",
      "        ...,\n",
      "        [-0.03535238, -0.01648456,  0.03350973, ..., -0.07760615,\n",
      "         -0.12417725, -0.02333268],\n",
      "        [ 0.06530996,  0.07148658,  0.11575306, ..., -0.0649854 ,\n",
      "         -0.07776984, -0.10326976],\n",
      "        [-0.10201781, -0.00433419,  0.13863441, ...,  0.13784684,\n",
      "          0.01469402, -0.12081672]]], dtype=float32), array([[[-0.21919248],\n",
      "        [ 0.02409465],\n",
      "        [-0.20191278],\n",
      "        [-0.1713414 ],\n",
      "        [ 0.00236368],\n",
      "        [ 0.09108012],\n",
      "        [-0.1544864 ],\n",
      "        [ 0.02259797],\n",
      "        [-0.05256042],\n",
      "        [-0.04470447],\n",
      "        [-0.04082765],\n",
      "        [-0.12689129],\n",
      "        [-0.07025623],\n",
      "        [-0.12645435],\n",
      "        [ 0.21813811],\n",
      "        [ 0.1472072 ],\n",
      "        [ 0.06156003],\n",
      "        [ 0.16906792],\n",
      "        [ 0.2219287 ],\n",
      "        [-0.17883551],\n",
      "        [ 0.18193501],\n",
      "        [ 0.20792265],\n",
      "        [ 0.1968417 ],\n",
      "        [ 0.13110404],\n",
      "        [-0.10474238],\n",
      "        [ 0.15250094],\n",
      "        [-0.19935316],\n",
      "        [ 0.10292428],\n",
      "        [ 0.07134227],\n",
      "        [-0.17958495],\n",
      "        [ 0.17367113],\n",
      "        [-0.08024246]],\n",
      "\n",
      "       [[-0.00447392],\n",
      "        [ 0.12179632],\n",
      "        [-0.08349517],\n",
      "        [-0.24177447],\n",
      "        [ 0.07382018],\n",
      "        [ 0.04754252],\n",
      "        [-0.05539186],\n",
      "        [-0.13759983],\n",
      "        [ 0.03542756],\n",
      "        [ 0.13865417],\n",
      "        [-0.04067766],\n",
      "        [ 0.22095807],\n",
      "        [ 0.21523753],\n",
      "        [ 0.0432766 ],\n",
      "        [-0.2350905 ],\n",
      "        [ 0.23747863],\n",
      "        [-0.17379825],\n",
      "        [ 0.0414938 ],\n",
      "        [ 0.1925047 ],\n",
      "        [-0.15772122],\n",
      "        [-0.15212812],\n",
      "        [-0.1726285 ],\n",
      "        [-0.21023998],\n",
      "        [ 0.07194267],\n",
      "        [-0.18351138],\n",
      "        [-0.12728794],\n",
      "        [ 0.17694849],\n",
      "        [ 0.08672209],\n",
      "        [-0.22340359],\n",
      "        [ 0.07137984],\n",
      "        [ 0.0458828 ],\n",
      "        [-0.1743834 ]],\n",
      "\n",
      "       [[ 0.05863924],\n",
      "        [ 0.02119152],\n",
      "        [ 0.08443391],\n",
      "        [-0.04679196],\n",
      "        [ 0.00075537],\n",
      "        [ 0.0960625 ],\n",
      "        [ 0.20505366],\n",
      "        [-0.01066582],\n",
      "        [-0.22989914],\n",
      "        [-0.09082408],\n",
      "        [-0.11796901],\n",
      "        [ 0.08298402],\n",
      "        [-0.10312782],\n",
      "        [ 0.12745254],\n",
      "        [-0.13294847],\n",
      "        [ 0.0244951 ],\n",
      "        [ 0.24394284],\n",
      "        [ 0.21726623],\n",
      "        [ 0.0320463 ],\n",
      "        [-0.23851897],\n",
      "        [-0.23234996],\n",
      "        [ 0.06359472],\n",
      "        [-0.17890878],\n",
      "        [-0.07409804],\n",
      "        [-0.12238591],\n",
      "        [ 0.12548281],\n",
      "        [ 0.19513878],\n",
      "        [-0.12017664],\n",
      "        [ 0.22710748],\n",
      "        [ 0.21020111],\n",
      "        [ 0.23449808],\n",
      "        [-0.16788468]]], dtype=float32), array([[-0.14005455,  0.20469582,  0.20522216, -0.13111044,  0.23984659,\n",
      "        -0.16046886, -0.3088421 , -0.08959556, -0.2176388 ,  0.00302071,\n",
      "         0.10253571,  0.01934478,  0.05569064,  0.3597667 ,  0.3303083 ,\n",
      "        -0.11335777, -0.34811345,  0.16569878,  0.09586461, -0.04343753,\n",
      "         0.12899458, -0.25318825, -0.26568577, -0.1057029 , -0.22695693,\n",
      "        -0.25437087, -0.04327854, -0.11638983, -0.04869008,  0.11401311,\n",
      "        -0.01583507, -0.3259166 ],\n",
      "       [ 0.17425887, -0.22352517,  0.3506786 ,  0.34767666,  0.15458772,\n",
      "         0.02908145,  0.31216756, -0.09129068, -0.01055499,  0.20598269,\n",
      "        -0.41103283,  0.31597328, -0.17456362,  0.13917153, -0.08829739,\n",
      "         0.26785448, -0.25784478,  0.33256298, -0.11172856,  0.2696653 ,\n",
      "        -0.23096074,  0.03441823, -0.06690199,  0.31424356,  0.3413101 ,\n",
      "         0.1877444 , -0.18282774,  0.18729547,  0.2663749 , -0.28369892,\n",
      "         0.02731215, -0.08437318],\n",
      "       [ 0.3450976 , -0.11830922,  0.3286799 ,  0.00644452,  0.02449392,\n",
      "        -0.12270369,  0.2622034 ,  0.07295592,  0.12893623, -0.19574735,\n",
      "        -0.21623491, -0.24976559,  0.09921575, -0.18104103,  0.05866836,\n",
      "         0.268313  ,  0.34236768, -0.16976207, -0.18042642, -0.350976  ,\n",
      "        -0.27926606, -0.20144913,  0.13741049, -0.19068395,  0.04175761,\n",
      "        -0.03853563, -0.15364222,  0.26407632,  0.1941885 , -0.35355818,\n",
      "        -0.15188785, -0.09672513],\n",
      "       [-0.31290075,  0.33627155,  0.31234762, -0.04892588,  0.0698607 ,\n",
      "         0.20980372,  0.29471612,  0.02194565, -0.24285759, -0.1438337 ,\n",
      "         0.19059014, -0.25952053,  0.39730605, -0.35188374, -0.24601942,\n",
      "        -0.18977714,  0.14601155,  0.1874014 ,  0.0703575 , -0.02559868,\n",
      "         0.02479939,  0.34325323,  0.13807635,  0.19617395,  0.27743876,\n",
      "         0.39447933,  0.25525212, -0.19444689, -0.2146397 ,  0.23300707,\n",
      "         0.06328114, -0.38741365],\n",
      "       [-0.29223847,  0.34360963,  0.09648763,  0.17357928, -0.31739518,\n",
      "         0.10530499,  0.22148643,  0.27704614, -0.03341163,  0.43038526,\n",
      "        -0.2052547 ,  0.24467649,  0.09443777,  0.3074941 ,  0.3442707 ,\n",
      "         0.23503701,  0.20002021,  0.26596773,  0.22643723,  0.07650936,\n",
      "        -0.2551733 , -0.00811093,  0.34188595, -0.42070925, -0.36321303,\n",
      "        -0.11734051,  0.1247237 ,  0.0715998 ,  0.29254177, -0.3668219 ,\n",
      "        -0.29385558,  0.37541232]], dtype=float32), array([-0.00206579,  0.01172424,  0.04837191,  0.02499282,  0.02731691,\n",
      "        0.01062939,  0.02406261,  0.01445443,  0.0345738 , -0.00124727,\n",
      "        0.01480695,  0.05253328, -0.00113533, -0.00144155, -0.00136874,\n",
      "       -0.00123991,  0.01769305,  0.01868639, -0.00070376, -0.00169399,\n",
      "        0.01177088,  0.03392435, -0.00195292, -0.00156174, -0.00168059,\n",
      "        0.00888662, -0.00132259, -0.00193178, -0.00140013,  0.01231436,\n",
      "       -0.00125473, -0.00163826], dtype=float32), array([[-0.26891807],\n",
      "       [ 0.46514016],\n",
      "       [ 0.31046978],\n",
      "       [ 0.51660377],\n",
      "       [ 0.08584476],\n",
      "       [ 0.107581  ],\n",
      "       [ 0.46040732],\n",
      "       [ 0.4573108 ],\n",
      "       [ 0.55783844],\n",
      "       [-0.15235129],\n",
      "       [ 0.28059027],\n",
      "       [ 0.22707179],\n",
      "       [-0.16012378],\n",
      "       [-0.349619  ],\n",
      "       [-0.1586277 ],\n",
      "       [-0.14668128],\n",
      "       [ 0.22606361],\n",
      "       [ 0.08042968],\n",
      "       [-0.02984177],\n",
      "       [-0.23837496],\n",
      "       [ 0.43782052],\n",
      "       [ 0.310178  ],\n",
      "       [-0.15428975],\n",
      "       [-0.05437031],\n",
      "       [-0.388136  ],\n",
      "       [ 0.2578308 ],\n",
      "       [-0.24484192],\n",
      "       [-0.29409674],\n",
      "       [-0.31732965],\n",
      "       [ 0.16429442],\n",
      "       [-0.39690995],\n",
      "       [-0.24529865]], dtype=float32), array([0.0110846], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(lstmgan.discriminator.layers[1].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[0.02082506, 0.00783708, 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       [[0.03606154, 0.00216959, 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       [[0.03555323, 0.01896123, 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.03682151, 0.00366023, 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       [[0.03572324, 0.00200735, 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       [[0.0562045 , 0.05647721, 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]]], dtype=float32)]\n",
      "(1, 40000, 1, 20)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "hidden_layers = keras.backend.function(\n",
    "[lstmgan.discriminator.layers[0].input],  # we will feed the function with the input of the first layer  \n",
    "[lstmgan.discriminator.layers[0].output,] # we want to get the output of the first layer\n",
    ")\n",
    "h=hidden_layers([df_train])\n",
    "print(h)\n",
    "print(np.shape(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=np.array(h)  \n",
    "\n",
    "arr_reshaped = np.array(h).reshape(40000, 20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 20)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(arr_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('normal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.03606154 0.00216959 0.         0.         0.         0.01069222\n",
      "   0.         0.02286875 0.         0.02866585 0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "#To check 1st row\n",
    "print(h[:,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_1 = pd.read_csv('normal.csv')\n",
    "csv_2 = pd.read_csv('normallabel.csv')\n",
    "\n",
    "result = pd.concat([csv_1, csv_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"lgcnormalresult.csv\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fe17106b88>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXhU1eHG8e+ZyZ6wE3YhIDuILGErqEVQEKzrr1atu5baWpfa1mqVVlurdnNr1WqpVbFirVq1LohgEVQQwr7vYYcEAiQEsp/fH7NkJpmESSSZm5n38zx5uHPvnZtzE/Lm5NyzGGstIiLiXK5IF0BERGqnoBYRcTgFtYiIwymoRUQcTkEtIuJwcQ1x0bZt29qMjIyGuLSISFRaunTpQWtteqhjDRLUGRkZZGVlNcSlRUSikjFmR03H1PQhIuJwCmoREYdTUIuIOJyCWkTE4RTUIiIOp6AWEXE4BbWIiMM5KqifnruZzzblRroYIiKO4qigfm7eVr7YcjDSxRARcRRHBXWcy1BWroUMREQCOSqoXS5DeUVFpIshIuIojgrqOJehrEI1ahGRQI4KarfLUKE1HEVEgjgqqNVGLSJSnaOC2tNGraAWEQnkqKCOcxnK1fQhIhLEUUHt1sNEEZFqHBfU5WqjFhEJ4rCgdqnpQ0SkCkcFdZweJoqIVOOooHapjVpEpBpHBXWchpCLiFRz0qA2xvQxxqwI+Mg3xtzVEIVxq+lDRKSauJOdYK3dCAwGMMa4gT3AfxqkMC5Dablq1CIigera9DEe2Gqt3dEQhVE/ahGR6uoa1FcCM0MdMMZMNcZkGWOycnPrt0qL22WoUFCLiAQJO6iNMQnARcC/Qx231r5grc201mamp6fXqzCa5lREpLq61KgvAJZZaw80WGGMHiaKiFRVl6C+ihqaPU6VOLeCWkSkqrCC2hiTApwHvN2QhXG7XGr6EBGp4qTd8wCstceBNg1cFhLjXJSUqXueiEggR41MTIxzUVxWHuliiIg4isOC2k1xqWrUIiKBnBXU8S6K1fQhIhLEUUGd4HZRUl6hQS8iIgEcFdSJ8Z7ilGi+DxERP2cFdZwbQO3UIiIBHBbUnuKo54eISCWHBrVq1CIiPs4K6nhv04eCWkTEz1FBneBW04eISFWOCmpfrw/VqEVEKjkrqH1t1Or1ISLi57Cg9rVRq+lDRMTHYUGtpg8RkaocFdRJvpGJCmoRET9HBXVl04eCWkTEx1FBnaCRiSIi1TgqqNXrQ0SkOkcFdXKCp+njWHFZhEsiIuIcjgrqxDg3zZLiyCssiXRRREQcw1FBDZCWGMeS7LxIF0NExDHCWoW8MR06VsK+o0UcKy4jLdFxxRMRaXSOq1HfMCYDgGNFaqcWEQEHBnXfDs0AKCpVFz0REXBgUCd556Q+oaAWEQEcGNTJ3qBWjVpExMNxQe2bk7pIg15ERIAwg9oY09IY86YxZoMxZr0xZnRDFcjX9FGkYeQiIkD43fOeAmZZa//PGJMApDRUgfxNHyUKahERCCOojTHNgbOBGwCstSVAgw0d9AV1oYJaRAQIr+mjB5AL/MMYs9wYM90Yk1r1JGPMVGNMljEmKzc3t94F6tQymeR4N6t3H6n3NUREokk4QR0HDAWes9YOAQqBe6ueZK19wVqbaa3NTE9Pr3eBEuJc9EhPZdfhE/W+hohINAknqHcDu621X3lfv4knuBtMWmKcZtATEfE6aVBba/cDu4wxfby7xgPrGrJQRaXlLN6ex6684w35aUREmoRw+1HfDvzTGLMKGAw80nBFgpW7jwIwY9GOhvw0IiJNQljd86y1K4DMBi5LNakJmj1PRMRxIxMDpSa6I10EEZGIc2RQv3/7WECrkYuIgEODekCn5sS5jHp+iIjg0KA2xpCWFEdBUWmkiyIiEnGODGqATi2S2a1BLyIizg3q7m1TyT5YGOliiIhEnGODum1aAoePq+lDRMSxQZ3qHUZurY10UUREIsrRQV1eYdVFT0RinmODulmSZ1SiuuiJSKxzbFD7ho8XKqhFJMY5NqjTvDXq/BMKahGJbY4N6ow2nkVkXlusGfREJLY5Nqh7pHuCeubiXer5ISIxzbFBHe92cd8FfQHIKSiOcGlERCLHsUEN0LlVMgBHT2jgi4jELkcHdVqi54FiQZEeKIpI7HJ0UPv6Um8+UMAXWw5GuDQiIpHh6LWu0hLjAbj37dUAZD82JZLFERGJCEfXqH19qX0qKtT7Q0Rij6ODulVKfNDrorLyCJVERCRyHB3UKQlx/geKAMdLFNQiEnscHdQA7Zon+rdPKKhFJAY5PqhHZLT2b8/bmBPBkoiIRIbjg/qnE/v4t6e9uzaCJRERiQzHB3XbtERevCETgKFdW0a4NCIijc/xQQ1wbt/2nHlaS9KS4k9+sohIlAlrwIsxJhsoAMqBMmttZkMWKpSWyfGs2HWErbnHOD09rbE/vYhIxNSlRj3OWjs4EiENnj7VR0+UMv5Pn0Xi04uIREyTaPoAaJ6sZg8RiU3hBrUFZhtjlhpjpoY6wRgz1RiTZYzJys3NPXUl9GqpoBaRGBVuUI+x1g4FLgBuM8acXfUEa+0L1tpMa21menr6KS0kwA/H9aS9d/DLkuy8U359ERGnCiuorbV7vf/mAP8BRjRkoUJJindz7ahuAHz7rwvZuL+gsYsgIhIRJw1qY0yqMaaZbxs4H1jT0AULpV3zJP/2xCfns/mAwlpEol84Ner2wOfGmJXAYuADa+2shi1WaJcP7RL0+rl5WyNRDBGRRnXSoLbWbrPWnun9GGCt/W1jFCwUt8vw9FVD/K/3Hj0RqaKIiDSaJtM9z+eiMzv5tysqIlgQEZFG0uSCOtDi7Dw+3XAg0sUQEWlQTTqoAW56KYs1e45SrNVfRCRKNcmg/tG4nkGvL/zz5/R5YBZHT5RGqEQiIg2nSQb1Tyf2Yesjk2lRZbTiku0aCCMi0adJBjV4eoC88f3RQfsKS8oiVBoRkYbTZIMaoHvb1KDXT83dHKGSiDSMA/lFWGsjXQyJsCYd1AlxnuIP6tKClAQ38S4X2w8Wani5RIWtuccY+chcpi/Yzr6jJ3h89kaFdowKa+EAJ1v14PkkuF3c9/Zq/rN8D+P+OA+A124ZyTd6to1s4UTq4ERJORXWkpro+bHcmXccgAVbDvLx2v1k7TjM+QM6MLBzi0gWUyKgSdeoAZonxZMU7yY10R20f9vBwgiVSKR+Rvx2DgN+9bH/tQk4VuTtflqhGnVMavJB7eOrhfgcLiyJUElE6qegOPTDcGstJii2JdZETVAnxgXXqHOPFXP0RKna9KTJMqZ6OOu/c2yKmqCu2qf6lYU7OPOh2by3cm+ESiRy6oTIbIkhURPU3xl+Wsj9X2451MglETk1QmWzKtSxKWqCOi0xjrd+MJqbxnSnbVqCf/+8TTks3p5HfpGGl0vT5QttNeXFpqgJaoBh3Vrzy2/1Z/494/z7DuQXc8XzC7lj5nJN3CRNkrJZoiqofVISqncPn7cxl7eW7olAaUTqR+3S4hOVQV2TLTnHIl0EkfpRase0Jj8ysS52HCpkxsJsdh85wfi+7Zm3MYd7JvWNdLFEwqZWkNgUtUH96U/OwWUM7ZsnUVJWwe8+3sCbS3czd0MOAM9/tg2A60Zn0KFFUm2XEokoiw14mBjRokiERG3TR4/0NDLappKc4KZFSjxndmlBSVn1RRZHPTqX+95eRXmFfgLEWQJHI6rlI7ZFbVBXdXp6Wo3HZi7epfZrcazgWrQqFLEoaps+qhrUpWWtxxdszuW/K/fSv1NzRnZvTZu0xEYqmUhoqkWLT8wEdUKcixW/PI91+/K5+m9fVTv+8Afr/dtn9WrLjJtHNmbxRGqkdmmJmaYPgJYpCYzu0eak5x3IL/Jv/2vJTv6r+UIkAkyIbYV2bIqpoAbPjGTPXzuMCwZ2qPGcTQeOsWLXEQB+/tZqbp+5vLGKJ1KNxYacSU9iR8w0fQSaOKADEwd0YNaafdz66rKQ51zyzBeM1QoxEkkhslkV6tgUdo3aGOM2xiw3xrzfkAVqTBP6tadD85r7UH++5WAjlkYkNGvV9BHr6tL0cSew/qRnNSFxbhef/3wcY3u25eqRXcN+3+ebDzJ9wbYGLJmI+lFLpbCC2hjTBZgCTG/Y4jS+OLeLV28ZyUMXDQj7Pdf8/augXiIiDUmVaAm3Rv0kcA9QfWiflzFmqjEmyxiTlZube0oK15ji3S4mDehAv47NAcjs1iroeIVGLkojC1WL1nzUsemkQW2MuRDIsdYure08a+0L1tpMa21menr6KStgY/rrtcN497YxXDe6G49dfkbQsR6/+JCMez9g1pr9/n1FpZrfWhqXYjo2hVOjHgNcZIzJBl4HzjXGvNqgpYqghDgXv754IKenp5HRJoXxfdsFHb/11crfVz96Td32pBHYyvbqCtWoY9JJg9pae5+1tou1NgO4EvjUWntNg5cswowxzPvZOO4+v3eN58xZf4AH31urCZ2kQQRlsgmxT2JGzA14qat+HZoz5YyONR5/6ctsTv/Fh8xcvJNbXl5CaXkFOQVFNZ4vEi7rbeiwAQ0eCurYVKegttbOs9Ze2FCFcSKXy/DMd4fy+BVn1nrefW+vZs76HH7yxkpG/Hauf2SjSL0FhLLvuaKaPmKTatRhumxoF35/+SAAzuvfvsbz3vPOC7LjUGGjlEuiV6hIVkzHJgV1HbRKTQCgrLyCaRf2B2BERuuQ5ybFuxutXBKdQtWeVaOOTTE510d9jenZhnN6p3P/lH50bJFMt9YpjO/XjiXZh/liy0GemrvZf+4n6w7QpVUyAzq18O9btvMwmw8U8J3h4Y+ClNjly2Rrwbh82wrqWKQadR2kJMTx8k0j6NmuGamJcUzo3x5jDCO6t+bH5/VmTM/KKVTfXLqbKU9/zqFjxTzzvy2UlVdw2bNf8vO3Voe8dml5BUdPlDbWrUgTELLpQzkdkxTUp1C3NqnV9g17eA5/+Hgjs9cd8O+b8PhnFBRVhvLBY8X0uv8jznxodtB731u5l/1H1YMkVvlqz5bAftQRLJBEjIL6FPrRuJ7+B45VPfO/Lf7tLTnHmL32AEeOlwAwJyDES8s9o/SLSsu5Y+Zyvjt9UQOWWJzghflb+Wj1vmr7Q9eoldSxSEF9CnVqmcwVw08LeWzt3vyg1z/590oG//oTisvK2R7QQySnoBiAY8VlAOw+fKKBSitO8ciHG/jBP0PMi+5vo7b+eT9Uo45NCuoG8K+po5h+XSaz7jqL7Y9O5qYx3QF4//axbHr4gqBz+zwwi31HKps3xjz2KXmFJRwv9swjYgxs2J9Pjnd5sC+3HqSkrMa5sSSK2BB1atWoY5N6fTSAkVXWZfzlt/rz04m9SUnwfLkT41wUB4Ttgs3Bsw2u3HWEtXuP+l9PenIBHVsk8fy1w7j6b1/xvbO6c/+U/g14B+IE/l4fgfsiUhKJNNWoG4kvpIFqK5wfPh7c2+PGl5bwx9mbACgq9QT6vqNFHDzmaRbZdOBYQxZVHCKw8lzZ9KGojkUK6ggY0b01Wx+ZzOwfn12n95WWe35Ia1vtY/2+fP15HCV838XAb6e+tbFJQR0hbpehd/tmuLyhG85SYD/798oajy3beZg/fryRC55awNvL9pyqYkoEBf7C1TSnsU1BHWEjunuGoH//7B4AJMW7+F2VRQt88ovK/Nv9ps3i6bmbOVZcRlFpOZc9+yV/8XYB3JKrppFoEDTLqaY5jWl6mBhhz313GJ+sP0C3Nqm8ctMI+nZoRqvUhBpHMALM2+h5+Pj4J5uYvW4/o6s8vExw6/dvNAj9MFFJHYv0Ex1hrVITuCLT0/f67N7ptGueRHwdgnbNnnz+tmB70L6n5m4mr7DklJZTIiHEpEzqmRmTFNQO9ZPzevPri8NfGb2qFz/ffvKTxNFsiKeJaqOOTQpqh7p9fC+uGtGVCwd15N3bxgQdm9CvXQ3vqrRw2yFuemkJH63ex6FjxVourAnSfNTio6B2sHi3i79cPZQzT2vJv28d7d9/zahuAMz83qga37t0x2E+3ZDDD/65jGEPz+H0X3xYbUTj0h2H+WBV9TkmxBkC26iN92miul7GJj1MbCKGZ7Rm6QMTAGiTlkj2Y1MAOKd3Op9tyq3trX7/WrKTa0dnsCQ7j1cX7eDdFZ7VaKYMmsLynYfZmXeciwd3Zueh46QmummTltgwNyNhCT2EPAIFkYhTUDchoYLzscvPYNo7a/nhuNO56aUlHPGOcmyZEu/f9pn27lpOlJbz5JzNHC8p9++31nLps18CnpVpvj9jKQA3fCODey/oq9VqIiSwteqod6ZFtWDFJjV9NHEdWyQz/fpMhnZtxYpfno8x8O1hXfjlhaHnAnnkww1BIQ1wxfML/du+kAbPCuvvrthDTkER5RWWL7Yc9M/qJ6dGbU0ZvmNl5ZaVuz1zv+hhYmxSjTrKbH/U0ySyJafAv+/s3ums25vvnyukqiXZh2u8nq8/98DOzVmzJ58J/dox/frhrNp9hOKyCobXsGakhCecGnJhSeUvRwV1bFKNOkr1bNeMjQ9PIvuxKbxy0whaJH+938lr9njm056zPofNBwq46C9f8O2/Lqx2nrWWp+ZsZsP+yvm3l2TnqSZexVfbDjF9wbZag9d36FjAiNSycgV1LFJQR7HEuMq25X/cMILfePtlu12GF2/I5LZxp4d831m92tZ63fOemB9yv7WWuetzeGLOJiY9uYDjJWUcLizh239dyF2vL6/nXUQX37qY33lhEQ9/sL7GbpPr9ub7zy0IDGqNeIlJavqIEV3bpHDt6Ax2Hz7BxIEdGNq1Fef2bc+PxvXiX0t20jotkR5tU1m8PY+LBnfiREk5Z/3+f2Fd++43VrBw6yH2VVnf8fXFuzivf3ug+go3sejfWbv42Zur+PiuylkTQ9WorbVMfnqB/3VJeWU4l+lpYkxSUMeY+yb3C3qdnODmBu8KNAADO7fwb/94Qm+emLOp1uvtyjte42x976zYw1hv7dxV29ysAVbsOkL/js1JiIu+P/Z8c7RsOlD5/CBU7tbWDF2upo+YFH0/DXLK3DG+Jyt/dT7n9E4P2u+rJQO11rpX7T7K7sPHAXB5/6cVlZYz6cn5LNx6yP/a17shr7CES575gjtmLmfexhwy7v2AfUfDWzNy4/4CHv1ofZMYELJ6T+XqPaGaPkprad5QjTo2nTSojTFJxpjFxpiVxpi1xpiHGqNgEnnGGFokxzP9+kzWPjSRV70r06Q3S+Shi8Kbh+Sml7IAT406t6CYdfvy2bC/gIf+u5a1e4/Sd9osnpizGcC/KvustfuZsXAH4An7cNzwj8U8/9k2cmvo2eIEvgEsL8zf5t9XESqoa6k1ayqA2BROjboYONdaeyYwGJhkjKl57LJEnXi3i9TEOMb0bMNvLx3ILyb3o2e7tDpdY8eh4wz/7RxueHExABv2FzDl6c8BeDNrF0BQz5Byb834+zOWsnF/AQVFpYSjqfWKCNVGfbyWHjKqUcemkwa19fDNRB/v/dD/lhhkjOG7I7uRlhjHyO6taVuPIeaBix/47D1axCsLs/nDxxv9+3ztuQATn5zPNdO/8r/eHNDG6xPn9rSBO3mF9lCtMuUhdv7y3bU1XqOs3Ln3Jw0nrIeJxhg3sBToCTxjrf0qxDlTgakAXbuefFkpadri3C7m3n0ON7+8hPP6t2dC//YUFJXRqUUSIx6ZC8Dfr8/k5pezwrpebeEEsHL3UcrKK5jy9OdsPFDAqB6teX1q5URVvjm8i8rKa7qEI4Vqjl6SnVfj+apRx6awgtpaWw4MNsa0BP5jjBlorV1T5ZwXgBcAMjMz9b8pBrRIiefNH3yj2v7rR3fjUGEJ4/tVPnQ8r397xvVpxy/+U/PKNSfz7oq9bPTWphdtCw4z36o2ry/exdUju9K7fbN6f56GEm6NurSWWrPaqGNTnXp9WGuPAPOASQ1SGokKD108kL9cPRTwDJ5JcLv423WZXD2yK6sePJ+WKfE1vveMgO6BVf131d5q+2at2cf6ffn+GvVLX2Zz/hPz2ZV3POQ1rLVht3c3hlBNGbXVmlWjjk3h9PpI99akMcYkAxOADQ1dMIkOM24eycaHK3+vN0+K58Fv1dxj5LXvjazxWGC7NUDGvR9w66vLuOCpBew9EtyNz9dtcEvOMWYszPYvTfbC/G2c8eBscgsav3dIqGlLQ/XwqO2BaLlGJsakcGrUHYH/GWNWAUuAT6y17zdssSSamCqDXZoleVrcbh7bvdq5KQmeY4G9Sq4d1Y3WqQm1fo5DIdaItNYy8cn5THt3LbfPXMbc9Qd4b6WnVn7bP5f5h2jXRUlZBfmnsEYeaki4+lFLVSdto7bWrgKGNEJZJEac27cdv798EBcN7kRqYhxPz93sP+Z2GZZPO4/EeBe78k6wODuPK4efxrczu3DRX76o0+fpft+H/u0vthziiy2H/K8XZ+fx+OyNfLoxh7E903n0sjNCXqO0vILpC7Zz45gMkuLdTJ2RxbyNuf6FG+oiVBt1aVndFgdQG3Vs0shEaXTGGK4YfhpJ8W7uPq93tdBrlZpASkIcfTo049pR3Yh3uxjUpSXZj01hwT3jar32tkcmM6xbq6B93/Gu8l7Vywt3sCvvBDMX72TPkRMhm0NeX7KL383aQN9ps4DqzS9fV0l53XqpHDqm1eVjkYJaHOHJ7wzmxjEZJz3vtNYpAPTtUNmr49OfnOPfdrkMb/3gG/zmkoEAnN+/PQ+FsZr7mMc+5Wxvu3ZxWTlbcgqY+koW8zbk+M/5X8B2ffprh6oLF9fxOp9vOai+1DFIkzKJI1wypDOXDOkc1rmf/uQc2jZL5JO1B2iZEk+75knVznF5m8VbpsSHXEqsffNEDuQH16BPeOcd6fPArJCf98aXlvi3H3hnNY9cegZx7q9X16lP4BcWl9MiRXWsWKLvtjQ5PdLTaJ4Uz+XDujC+X3uSQsy018fbjzqzW/UVaC4c1JHLhnYJee3Adu3avJG1m7kbcrjl5SXsO3qCx2dv5Jn/bfEfX77zcFghXJ+gPlaiRRhijWrU0uSFqtVmZrRm/s/GcVrrZAD+cvUQDIbPt+Tyq28N4OCxYr7adohlO48AcFrrZHblhTdTn8/by3YzZ30OWTsW+BcSvm1cT9bvy+fSZ79kQr92XDqkCwu3HeS60RkhHxKW1KMZ41hRGdZa3sjaxeQzOtIsqeZ+6RIdVKOWqNW1TYq/a+CFgzoxZVBHHr1sEEnxbrq0SuHtH46hq7fN+6fn9wHgvR+N4fWpoeccc1WZUvvjtQcAglZ7X77zsH/S/znrc7jttWW8umgnd8xcTqhW6nBr1KkJbv5x43DAM3nVkuzD/Pyt1fzm/XVhvV+aNtWoJabdM6kP97y5iokDOrD90ckYYzhRZZX2B6b04+ze6STHu0+66s2lz34Zcn9NCyGEG9RulyHV28f85peXkO/tAx6JgTvS+FSjlph24aBOrPv1JJLi3f7ad3KCm8evONN/zlUjPHOHnNY6hezHpjDn7rNrulyNVu0+GnJu7ZqaPu4+r3fQ6zi3i04tPQ9Njxwv9a8Ms3zXEWYs2lHn8kjToqAWCeHSIZ25fnQ3/vujsaQmBv/hWZ/pXQFyQtR+a6pRn9u3XdBrt8vQpVUKL980Imj/keOlTHtnTZNY2UbqT00fIiEYY3jo4oEhj4V6eLf90ck8MWdz0CjLp64czIjurRn96Kc1fp6a+lEnxQfXodze2v7YnqFXiD9UWELbtESWZOfx0pfZJMe7GXxaS1buOsLlw7rw7oq9/ObiAV+7OyF4lk8rr7DVfoFJw9FXWqLC+7ePDXsB3a/LHfBU8cwuLVi5+yjGGK4Z2ZUvthzkwkEduXBQJ9KbnbzmXVONOjEuuO93coLb/7nP7duOTwMG34Bnzchr3v+KDfsrF1V4c+luAP7t/Xd833ZMCFjv8mTeWLKLe95axcaHJwWVZ/LTC9iWW1ivYfRSPwpqiQoDa5ketSHcPLY7FdZyz8S+HPf2a27XPIm3QszPfc2orry6aGfI69TURp1YpUbtm8gKYECn5tWC+rvTq63l4ecyntXOb3kli3/cOJyKCsv6fflMPqMjHVokkX+ijC05xxjbqy3WWvYcOUGXVin86RPPijszFu6gsLicOyf0Yto7a9iWW1jj55KGoaAWqYdpF/b3b/tquzV5+JIz2Jl3gvmbqs8T8ty8rSHfU7VGHTgd6h3je3FWr3RufmkJBbWsr+gTOI/Tjf+oHF35x9mbgs772cQ+/uXQxvdt5y/Dwx+sB+DOCb2CHlxaa3nov+uYvzmXT3/yzZOWQ+pPDxNFGkHzpLrViaq2Ua/fl+/fjne7GNG9NW//8Bs8cmnoWf/qI3DNyrkbcthZZfGF388Knoa+oLiMl77MZltuIdZaDheWsONQZW37QH5R0Pkb9udz9LhzFm1oShTUIo0g1HwjtUkI46Ffr/bNuHpkV/8vgRdvyCT7sSn83zDP8PgP7hhbeb0a+nHXxbNVav+Bfbgf+2gDQ37zCef8YR4Ai7fnMfKRubwfsCrPpCcXcOXfFtV4/YPHism49wM+Xrv/a5c12iioRRpBch2DOnCxhUkDOvDkdwbXeO6Hd57FnLvP4dy+ngeFD18ykI/uPIsBnVrwwJR+fHzX2dx6dg8A2ngXYHjrB6MZ3aNNXW8jyPg/febffn7+Nv/2jIXZ3O9dG/Mz77SwRaWeQUTr9+XzxCebgroT7j9ahLWW1d5+5v/8KnR7fixTG7VII2ieXPcftRk3e/pMn9UrvdbzurRKCXqdFO+mX8fmANxyliegs3Z4Bsv8dGIfrhrRFYCZU0exbm++f8h7Tc7o3ILVe6oP1qnJtIAV5fMKS7jr9eVBk2A9NXczlw3tzFfb87jnzVX+/b61NFPi3Rw6VkxxWQWdWibX+rmKSsuJd7uCeuJEI9WoRRpBr3Z1XxX9rF7pJw3pcF01vCt/vmpItUUU+ndqzp3jewGeB6SpVR6MpjdL5D8/rN6TJVxzN+Twzoq9XPfi4qD9JWUVPP9ZcFOKb86U5AQ3wx6ew8Qn5/PXz7by0ep9WGs5UVLO5rZIZzsAAApKSURBVAMFvJG1y18j7zttFrfPXFbv8jUVqlGLNIJJAzvwwwOnV2vnDSXrgQmn/PO7XIZvndkp5LE7xveic8tkLhvamXP7tmPRtkNMHtiRM389m8uGdCbO7eKLe8/lzazd9OmQxp2vr/AP1JnQrz1z1h+oc3mmL9jO1hq6+f1n+R4ACorKeOwjzwPMhy8ZyAPvrKFHeirbcgvp37E5Azp5/mr4cPV+jhwvoWVK7etqNmWmIYaeZmZm2qysrFN+XZGmrKColDMenF3rOSkJbtb9elKt5zSWnIIi2qQmhmxWWLHrCF1aJZOWGOdfpqwxXTuqG/dP6Rf0ubc9MpkDBUW0Skkgt6CYNXuOMrx7az5cvY+Lz+xMixRnTwdrjFlqrc0MdUw1apFG4ntAmBTv4v7J/fhozX6+3Hoo6JwKB83Z0a5Z9ZVzfAaf1rLavtenjuLKFxYx+YwOfLi6es+Ni87s5F8F/uuasWgHGw8UBO3r8QvPog+np6eSGOdm3b58BnRqztq9+fzy3bU8c/VQLhjYAfD8hVGb91ftZWjXVidtI28saqMWaWRuY7h2dAa3jesJwB3n9vQfu6KGhXidbGzPtvRsl8aoHm3IfmwKz353GG98fzSXVllarUur8EPv4sGhm2kCLd6eF3L/1txC1nn7na/dW9n//LbXltHjFx8y/vHPeHfFnmprT/paF0rLK/jRa8v59l8X+o9lHyzk97M2RGzyKwW1SCPx1eF8kxmN6dmW7MemcNeEyilNH/zWyRfidZpXbxnJnLvPCdo3ontr/vB/gwC4bnQ3HrvsDO4Y34t4d2VNdtWD59M6NXS78v2T+zG0q6fWfvXIrnx4x1kM6nJqpgnYfrCQO19fwSsLPaMsDxeWcO4f59H9vg/ZklPAZd45xfccOcH+o55BO7e+upRn521lx6Hj1a73p9kbybj3Ax54ZzUb9xdUO34qKKhFGklqYhwPTOnHv74/Omh/4J/hJ/uTvCmJc7tY+9BEfvWtAVw5oitJ8W5W/up8Ft03nvdvH0vzpHgW3TfeP/f2bwJWi09vlsgT3r7jkwZ0oH+n5rRI9rQx331e7zrVzmvy98+3s3THYYb85hO2HfQ82Hz8k01BXRFHPTqXTQcK/JNdhZrt8M+fetbKfHXRTiY+Of9rlysUtVGLNCJfv+ZYUXUq1JSEOFIS4ujQwtP+nRDn4offPJ2pZ/cgKd7t74NtjKFbm9SgGfpKvU0VnVom8/nPz6WotJzn5m3lhm9kcONLS1ix60idyrbnyAkufy54RZ5QbevnP1EZvit2HSYlwc3sdQe4aUwGf/1sW7XzG4KCWsQBXAZuGtM90sWIiDi3C98cVD+b2Mdfc64qr7AEwL/STVK8mx97a+Pv3DaGjHs/8J/70EUD6N42tVr/7T9fNYTbZy6vd1l//tZq//aLn29nz5G6LYhcXwpqEQfY9qjmdgb8D1hD+dnEvvxt/jZGZLQOeXz1g+ezdMdhvj9jKRcP7kRBUeXMghP6tWPahf1rXKihPhorpCGMNmpjzGnGmP8ZY9YbY9YaY+5sjIKJiAQ6r3973rh1dI2r1DRLiuebfdqx8eELaJmS4J9+1mVg+vXD6dYmlW5tUkK+F/Avc3b1yK78+9bRzPxe5Wr0fdrXPLL095cPqvNcLnUVTo26DPiJtXaZMaYZsNQY84m1VuvUi4hj+cIzcMBO1Xm+fYZ0bck5vdNZNu08WqXE+/u8P3XlYO58fUWtc7W4XYaGfgZ80qC21u4D9nm3C4wx64HOgIJaRBwrOd5Nz3Zp3OGdy8Rn48OT2LCvgKv/tohPf/pN0hLjiPfW0qt2Fxye0ZozOrfg5rHduWhwZ9LTEshom4rLGP9Dxl7t0+jToRnLdtbtYWZd1GkIuTEmA5gPDLTW5lc5NhWYCtC1a9dhO3ZoCXsRiW5FpeUkxbv589zN/OkTz4o59V1L8pQMITfGpAFvAXdVDWkAa+0LwAvgmeujXiUVEWlCfAtC3HJWD1bsOsI1o7o1yOcJK6iNMfF4Qvqf1tq3G6QkIiJNVHKCm7/fMLzBrh9Orw8D/B1Yb619vMFKIiIiIYUzhHwMcC1wrjFmhfdjcgOXS0REvMLp9fE5lfPJiIhII9OkTCIiDqegFhFxOAW1iIjDKahFRBxOQS0i4nANsgq5MSYXqO8Y8rbAwVNYnKZA9xwbdM/R7+vcbzdrbXqoAw0S1F+HMSarpvHu0Ur3HBt0z9Gvoe5XTR8iIg6noBYRcTgnBvULkS5ABOieY4PuOfo1yP06ro1aRESCObFGLSIiARTUIiIO55igNsZMMsZsNMZsMcbcG+nynCo1reJujGltjPnEGLPZ+2+rgPfc5/06bDTGTIxc6b8eY4zbGLPcGPO+93VU37MxpqUx5k1jzAbv93t0DNzzj73/r9cYY2YaY5Ki7Z6NMS8aY3KMMWsC9tX5Ho0xw4wxq73Hnja+FXTDYa2N+AfgBrYCPYAEYCXQP9LlOkX31hEY6t1uBmwC+gO/B+717r8X+J13u7/3/hOB7t6vizvS91HPe78beA143/s6qu8ZeBm4xbudALSM5nvGs8j1diDZ+/oN4IZou2fgbGAosCZgX53vEVgMjMYzbfRHwAXhlsEpNeoRwBZr7TZrbQnwOnBxhMt0Slhr91lrl3m3CwDfKu4X4/nBxvvvJd7ti4HXrbXF1trtwBY8X58mxRjTBZgCTA/YHbX3bIxpjucH+u8A1toSa+0RovieveKAZGNMHJAC7CXK7tlaOx/Iq7K7TvdojOkINLfWLrSe1H4l4D0n5ZSg7gzsCni927svqnhXcR8CfAW0t9buA0+YA+28p0XL1+JJ4B6gImBfNN9zDyAX+Ie3uWe6MSaVKL5na+0e4I/ATmAfcNRaO5sovucAdb3Hzt7tqvvD4pSgDtVWE1X9Bk+2invgqSH2NamvhTHmQiDHWrs03LeE2Nek7hlPzXIo8Jy1dghQiOdP4po0+Xv2tstejOdP/E5AqjHmmtreEmJfk7rnMNR0j1/r3p0S1LuB0wJed8HzJ1RUqGEV9wPeP4fw/pvj3R8NX4sxwEXGmGw8zVjnGmNeJbrveTew21r7lff1m3iCO5rveQKw3Vqba60tBd4GvkF037NPXe9xt3e76v6wOCWolwC9jDHdjTEJwJXAexEu0ylRyyru7wHXe7evB94N2H+lMSbRGNMd6IXnIUSTYa29z1rbxVqbged7+am19hqi+573A7uMMX28u8YD64jie8bT5DHKGJPi/X8+Hs8zmGi+Z5863aO3eaTAGDPK+7W6LuA9JxfpJ6oBT1En4+kRsRW4P9LlOYX3NRbPnzirgBXej8lAG2AusNn7b+uA99zv/TpspA5Php34AXyTyl4fUX3PwGAgy/u9fgdoFQP3/BCwAVgDzMDT2yGq7hmYiacNvhRPzfjm+twjkOn9Om0F/oJ3ZHg4HxpCLiLicE5p+hARkRooqEVEHE5BLSLicApqERGHU1CLiDicglpExOEU1CIiDvf/kAongiRz8bQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['gen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fe17210548>,\n",
       " <matplotlib.lines.Line2D at 0x1fe172106c8>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['dis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature12</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.049896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.057032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052905</td>\n",
       "      <td>0.043763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.052114</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.024351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.049743</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.017709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.048329</td>\n",
       "      <td>0.004298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0  0.049896  0.000000       0.0  0.074577  0.000000  0.003559       0.0   \n",
       "1  0.057032  0.000000       0.0  0.000000  0.052905  0.043763       0.0   \n",
       "2  0.052114  0.000672       0.0  0.000000  0.015918  0.024351       0.0   \n",
       "3  0.049743  0.003010       0.0  0.000000  0.002950  0.017709       0.0   \n",
       "4  0.048329  0.004298       0.0  0.000000  0.000000  0.014554       0.0   \n",
       "\n",
       "   feature8  feature9  feature10  ...  feature12  feature13  feature14  \\\n",
       "0  0.012226       0.0   0.019288  ...        0.0        0.0        0.0   \n",
       "1  0.000000       0.0   0.031136  ...        0.0        0.0        0.0   \n",
       "2  0.001684       0.0   0.026478  ...        0.0        0.0        0.0   \n",
       "3  0.006635       0.0   0.025008  ...        0.0        0.0        0.0   \n",
       "4  0.009139       0.0   0.024144  ...        0.0        0.0        0.0   \n",
       "\n",
       "   feature15  feature16  feature17  feature18  feature19  feature20  label  \n",
       "0   0.000000        0.0   0.066148        0.0        0.0        0.0      0  \n",
       "1   0.006044        0.0   0.000000        0.0        0.0        0.0      0  \n",
       "2   0.000000        0.0   0.000000        0.0        0.0        0.0      0  \n",
       "3   0.000000        0.0   0.000000        0.0        0.0        0.0      0  \n",
       "4   0.000000        0.0   0.000000        0.0        0.0        0.0      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test=df1.label\n",
    "\n",
    "x_test=df1.drop('label',axis=1).values.reshape(-1,20,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.12545866]]\n",
      "\n",
      " [[0.12732393]]\n",
      "\n",
      " [[0.12734361]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.12773241]]\n",
      "\n",
      " [[0.12771946]]\n",
      "\n",
      " [[0.12769538]]]\n"
     ]
    }
   ],
   "source": [
    "#y_pred=lstmgan.generator.predict(x_test)\n",
    "y_pred=lstmgan.discriminator.predict(x_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "yhat_probs = lstmgan.discriminator.predict(x_test, verbose=0)\n",
    "yhat_classes=np.argmax(yhat_probs,axis=1)\n",
    "# predict crisp classes for test set\n",
    "#yhat_classes = model.predict_classes(x_val, verbose=0)\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "#yhat_classes = yhat_classes[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-0ee39934b722>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# We set the threshold equal to the training loss of the autoencoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtesting_set_predictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlstmgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "# Helper function that calculates the reconstruction loss of each data sample\n",
    "def calculate_losses(x,preds):\n",
    "    losses=np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        losses[i]=((preds[i] - x[i]) ** 2).mean(axis=None)\n",
    "        \n",
    "    return losses\n",
    "\n",
    "# We set the threshold equal to the training loss of the autoencoder\n",
    "threshold=history.history[\"loss\"][-1]\n",
    "\n",
    "testing_set_predictions=lstmgan.discriminator.predict(x_test)\n",
    "test_losses=calculate_losses(x_test,testing_set_predictions)\n",
    "testing_set_predictions=np.zeros(len(test_losses))\n",
    "testing_set_predictions[np.where(test_losses>threshold)]=1\n",
    "\n",
    "\n",
    "error_df = pd.DataFrame({'Reconstruction_error': testing_set_predictions, 'True_class': y_test})\n",
    "error_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-a257af694ff2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0maccuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtesting_set_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtesting_set_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtesting_set_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtesting_set_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_val' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,precision_score,f1_score\n",
    "accuracy=accuracy_score(y_val,testing_set_predictions)\n",
    "recall=recall_score(y_val,testing_set_predictions)\n",
    "precision=precision_score(y_val,testing_set_predictions)\n",
    "f1=f1_score(y_val,testing_set_predictions)\n",
    "print(\"Performance over the testing data set \\n\")\n",
    "print(\"Accuracy : {} , Recall : {} , Precision : {} , F1 : {}\\n\".format(accuracy,recall,precision,f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/#:~:text=Optimal%20Threshold%20for%20Precision%2DRecall%20Curve,-Unlike%20the%20ROC&text=Recall%20is%20calculated%20as%20the,positives%20and%20the%20false%20negatives.\n",
    "#Optimal Threshold Tuning\n",
    "# search thresholds for imbalanced classification\n",
    "from numpy import arange\n",
    "from numpy import argmax\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')\n",
    " \n",
    "\n",
    "# predict probabilities\n",
    "#yhat = model.predict_proba(x_val)\n",
    "# keep probabilities for the positive outcome only\n",
    "#probs = yhat[:, 1]\n",
    "# define thresholds\n",
    "thresholds = arange(0, 1, 0.001)\n",
    "# evaluate each threshold\n",
    "scores = [f1_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each threshold\n",
    "scores = [accuracy_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, Accuracy-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each threshold\n",
    "scores = [precision_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, Precision-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each threshold\n",
    "scores = [recall_score(y_test, to_labels(yhat_probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, recall-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# ROC AUC\n",
    "auc = roc_auc_score(y_test, yhat_probs)\n",
    "print('ROC AUC: %f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=8\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(df_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "#x_test = keras.preprocessing.sequence.pad_sequences(x_test , maxlen=maxlen)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "#https://www.kaggle.com/suicaokhoailang/10-fold-lstm-with-attention-0-991-lb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import sklearn.metrics as sk\n",
    "from sklearn.metrics import roc_curve,f1_score\n",
    "from sklearn.metrics import auc\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=42069)\n",
    "preds = []\n",
    "fold = 0\n",
    "aucs = 0\n",
    "for train_idx, val_idx in kf.split(x_train):\n",
    "    x_train_f = x_train[train_idx]\n",
    "    y_train_f = y_test[train_idx]\n",
    "    x_val_f = x_train[val_idx]\n",
    "    y_val_f = y_test[val_idx]\n",
    "    lstmgan.discriminator.compile(optimizer, \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    #model = get_model()\n",
    "    lstmgan.discriminator.fit(x_train_f, y_train_f,\n",
    "              batch_size=256,\n",
    "              epochs=10,\n",
    "              verbose = 1,\n",
    "              validation_data=(x_val_f, y_val_f))\n",
    "\n",
    "    # Get accuracy of model on validation data. It's not AUC but it's something at least!\n",
    "    preds_val = lstmgan.discriminator.predict([x_val_f], batch_size=512)\n",
    "    preds.append(lstmgan.discriminator.predict(x_test))\n",
    "    fold+=1\n",
    "    fpr, tpr, thresholds = roc_curve(y_val_f, preds_val, pos_label=1)\n",
    "    # calculate scores\n",
    "    #lr_f1, lr_auc = f1_score(testy, yhat), auc(lr_recall, lr_precision)\n",
    "    aucs += auc(fpr,tpr)\n",
    "    print('Fold {}, AUC = {}'.format(fold,auc(fpr, tpr)))\n",
    "print(\"Cross Validation AUC = {}\".format(aucs/10))\n",
    "#print(sk.confusion_matrix(y_val_f,preds_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, accuracy_score ,recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#range of output value\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(range(len(y_pred)),y_pred.reshape(-1,1))\n",
    "plt.ylabel('y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for the optimal probability threshold\n",
    "#prob=[0.5,0.55,0.6,0.65,0.75,0.8,0.85,0.9,0.95,1,2,2.5,5]\n",
    "prob=[-0.2,-0.1,-0.05,0,0.005,0.1,0.157,0.2,0.3,0.5,1,1.5,2]\n",
    "for p in prob:\n",
    "    pred_value =[1 if i<p   else 0 for i in y_pred]\n",
    "    f1=f1_score(y_test,pred_value)\n",
    "    acc=accuracy_score(y_test,pred_value)\n",
    "    precision=average_precision_score(y_test,pred_value)\n",
    "    recall= recall_score(y_test,pred_value)\n",
    "    print(f'prob ={p} and f1 score ={f1} : accuracy={acc} : precision={precision} : recall={recall}')\n",
    "    print(pred_value.count(0),pred_value.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=dict()\n",
    "history['gen']=[]\n",
    "history['dis']=[]\n",
    "def train1(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for batch in dataset:\n",
    "\n",
    "           gen_loss,dis_loss= lstmgan.train(batch)\n",
    "        history['gen'].append(gen_loss)\n",
    "        history['dis'].append(dis_loss)\n",
    "        print ('Time for epoch {} is {} sec '.format(epoch + 1, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = lstmgan.discriminator.evaluate(x_test, y_test)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = lstmgan.generator.predict_classes(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average=\"binary\")\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"binary\")\n",
    "#np.savetxt('results/expected1.txt', y_test, fmt='%01d')\n",
    "#np.savetxt('results/predicted1.txt', y_pred, fmt='%01d')\n",
    "\n",
    "print(\"confusion matrix\")\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"accuracy\")\n",
    "print(\"%.6f\" %accuracy)\n",
    "print(\"racall\")\n",
    "print(\"%.6f\" %recall)\n",
    "print(\"precision\")\n",
    "print(\"%.6f\" %precision)\n",
    "print(\"f1score\")\n",
    "print(\"%.6f\" %f1)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"==============================================\")\n",
    "print (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
